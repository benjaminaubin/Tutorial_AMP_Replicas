@incollection{Aubin2018,
  title     = {The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author    = {Aubin, Benjamin and Maillard, Antoine and Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages     = {3223--3234},
  year      = {2018},
  Eprint = {https://arxiv.org/abs/1806.05451},
}

@article{Aubin_2019,
  url       = {https://doi.org/10.1088%2F1751-8121%2Fab227a},
  year      = 2019,
  publisher = {{IOP} Publishing},
  volume    = {52},
  number    = {29},
  pages     = {294003},
  author    = {Benjamin Aubin and Will Perkins and Lenka Zdeborov{\'{a}}},
  title     = {Storage capacity in symmetric binary perceptrons},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  abstract  = {We study the problem of determining the capacity of the binary perceptron for two variants of the problem where the corresponding constraint is symmetric. We call these variants the rectangle-binary-perceptron (RPB) and the u-function-binary-perceptron (UBP). We show that, unlike for the usual step-function-binary-perceptron, the critical capacity in these symmetric cases is given by the annealed computation in a large region of parameter space (for all rectangular constraints and for narrow enough u-function constraints, K  <  K
*). We prove this fact (under two natural assumptions) using the first and second moment methods. We further use the second moment method to conjecture that solutions of the symmetric binary perceptrons are organized in a so-called frozen-1RSB structure, without using the replica method. We then use the replica method to estimate the capacity threshold for the UBP case when the u-function is wide K  >  K
*. We conclude that full-step-replica-symmetry breaking would have to be evaluated in order to obtain the exact capacity in this case.},
  Eprint={https://arxiv.org/abs/1901.00314}
}


@incollection{Aubin2019c,
  title     = {The spiked matrix model with generative priors},
  author    = {Aubin, Benjamin and Loureiro, Bruno and Maillard, Antoine and Krzakala, Florent and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8366--8377},
  year      = {2019},
  url       = {http://papers.nips.cc/paper/9045-the-spiked-matrix-model-with-generative-priors.pdf},
  abstract  = {Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical and algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets.},
    Eprint={https://arxiv.org/abs/1905.12385}
}

@inproceedings{aubin2020exact,
  title        = {Exact asymptotics for phase retrieval and compressed sensing with random generative priors},
  author       = {Aubin, Benjamin and Loureiro, Bruno and Baker, Antoine and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle    = {Mathematical and Scientific Machine Learning},
  pages        = {55--73},
  year         = {2020},
  organization = {PMLR},
  Eprint={https://arxiv.org/abs/1912.02008}
}


@inproceedings{abbara2020rademacher,
  title        = {Rademacher complexity and spin glasses: A link between the replica and statistical theories of learning},
  author       = {Abbara, Alia and Aubin, Benjamin and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle    = {Mathematical and Scientific Machine Learning},
  pages        = {27--54},
  year         = {2020},
  organization = {PMLR},
  Eprint={https://arxiv.org/abs/1912.02729}
}

@inproceedings{baker2020tramp,
  title   = {TRAMP: Compositional Inference with TRee Approximate Message Passing},
  author  = {Baker, Antoine and Aubin, Benjamin and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle = {arXiv preprint arXiv:2004.01571. Submitted to Journal of Machine Learning Research},
  year    = {2020},
  Eprint={https://arxiv.org/abs/2004.01571}
}


@incollection{aubin2020generalization,
  title     = {Generalization error in high-dimensional perceptrons: Approaching Bayes error with convex optimization},
  author    = {Aubin, Benjamin and Krzakala, Florent and Lu, Yue M and Zdeborov{\'a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems 33},
  year      = {2020},
  Eprint={https://arxiv.org/abs/2006.06560}
}



@article{Bartlett03,
	Author = {Bartlett, Peter L. and Mendelson, Shahar},
	Issn = {1532-4435},
	Issue_Date = {3/1/2003},
	Journal = {J. Mach. Learn. Res.},
	Keywords = {data-dependent complexity, maximum discrepancy, error bounds, Rademacher averages},
	Month = mar,
	Number = {null},
	Numpages = {20},
	Pages = {463-482},
	Publisher = {JMLR.org},
	Title = {Rademacher and Gaussian Complexities: Risk Bounds and Structural Results},
	Volume = {3},
	Year = {2003}}

@inproceedings{Bottou10,
	Address = {Paris, France},
	Author = {Bottou, L\'{e}on},
	Booktitle = {Proceedings of the 19th International Conference on Computational Statistics},
	Month = {August},
	Pages = {177--187},
	Publisher = {Springer},
	Title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
	Year = {2010}
	}

@article{Robbins07,
	Author = {Herbert E. Robbins},
	Journal = {Annals of Mathematical Statistics},
	Pages = {400-407},
	Title = {A Stochastic Approximation Method},
	Volume = {22},
	Year = {1951}}

@article{Hornik91,
	Abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(*) performance criteria, for arbitrary finite input environment measures *, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	Author = {Kurt Hornik},
	Doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
	Number = {2},
	Pages = {251 - 257},
	Title = {Approximation capabilities of multilayer feedforward networks},
	Url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	Volume = {4},
	Year = {1991},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(91)90009-T}}

@article{Cybenko89,
	Abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	Author = {Cybenko, G.},
	Da = {1989/12/01},
	Date-Added = {2020-05-18 14:28:33 +0000},
	Date-Modified = {2020-05-18 14:28:33 +0000},
	Doi = {10.1007/BF02551274},
	Id = {Cybenko1989},
	Isbn = {1435-568X},
	Journal = {Mathematics of Control, Signals and Systems},
	Number = {4},
	Pages = {303--314},
	Title = {Approximation by superpositions of a sigmoidal function},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/BF02551274},
	Volume = {2},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02551274},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/BF02551274}}

@book{Petersen2008,
	Abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
	Added-At = {2011-01-17T12:52:58.000+0100},
	Author = {Petersen, K. B. and Pedersen, M. S.},
	Biburl = {https://www.bibsonomy.org/bibtex/263c840382cc4b1efb8cefe447465b7ac/hkayabilisim},
	File = {:home/hkaya/Projeler/diagnus/Screener/doc/literature/Petersen2008.pdf:PDF},
	Interhash = {6368b9b490c0225e22334ea0a0841a33},
	Intrahash = {63c840382cc4b1efb8cefe447465b7ac},
	Keywords = {matrixderivative inverse Matrixidentity matrixrelations},
	Month = oct,
	Note = {Version 20081110},
	Publisher = {Technical University of Denmark},
	Review = {Matrix Cookbook},
	Timestamp = {2011-01-17T12:52:58.000+0100},
	Title = {The Matrix Cookbook},
	Url = {http://www2.imm.dtu.dk/pubdb/p.php?3274},
	Year = 2008,
	Bdsk-Url-1 = {http://www2.imm.dtu.dk/pubdb/p.php?3274}}

@book{Fisher1925,
	Author = {Fisher, R.A.},
	Publisher = {American Psychological Association},
	Title = {Statistical methods for research workers},
	Year = 1925}

@article{Jaynes57,
	Author = {Jaynes, E. T.},
	Doi = {10.1103/PhysRev.106.620},
	Issue = {4},
	Journal = {Phys. Rev.},
	Month = {May},
	Numpages = {0},
	Pages = {620--630},
	Publisher = {American Physical Society},
	Title = {Information Theory and Statistical Mechanics},
	Url = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
	Volume = {106},
	Year = {1957},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRev.106.620}}

@book{jaynes03,
	Added-At = {2011-05-09T23:10:52.000+0200},
	Address = {Cambridge},
	Author = {Jaynes, E. T.},
	Biburl = {https://www.bibsonomy.org/bibtex/2ed3616cca9af65830fb13b9f53e0f19b/josephausterwei},
	Interhash = {27c58f26b65cfde811cbc41b7fe319cd},
	Intrahash = {ed3616cca9af65830fb13b9f53e0f19b},
	Keywords = {imported},
	Publisher = {Cambridge University Press},
	Timestamp = {2011-05-10T10:42:42.000+0200},
	Title = {Probability theory: The logic of science},
	Year = 2003}

@inbook{Kesavan2009,
	Abstract = {Keywords},
	Address = {Boston, MA},
	Author = {Kesavan, H. K.},
	Booktitle = {Encyclopedia of Optimization},
	Doi = {10.1007/978-0-387-74759-0_312},
	Isbn = {978-0-387-74759-0},
	Pages = {1779--1782},
	Publisher = {Springer US},
	Title = {Jaynes' maximum entropy principleJaynes' Maximum Entropy Principle},
	Url = {https://doi.org/10.1007/978-0-387-74759-0_312},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-0-387-74759-0_312},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-0-387-74759-0_312}}

@article{lebowitz1973modern,
	Author = {Lebowitz, Joel L and Penrose, Oliver},
	Journal = {Physics Today},
	Number = {2},
	Pages = {23--29},
	Publisher = {American Institute of Physics},
	Title = {Modern ergodic theory},
	Volume = {26},
	Year = {1973}}

@book{Mitchel1997,
	Address = {USA},
	Author = {Mitchell, Thomas M.},
	Edition = {1},
	Isbn = {0070428077},
	Publisher = {McGraw-Hill, Inc.},
	Title = {Machine Learning},
	Year = {1997}}

@book{Goodfellow2016,
	Author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	Isbn = {0262035618},
	Publisher = {The MIT Press},
	Title = {Deep Learning},
	Year = {2016}}

@article{mnist10,
	Title = {MNIST handwritten digit database},
	Author = {LeCun, Yann and Cortes, Corinna},
	Biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	Groups = {public},
	Year = {2010},
	Howpublished = {http://yann.lecun.com/exdb/mnist/}}

@misc{fashionmnist2017,
	Archiveprefix = {arXiv},
	Arxivid = {1708.07747},
	Author = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	Title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	Year = {2017}}

@article{cifar10,
	Abstract = {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
	Author = {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
	Keywords = {Dataset},
	Title = {CIFAR-10 (Canadian Institute for Advanced Research)},
	Url = {http://www.cs.toronto.edu/~kriz/cifar.html},
	Year = {2010},
	Bdsk-Url-1 = {http://www.cs.toronto.edu/~kriz/cifar.html}}




@article{imagenet09,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}


@book{Mohri12,
	Author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	Isbn = {026201825X},
	Publisher = {The MIT Press},
	Title = {Foundations of Machine Learning},
	Year = {2012}}

@article{LeCun15,
	Abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	Author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	Da = {2015/05/01},
	Date-Added = {2020-05-18 13:19:05 +0000},
	Date-Modified = {2020-05-18 13:19:05 +0000},
	Doi = {10.1038/nature14539},
	Id = {LeCun2015},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7553},
	Pages = {436--444},
	Title = {Deep learning},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature14539},
	Volume = {521},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14539},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/nature14539}}

@article{SK1978,
	Author = {Kirkpatrick, Scott and Sherrington, David},
	Doi = {10.1103/PhysRevB.17.4384},
	Issue = {11},
	Journal = {Phys. Rev. B},
	Month = {Jun},
	Numpages = {0},
	Pages = {4384--4403},
	Publisher = {American Physical Society},
	Title = {Infinite-ranged models of spin-glasses},
	Url = {https://link.aps.org/doi/10.1103/PhysRevB.17.4384},
	Volume = {17},
	Year = {1978},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevB.17.4384},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevB.17.4384}}

@article{Parisi79,
	Author = {Parisi, G.},
	Doi = {10.1103/PhysRevLett.43.1754},
	Issue = {23},
	Journal = {Phys. Rev. Lett.},
	Month = {Dec},
	Numpages = {0},
	Pages = {1754--1756},
	Publisher = {American Physical Society},
	Title = {Infinite Number of Order Parameters for Spin-Glasses},
	Url = {https://link.aps.org/doi/10.1103/PhysRevLett.43.1754},
	Volume = {43},
	Year = {1979},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.43.1754},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.43.1754}}

@article{Parisi80,
	Abstract = {In the framework of the new version of the replica theory, a sequence of approximated solutions is computed for the Sherrington-Kirkpatrick model (see Phys. Rev. Lett., vol.35, p.1972, 1975) of spin glasses.},
	Author = {G Parisi},
	Doi = {10.1088/0305-4470/13/4/009},
	Journal = {Journal of Physics A: Mathematical and General},
	Month = {apr},
	Number = {4},
	Pages = {L115--L121},
	Publisher = {{IOP} Publishing},
	Title = {A sequence of approximated solutions to the S-K model for spin glasses},
	Url = {https://doi.org/10.1088%2F0305-4470%2F13%2F4%2F009},
	Volume = {13},
	Year = 1980,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0305-4470%2F13%2F4%2F009},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4470/13/4/009}}

@article{Talagrand06,
	Abstract = {We compute at any temperature the free energy of the multi p-spin spherical model when only terms for p even are considered.},
	Author = {Talagrand, Michel},
	Da = {2006/03/01},
	Date-Added = {2020-05-13 16:07:40 +0000},
	Date-Modified = {2020-05-13 16:07:40 +0000},
	Doi = {10.1007/s00440-005-0433-8},
	Id = {Talagrand2006},
	Isbn = {1432-2064},
	Journal = {Probability Theory and Related Fields},
	Number = {3},
	Pages = {339--382},
	Title = {Free energy of the spherical mean field model},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/s00440-005-0433-8},
	Volume = {134},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00440-005-0433-8},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s00440-005-0433-8}}


@article{Castellani2005,
	Abstract = {In these notes the main theoretical concepts and techniques in the field of mean-field spin-glasses are reviewed in a compact and pedagogical way, for the benefit of the graduate and undergraduate student. One particular spin-glass model is analyzed (the p-spin spherical model) by using three different approaches. Thermodynamics, covering pure states, overlaps, overlap distribution, replica symmetry breaking, and the static transition. Dynamics, covering the generating functional method, generalized Langevin equation, equations for the correlation and the response, the Mode Coupling approximation, and the dynamical transition. And finally complexity, covering the mean-field (TAP) free energy, metastable states, entropy crisis, threshold energy, and saddles. Particular attention has been paid on the mutual consistency of the results obtained from the different methods.},
	Author = {Castellani, Tommaso and Cavagna, Andrea},
	Isbn = {1742-5468},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {Cavity and replica method,Disordered systems (theory),Slow dynamics and ageing (theory),Spin glasses (theory)},
	Mendeley-Groups = {Ressources PhD},
	Number = {5},
	Pages = {215--266},
	Pmid = {229586200004},
	Title = {{Spin-glass theory for pedestrians}},
	Year = {2005}
	}

@article{Edwards1975,
	Abstract = {A new theory of the class of dilute magnetic alloys, called the spin glasses, is proposed which offers a simple explanation of the cusp found experimentally in the susceptibility. The argument is that because the interaction between the spins dissolved in the matrix oscillates in sign according to distance, there will be no mean ferro- or antiferromagnetism, but there will be a ground state with the spins aligned in definite directions, even if these directions appear to be at random. At the critical temperature the existence of these preferred directions affects the orientation of the spins, leading to a cusp in the susceptibility. This cusp is smoothed by an external field. Although the behaviour at low t needs a quantum mechanical treatment, it is interesting to complete the classical calculations down to t=0. Classically the susceptibility tends to a constant value at t=0, and the specific heat to a constant value.},
	Author = {S F Edwards and P W Anderson},
	Doi = {10.1088/0305-4608/5/5/017},
	Journal = {Journal of Physics F: Metal Physics},
	Month = {may},
	Number = {5},
	Pages = {965--974},
	Publisher = {{IOP} Publishing},
	Title = {Theory of spin glasses},
	Url = {https://doi.org/10.1088%2F0305-4608%2F5%2F5%2F017},
	Volume = {5},
	Year = 1975,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0305-4608%2F5%2F5%2F017},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4608/5/5/017}}

@article{Krzakala07,
	Abstract = {An instance of a random constraint satisfaction problem defines a random subset * (the set of solutions) of a large product space X N (the set of assignments). We consider two prototypical problem ensembles (random k-satisfiability and q-coloring of random regular graphs) and study the uniform measure with support on S. As the number of constraints per variable increases, this measure first decomposes into an exponential number of pure states ({\textquotedblleft}clusters{\textquotedblright}) and subsequently condensates over the largest such states. Above the condensation point, the mass carried by the n largest states follows a Poisson-Dirichlet process. For typical large instances, the two transitions are sharp. We determine their precise location. Further, we provide a formal definition of each phase transition in terms of different notions of correlation between distinct variables in the problem. The degree of correlation naturally affects the performances of many search/sampling algorithms. Empirical evidence suggests that local Monte Carlo Markov chain strategies are effective up to the clustering phase transition and belief propagation up to the condensation point. Finally, refined message passing techniques (such as survey propagation) may also beat this threshold.},
	Author = {Krzakala, Florent and Montanari, Andrea and Ricci-Tersenghi, Federico and Semerjian, Guilhem and Zdeborov{\'a}, Lenka},
	Doi = {10.1073/pnas.0703685104},
	Issn = {0027-8424},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {25},
	Pages = {10318--10323},
	Publisher = {National Academy of Sciences},
	Title = {Gibbs states and the set of solutions of random constraint satisfaction problems},
	Url = {https://www.pnas.org/content/104/25/10318},
	Volume = {104},
	Year = {2007},
	Bdsk-Url-1 = {https://www.pnas.org/content/104/25/10318},
	Bdsk-Url-2 = {http://dx.doi.org/10.1073/pnas.0703685104}}

@article{Nishimori80,
	Abstract = {The random Ising model with competing interactions is investigated on the basis of the gauge-invariant formulation of the problem. Exact results for the internal energy, specific heat and gauge-invariant correlation function are derived. The critical exponent alpha is shown to be negative at the phase boundary of the paramagnetic and ferromagnetic phases if the latter exists at fairly low concentration of antiferromagnetic bonds.},
	Author = {H Nishimori},
	Doi = {10.1088/0022-3719/13/21/012},
	Journal = {Journal of Physics C: Solid State Physics},
	Month = {jul},
	Number = {21},
	Pages = {4071--4076},
	Publisher = {{IOP} Publishing},
	Title = {Exact results and critical properties of the Ising model with competing interactions},
	Url = {https://doi.org/10.1088%2F0022-3719%2F13%2F21%2F012},
	Volume = {13},
	Year = 1980,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0022-3719%2F13%2F21%2F012},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0022-3719/13/21/012}}

@book{dedominicis_2006,
	Author = {De Dominicis, Cirano and Giardina, Irene},
	Doi = {10.1017/CBO9780511534836},
	Publisher = {Cambridge University Press},
	Title = {Random Fields and Spin Glasses: A Field Theory Approach},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1017/CBO9780511534836}}

@article{Toulouse1987,
	Author = {Toulouse, G and others},
	Journal = {Spin Glass Theory and Beyond: An Introduction to the Replica Method and Its Applications},
	Pages = {99},
	Publisher = {World Scientific Publishing Company},
	Title = {Theory of the frustration effect in spin glasses: I},
	Volume = {9},
	Year = {1987}}

@article{Palmer1982,
	Author = {R.G.xs Palmer},
	Journal = {Advances in Physics},
	Number = {6},
	Pages = {669-735},
	Publisher = {Taylor & Francis},
	Title = {Broken ergodicity},
	Volume = {31},
	Year = {1982}}

@article{Kac1968,
	Author = {Kac},
	Journal = {Trondheim Theoretical Physics Seminar, Nordita Publ},
	Number = {286},
	Title = {Broken ergodicity},
	Year = {1968}}

@article{MartinSiggiaRose1973,
	Author = {Martin, P. C. and Siggia, E. D. and Rose, H. A.},
	Doi = {10.1103/PhysRevA.8.423},
	Issue = {1},
	Journal = {Phys. Rev. A},
	Month = {Jul},
	Numpages = {0},
	Pages = {423--437},
	Publisher = {American Physical Society},
	Title = {Statistical Dynamics of Classical Systems},
	Url = {https://link.aps.org/doi/10.1103/PhysRevA.8.423},
	Volume = {8},
	Year = {1973},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevA.8.423},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevA.8.423}}

@article{Derrida1981,
	Author = {Derrida, Bernard},
	Doi = {10.1103/PhysRevB.24.2613},
	Issue = {5},
	Journal = {Phys. Rev. B},
	Month = {Sep},
	Numpages = {0},
	Pages = {2613--2626},
	Publisher = {American Physical Society},
	Title = {Random-energy model: An exactly solvable model of disordered systems},
	Url = {https://link.aps.org/doi/10.1103/PhysRevB.24.2613},
	Volume = {24},
	Year = {1981},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevB.24.2613},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevB.24.2613}}

@article{Parisi1983,
	Author = {Parisi, Giorgio},
	Doi = {10.1103/PhysRevLett.50.1946},
	Issue = {24},
	Journal = {Phys. Rev. Lett.},
	Month = {Jun},
	Numpages = {0},
	Pages = {1946--1948},
	Publisher = {American Physical Society},
	Title = {Order Parameter for Spin-Glasses},
	Url = {https://link.aps.org/doi/10.1103/PhysRevLett.50.1946},
	Volume = {50},
	Year = {1983},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.50.1946},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.50.1946}}

@article{Rieger1992,
	Author = {Rieger, H.},
	Doi = {10.1103/PhysRevB.46.14655},
	Issue = {22},
	Journal = {Phys. Rev. B},
	Month = {Dec},
	Numpages = {0},
	Pages = {14655--14661},
	Publisher = {American Physical Society},
	Title = {The number of solutions of the Thouless-Anderson-Palmer equations for p-spin-interaction spin glasses},
	Url = {https://link.aps.org/doi/10.1103/PhysRevB.46.14655},
	Volume = {46},
	Year = {1992},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevB.46.14655},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevB.46.14655}}


@misc{Urbani2018,
	Author = {Urbani, Pierfrancesco},
	Title = {Statistical Physics of glassy systems: tools and applications},
	Year = {2018}}

@book{parisi_urbani_zamponi_2020,
	Author = {Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Doi = {10.1017/9781108120494},
	Place = {Cambridge},
	Publisher = {Cambridge University Press},
	Title = {Theory of Simple Glasses: Exact Solutions in Infinite Dimensions},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1017/9781108120494}}

@article{Zdeborova2016,
	Author = {Lenka Zdeborov{\'a} and Florent Krzakala},
	Doi = {10.1080/00018732.2016.1211393},
	Journal = {Advances in Physics},
	Number = {5},
	Pages = {453-552},
	Publisher = {Taylor & Francis},
	Title = {Statistical physics of inference: thresholds and algorithms},
	Url = {https://doi.org/10.1080/00018732.2016.1211393},
	Volume = {65},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1080/00018732.2016.1211393},
	Bdsk-Url-2 = {http://dx.doi.org/10.1080/00018732.2016.1211393}}

@article{Almeida1978,
	Abstract = {The stationary point used by Sherrington and Kirkpatrick (1975) in their evaluation of the free energy of a spin glass by the method of steepest descent is examined carefully. It is found that, although this point is a maximum of the integrand at high temperatures, it is not a maximum in the spin glass phase nor in the ferromagnetic phase at low temperatures. The instability persists in the presence of a magnetic field. Results are given for the limit of stability both for a partly ferromagnetic interaction in the absence of an external field and for a purely random interaction in the presence of a field.},
	Author = {J R L de Almeida and D J Thouless},
	Doi = {10.1088/0305-4470/11/5/028},
	Journal = {Journal of Physics A: Mathematical and General},
	Month = {may},
	Number = {5},
	Pages = {983--990},
	Publisher = {{IOP} Publishing},
	Title = {Stability of the Sherrington-Kirkpatrick solution of a spin glass model},
	Url = {https://doi.org/10.1088%2F0305-4470%2F11%2F5%2F028},
	Volume = {11},
	Year = 1978,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0305-4470%2F11%2F5%2F028},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4470/11/5/028}}

@article{blandin1978theories,
	Author = {Blandin, Annie},
	Journal = {Le Journal de Physique Colloques},
	Number = {C6},
	Pages = {C6--1499},
	Publisher = {EDP Sciences},
	Title = {Theories versus experiments in the spin glass systems},
	Volume = {39},
	Year = {1978}}

@article{blandin1980mean,
	Author = {Blandin, Andr{\'e} and Gabay, Marc and Garel, Thomas},
	Journal = {Journal of Physics C: Solid State Physics},
	Number = {3},
	Pages = {403},
	Publisher = {IOP Publishing},
	Title = {On the mean-field theory of spin glasses},
	Volume = {13},
	Year = {1980}}

@article{sommers1979sherrington,
	Author = {Sommers, H-J},
	Journal = {Zeitschrift f{\"u}r Physik B Condensed Matter},
	Number = {2},
	Pages = {173--180},
	Publisher = {Springer},
	Title = {The Sherrington-Kirkpatrick spin glass model: results of a new theory},
	Volume = {33},
	Year = {1979}}

@article{sommers1978,
	Abstract = {The spin glass model of Edwards and Anderson is solved for Ising spins starting from a renormalized diagrammatic expansion. One gets two qualitatively distinct phases for arbitrary external field. The high temperature phase is identical with the solution of Sherrington and Kirkpatrick. The low temperature phase does not have unphysical properties forT→0, in contrast to previous investigations.},
	Author = {Sommers, H. -J.},
	Da = {1978/09/01},
	Date-Added = {2020-06-17 08:58:28 +0000},
	Date-Modified = {2020-06-17 08:58:28 +0000},
	Doi = {10.1007/BF01352355},
	Id = {Sommers1978},
	Isbn = {1431-584X},
	Journal = {Zeitschrift f{\"u}r Physik B Condensed Matter},
	Number = {3},
	Pages = {301--307},
	Title = {Solution of the long-range gaussian-random Ising model},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/BF01352355},
	Volume = {31},
	Year = {1978},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF01352355},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/BF01352355}}

@article{dedominicis1979,
	Author = {De Dominicis C. and Garel T.},
	Journal = {J. Phys. Lett.},
	Number = {L575},
	Volume = {40},
	Year = {1979}}

@article{Bray1980,
	Abstract = {The average number (NS(f)) of uncorrelated metastable states of free energy kBTf per spin in the Sherrington-Kirkpatrick model of an Ising spin glass is shown to equal the extremum with respect to n of exp(nf N)(Zn), where (Zn) is the bond-averaged nth power of the partition function calculated within the two-group replica symmetry breaking scheme.},
	Author = {A J Bray and M A Moore},
	Doi = {10.1088/0022-3719/13/31/006},
	Journal = {Journal of Physics C: Solid State Physics},
	Month = {nov},
	Number = {31},
	Pages = {L907--L912},
	Publisher = {{IOP} Publishing},
	Title = {Broken replica symmetry and metastable states in spin glasses},
	Url = {https://doi.org/10.1088%2F0022-3719%2F13%2F31%2F006},
	Volume = {13},
	Year = 1980,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0022-3719%2F13%2F31%2F006},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0022-3719/13/31/006}}

@article{Parisi1980_sequence,
	Abstract = {In the framework of the new version of the replica theory, a sequence of approximated solutions is computed for the Sherrington-Kirkpatrick model (see Phys. Rev. Lett., vol.35, p.1972, 1975) of spin glasses.},
	Author = {G Parisi},
	Doi = {10.1088/0305-4470/13/4/009},
	Journal = {Journal of Physics A: Mathematical and General},
	Month = {apr},
	Number = {4},
	Pages = {L115--L121},
	Publisher = {{IOP} Publishing},
	Title = {A sequence of approximated solutions to the S-K model for spin glasses},
	Url = {https://doi.org/10.1088%2F0305-4470%2F13%2F4%2F009},
	Volume = {13},
	Year = 1980,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0305-4470%2F13%2F4%2F009},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4470/13/4/009}}

@article{Parisi1980_magnetic,
	Abstract = {The magnetic properties of spin glasses are studied in a recently proposed mean field theory; in this approach the replica symmetry is broken and the order parameter is a function (q(x)) on the interval 0-1. Exact results at the critical temperature and approximated results at all the temperatures are derived. The comparison with the computer simulations is briefly presented.},
	Author = {G Parisi},
	Doi = {10.1088/0305-4470/13/5/047},
	Journal = {Journal of Physics A: Mathematical and General},
	Month = {may},
	Number = {5},
	Pages = {1887--1895},
	Publisher = {{IOP} Publishing},
	Title = {Magnetic properties of spin glasses in a new mean field theory},
	Url = {https://doi.org/10.1088%2F0305-4470%2F13%2F5%2F047},
	Volume = {13},
	Year = 1980,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0305-4470%2F13%2F5%2F047},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4470/13/5/047}}

@article{Guerra2003,
	Abstract = {By using a simple interpolation argument, in previous work we have proven the existence of the thermodynamic limit, for mean field disordered models, including the Sherrington-Kirkpatrick model, and the Derrida p-spin model. Here we extend this argument in order to compare the limiting free energy with the expression given by the Parisi Ansatz, and including full spontaneous replica symmetry breaking. Our main result is that the quenched average of the free energy is bounded from below by the value given in the Parisi Ansatz, uniformly in the size of the system. Moreover, the difference between the two expressions is given in the form of a sum rule, extending our previous work on the comparison between the true free energy and its replica symmetric Sherrington-Kirkpatrick approximation. We give also a variational bound for the infinite volume limit of the ground state energy per site.},
	Author = {Guerra, Francesco},
	Da = {2003/02/01},
	Date-Added = {2020-06-17 09:17:08 +0000},
	Date-Modified = {2020-06-17 09:17:08 +0000},
	Doi = {10.1007/s00220-002-0773-5},
	Id = {Guerra2003},
	Isbn = {1432-0916},
	Journal = {Communications in Mathematical Physics},
	Number = {1},
	Pages = {1--12},
	Title = {Broken Replica Symmetry Bounds in the Mean Field Spin Glass Model},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/s00220-002-0773-5},
	Volume = {233},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00220-002-0773-5},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s00220-002-0773-5}}

@misc{mezard2009glasses,
	Archiveprefix = {arXiv},
	Author = {Marc Mezard and Giorgio Parisi},
	Eprint = {0910.2838},
	Title = {Glasses and replicas},
	Year = {2009}}

@incollection{parisi2003course,
	Author = {Parisi, Giorgio},
	Booktitle = {Slow Relaxations and nonequilibrium dynamics in condensed matter},
	Pages = {271--364},
	Publisher = {Springer},
	Title = {Course 6: Glasses, Replicas and All That}}

@article{Aji2000,
	Author = {S. M. {Aji} and R. J. {McEliece}},
	Journal = {IEEE Transactions on Information Theory},
	Number = {2},
	Pages = {325-343},
	Title = {The generalized distributive law},
	Volume = {46},
	Year = {2000}}

@incollection{dechter1988network,
	Author = {Dechter, Rina and Pearl, Judea},
	Booktitle = {Search in artificial intelligence},
	Pages = {370--425},
	Publisher = {Springer},
	Title = {Network-based heuristics for constraint-satisfaction problems},
	Year = {1988}}

@book{jensen1996introduction,
	Author = {Jensen, Finn V and others},
	Publisher = {UCL press London},
	Title = {An introduction to Bayesian networks},
	Volume = {210},
	Year = {1996}}

@book{gallager1968information,
	Author = {Gallager, Robert G},
	Publisher = {Springer},
	Title = {Information theory and reliable communication},
	Volume = {2},
	Year = {1968}}

@book{pearl1982reverend,
	Author = {Pearl, Judea},
	Title = {Reverend Bayes on inference engines: A distributed hierarchical approach},
	Year = {1982}}

@book{cover2012elements,
	Author = {Cover, Thomas M and Thomas, Joy A},
	Publisher = {John Wiley \& Sons},
	Title = {Elements of information theory},
	Year = {2012}}

@book{opper2001advanced,
	Author = {Opper, Manfred and Saad, David},
	Publisher = {MIT press},
	Title = {Advanced mean field methods: Theory and practice},
	Year = {2001}}

@article{jordan1999introduction,
	Author = {Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
	Journal = {Machine learning},
	Number = {2},
	Pages = {183--233},
	Publisher = {Springer},
	Title = {An introduction to variational methods for graphical models},
	Volume = {37},
	Year = {1999}}

@article{jaakkola2000bayesian,
	Author = {Jaakkola, Tommi S and Jordan, Michael I},
	Journal = {Statistics and Computing},
	Number = {1},
	Pages = {25--37},
	Publisher = {Springer},
	Title = {Bayesian parameter estimation via variational methods},
	Volume = {10},
	Year = {2000}}

@book{curie1895proprietes,
	Author = {Curie, Pierre},
	Number = {4},
	Publisher = {Gauthier-Villars et fils},
	Title = {Propri{\'e}t{\'e}s magn{\'e}tiques des corps a diverses temp{\'e}ratures},
	Year = {1895}}

@article{weiss1907hypothese,
	Author = {Weiss, Pierre},
	Title = {L'hypoth{\`e}se du champ mol{\'e}culaire et la propri{\'e}t{\'e} ferromagn{\'e}tique},
	Year = {1907}}

@article{pearl1986fusion,
	Author = {Pearl, Judea},
	Journal = {Artificial intelligence},
	Number = {3},
	Pages = {241--288},
	Publisher = {Elsevier},
	Title = {Fusion, propagation, and structuring in belief networks},
	Volume = {29},
	Year = {1986}}

@article{jordan2004graphical,
	Author = {Jordan, Michael I and others},
	Journal = {Statistical science},
	Number = {1},
	Pages = {140--155},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Graphical models},
	Volume = {19},
	Year = {2004}}

@article{hammersley1971markov,
	Author = {Hammersley, John M and Clifford, Peter},
	Journal = {Unpublished manuscript},
	Title = {Markov fields on finite graphs and lattices},
	Volume = {46},
	Year = {1971}}

@article{Potts,
	Author = {Wu, F. Y.},
	Doi = {10.1103/RevModPhys.54.235},
	Issue = {1},
	Journal = {Rev. Mod. Phys.},
	Month = {Jan},
	Numpages = {0},
	Pages = {235--268},
	Publisher = {American Physical Society},
	Title = {The Potts model},
	Url = {https://link.aps.org/doi/10.1103/RevModPhys.54.235},
	Volume = {54},
	Year = {1982},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/RevModPhys.54.235},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/RevModPhys.54.235}}

@article{Gardner1988a,
	Author = {Gardner, E},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/E{\_}Gardner{\_}1988{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}21{\_}030.pdf:pdf},
	Journal = {J. Phys. A},
	Pages = {257},
	Title = {{The space of interations in neural network models}},
	Volume = {21},
	Year = {1988}}

@article{Seung1992,
	Abstract = {Learning from examples in feedforward neural networks is studied within$\backslash$na statistical-mechanical framework. Training is assumed to be stochastic,$\backslash$nleading to a Gibbs distribution of networks characterized by a temperature$\backslash$nparameter T. Learning of realizable rules as well as of unrealizable$\backslash$nrules is considered. In the latter case, the target rule cannot be$\backslash$nperfectly realized by a network of the given architecture. Two useful$\backslash$napproximate theories of learning from examples are studied: the high-temperature$\backslash$nlimit and the annealed approximation. Exact treatment of the quenched$\backslash$ndisorder generated by the random sampling of the examples leads to$\backslash$nthe use of the replica theory. Of primary interest is the generalization$\backslash$ncurve, namely, the average generalization error ?g versus the number$\backslash$nof examples P used for training. The theory implies that, for a reduction$\backslash$nin ?g that remains finite in the large-N limit, P should generally$\backslash$nscale as ?N, where N is the number of independently adjustable weights$\backslash$nin the network. We show that for smooth networks, i.e., those with$\backslash$ncontinuously varying weights and smooth transfer functions, the generalization$\backslash$ncurve asymptotically obeys an inverse power law. In contrast, for$\backslash$nnonsmooth networks other behaviors can appear, depending on the nature$\backslash$nof the nonlinearities as well as the realizability of the rule. In$\backslash$nparticular, a discontinuous learning transition from a state of poor$\backslash$nto a state of perfect generalization can occur in nonsmooth networks$\backslash$nlearning realizable rules. We illustrate both gradual and continuous$\backslash$nlearning with a detailed analytical and numerical study of several$\backslash$nsingle-layer perceptron models. Comparing with the exact replica$\backslash$ntheory of perceptron learning, we find that for realizable rules$\backslash$nthe high-temperature and annealed theories provide very good approximations$\backslash$nto the generalization performance. Assuming this to hold for multilayer$\backslash$nnetworks as well, we propose a classification of possible asymptotic$\backslash$nforms of learning curves in general realizable models. For unrealizable$\backslash$nrules we find that the above approximations fail in general to predict$\backslash$ncorrectly the shapes of the generalization curves. Another indication$\backslash$nof the important role of quenched disorder for unrealizable rules$\backslash$nis that the generalization error is not necessarily a monotonically$\backslash$nincreasing function of temperature. Also, unrealizable rules can$\backslash$npossess genuine spin-glass phases indicative of degenerate minima$\backslash$nseparated by high barriers.},
	Author = {Seung, Hs and Sompolinsky, H and Tishby, N},
	Doi = {10.1103/PhysRevA.45.6056},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/38. Statistical mechanics of learning from examples.pdf:pdf},
	Isbn = {1050-2947 (Print)$\backslash$r1050-2947 (Linking)},
	Issn = {1050-2947},
	Journal = {Physical Review A},
	Number = {8},
	Pages = {6056--6091},
	Pmid = {9907706},
	Title = {{Statistical mechanics of learning from examples}},
	Url = {http://www.ncbi.nlm.nih.gov/pubmed/9907706{\%}5Cnhttp://link.aps.org/doi/10.1103/PhysRevA.45.6056},
	Volume = {45},
	Year = {1992},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.45.6056}}

@article{Sompolinsky1990,
	Abstract = {A statistical mechanical theory of learning from examples in layered networks at finite temperature is studied. When the training error is a smooth function of continuously varying weights the generalization error falls off asymptotically as the inverse number of examples. By analytical and numerical studies of single-layer perceptrons we show that when the weights are discrete the generalization error can exhibit a discontinuous transition to perfect generalization. For intermediate sizes of the example set, the state of perfect generalization coexists with a metastable spin-glass state. PACS numbers: 87. 10.+e, 02.50.+s, 05.20.-y Understanding how systems can be efficiently trained to perform tasks is of fundamental importance. A cen-tral issue in learning theory is the rate of improvement in the processing of novel data as a function of the number of examples presented during training, i.e. , the generali-zation curve. ' Numerical results on training in layered neural networks indicate that the generalization error improves gradually in some cases, and sharply in oth-ers. s In this work we use statistical mechanics to study generalization curves in large layered networks. We will first discuss the general theory and then present results for learning in a single-layer perceptron. The computational function of layered neural net-works is described in terms of the input-output relations that they generate. We consider here a multilayer net-work with M input nodes, whose states are denoted by S;, i =I, . . . , M, and a single output node denoted by cr=cr(W;S) where the W{\~{}}, i = I, . . . , N, denote the synaptic weights of the network. The network is trained by adjusting its weights to approximate or reproduce, if possible, a target function cto(S) on the input space. This is achieved by providing a set of examples consist-ing of P input-output pairs (S', cro({\$}')), /= I, . . . , P. We assume that the inputs {\$}' are chosen at random from the entire input space. The training process is often described as the minimi-zation of a training energy P E(W) = g e(W;S'), where e(W;S) is some measure of the deviation of the network output o(W;S) from the target output cro({\$}), e. g., e(w;S) =fcr(W;S) - o'o(S)] . We consider a sto chastic training process that leads at long times to a Gibbs distribution of networks P(W) =Z 'exp[ PE(w)], - where Z- : fdWexp[ PE(W)], dW= + dW, P- o(W), ((Z")) = Q d W exp(PG [W ]), - o=l where (2) G = - ln dSexp - P g e(W;S) tT= l (3) The variables W represent the weights of n copies (re-plicas) of the system. The importance of the form (2) lies in the fact that the effective (nonrandom) Hamil-tonian G is intensive, and does not depend on the number of examples P. A direct consequence is that the correct thermodynamic limit (N {\~{}}) is achieved when the number of examples scales as the total number of (in-dependently determined) weights, i.e. , P = a{\~{A}}. This scaling guarantees that both the entropy and the energy and Po(W) is the a priori measure on the weight space. The temperature T=P ' denotes the level of stochastic noise in the training. In the limit T 0 the training corresponds to finding the global minimum of E. The training energy E depends on the fixed random choice of examples S'. Focusing on average quantities, we will first average over P(W) with fixed S'. This thermal average will be denoted by ()T. We will then perform a quenched average, i.e. , average over the distribution of example sets, (()) - = fgt dS'. Here dS represents the normalized measure over the space of inputs. The average training error is e, (T, P)= P- x(((E(W))r)). The performance of a network on the whole input space is measured by the generalization er-ror, defined as e(W) =fdSc(W;S) The . average gen-eralization error, after training with P examples, is cg(T, P)- = (((e(w))T)). The deviations of the typical values of these quantities from their thermal and quenched averages are expected to vanish as N We apply the replica method' to evaluate quenched averages. The averge free energy F is given by - PF= - ((lnZ)) = lim n '(((Z")) - 1) . n 0 Using Eq. (I) we find 1683},
	Author = {Sompolinsky, H. and Tishby, N. and Seung, H. S.},
	Doi = {10.1103/PhysRevLett.65.1683},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.65.1683.pdf:pdf},
	Isbn = {1079-7114 (Electronic)$\backslash$r0031-9007 (Linking)},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {13},
	Pages = {1683--1686},
	Pmid = {10042332},
	Title = {{Learning from examples in large neural networks}},
	Volume = {65},
	Year = {1990},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.65.1683}}

@article{Opper1990,
	Abstract = {A linearly separable Boolean function is derived from a set of examples by a perceptron with optimal stability. The probability to reconstruct a pattern which is not learnt is calculated analytically using the replica method.},
	Author = {Opper, M. and Kinzel, W. and Kleinz, J. and Nehl, R.},
	Doi = {10.1088/0305-4470/23/11/012},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/M{\_}Opper{\_}1990{\_}J.{\_}Phys.{\_}A{\_}{\_}Math.{\_}Gen.{\_}23{\_}012.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: General Physics},
	Number = {11},
	Title = {{On the ability of the optimal perceptron to generalise}},
	Volume = {23},
	Year = {1990},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/23/11/012}}

@article{Advani_2013,
	Author = {Advani, Madhu and Lahiri, Subhaneil and Ganguli, Surya},
	Doi = {10.1088/1742-5468/2013/03/p03014},
	Issn = {1742-5468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Month = {Mar},
	Number = {03},
	Pages = {P03014},
	Publisher = {IOP Publishing},
	Title = {Statistical mechanics of complex neural systems and high dimensional data},
	Url = {http://dx.doi.org/10.1088/1742-5468/2013/03/P03014},
	Volume = {2013},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2013/03/P03014},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-5468/2013/03/p03014}}

@article{Carleo19,
	Author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborova, Lenka},
	Doi = {10.1103/revmodphys.91.045002},
	Issn = {1539-0756},
	Journal = {Reviews of Modern Physics},
	Month = {Dec},
	Number = {4},
	Publisher = {American Physical Society (APS)},
	Title = {Machine learning and the physical sciences},
	Url = {http://dx.doi.org/10.1103/RevModPhys.91.045002},
	Volume = {91},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/RevModPhys.91.045002},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/revmodphys.91.045002}}

@article{Zdeborova16,
	Author = {Zdeborova, Lenka and Krzakala, Florent},
	Doi = {10.1080/00018732.2016.1211393},
	Issn = {1460-6976},
	Journal = {Advances in Physics},
	Month = {Aug},
	Number = {5},
	Pages = {453-552},
	Publisher = {Informa UK Limited},
	Title = {Statistical physics of inference: thresholds and algorithms},
	Url = {http://dx.doi.org/10.1080/00018732.2016.1211393},
	Volume = {65},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/00018732.2016.1211393}}

@article{Mehta19,
	Author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G.R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
	Doi = {10.1016/j.physrep.2019.03.001},
	Issn = {0370-1573},
	Journal = {Physics Reports},
	Month = {May},
	Pages = {1-124},
	Publisher = {Elsevier BV},
	Title = {A high-bias, low-variance introduction to Machine Learning for physicists},
	Url = {http://dx.doi.org/10.1016/j.physrep.2019.03.001},
	Volume = {810},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.physrep.2019.03.001}}

@book{Rong89,
	Author = {R. Wong},
	Doi = {https://doi.org/10.1016/C2013-0-07651-7},
	Title = {Asymptotic Approximations of Integrals Computer Science and Scientific Computing},
	Year = 1989,
	Bdsk-Url-1 = {https://doi.org/10.1016/C2013-0-07651-7}}

@article{wu1982potts,
	Author = {Wu, Fa-Yueh},
	Journal = {Reviews of modern physics},
	Number = {1},
	Pages = {235},
	Publisher = {APS},
	Title = {The potts model},
	Volume = {54},
	Year = {1982}}

@article{onsager1944crystal,
	Author = {Onsager, Lars},
	Journal = {Physical Review},
	Number = {3-4},
	Pages = {117},
	Publisher = {APS},
	Title = {Crystal statistics. I. A two-dimensional model with an order-disorder transition},
	Volume = {65},
	Year = {1944}}

@article{Stanley68,
	Author = {Stanley, H. E.},
	Doi = {10.1103/PhysRevLett.20.589},
	Issue = {12},
	Journal = {Phys. Rev. Lett.},
	Month = {Mar},
	Numpages = {0},
	Pages = {589--592},
	Publisher = {American Physical Society},
	Title = {Dependence of Critical Properties on Dimensionality of Spins},
	Url = {https://link.aps.org/doi/10.1103/PhysRevLett.20.589},
	Volume = {20},
	Year = {1968},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.20.589},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.20.589}}

@article{DEGENNES1972,
	Abstract = {By an expansion to second order in ϵ = 4-d, we derive the mean square extension R2 for a random, self excluding walk of N jumps on a d-dimensional lattice. The result is: R2 = const. N1.195(for d = 3).},
	Author = {P.G. de Gennes},
	Doi = {https://doi.org/10.1016/0375-9601(72)90149-1},
	Issn = {0375-9601},
	Journal = {Physics Letters A},
	Number = {5},
	Pages = {339 - 340},
	Title = {Exponents for the excluded volume problem as derived by the Wilson method},
	Url = {http://www.sciencedirect.com/science/article/pii/0375960172901491},
	Volume = {38},
	Year = {1972},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0375960172901491},
	Bdsk-Url-2 = {https://doi.org/10.1016/0375-9601(72)90149-1}}

@article{Gaspari86,
	Author = {Gaspari, George and Rudnick, Joseph},
	Doi = {10.1103/PhysRevB.33.3295},
	Issue = {5},
	Journal = {Phys. Rev. B},
	Month = {Mar},
	Numpages = {0},
	Pages = {3295--3305},
	Publisher = {American Physical Society},
	Title = {n-vector model in the limit n\ensuremath{\rightarrow}0 and the statistics of linear polymer systems: A Ginzburg-Landau theory},
	Url = {https://link.aps.org/doi/10.1103/PhysRevB.33.3295},
	Volume = {33},
	Year = {1986},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevB.33.3295},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevB.33.3295}}

@article{bayati2011dynamics,
	Author = {Bayati, Mohsen and Montanari, Andrea},
	Journal = {IEEE Transactions on Information Theory},
	Number = {2},
	Pages = {764--785},
	Publisher = {IEEE},
	Title = {The dynamics of message passing on dense graphs, with applications to compressed sensing},
	Volume = {57},
	Year = {2011}}

@article{plefka1982convergence,
	Author = {Plefka, Timm},
	Journal = {Journal of Physics A: Mathematical and general},
	Number = {6},
	Pages = {1971},
	Publisher = {IOP Publishing},
	Title = {Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model},
	Volume = {15},
	Year = {1982}}

@book{georges2004introduction,
	Author = {Georges, Antoine and M{\'e}zard, Marc},
	Publisher = {{\'E}cole polytechnique, D{\'e}partement de Physique},
	Title = {Introduction {\`a} la th{\'e}orie statistique des champs: majeure de physique: Promotion 2001, Ann{\'e}e 3, Majeure 2, PHY557B},
	Year = {2004}}

@book{balian1986physique,
	Author = {Balian, Roger and Br{\'e}zin, {\'E}douard and Tol{\'e}dano, Jean-Claude},
	Publisher = {{\'E}cole polytechnique, D{\'e}partement de physique},
	Title = {Physique statistique},
	Year = {1986}}

@book{ma2018modern,
	Author = {Ma, Shang-Keng},
	Publisher = {Routledge},
	Title = {Modern theory of critical phenomena},
	Year = {2018}}

@book{diu1989elements,
	Author = {Diu, Bernard and Roulet, Bernard and Guthmann, Claudine and Lederer, Danielle},
	Publisher = {Hermann},
	Title = {El{\'e}ments de physique statistique},
	Year = {1989}}

@article{Anderson393,
	Author = {Anderson, P. W.},
	Doi = {10.1126/science.177.4047.393},
	Issn = {0036-8075},
	Journal = {Science},
	Number = {4047},
	Pages = {393--396},
	Publisher = {American Association for the Advancement of Science},
	Title = {More Is Different},
	Volume = {177},
	Year = {1972}
	}

@article{zia2009making,
	Author = {Zia, Royce KP and Redish, Edward F and McKay, Susan R},
	Journal = {American Journal of Physics},
	Number = {7},
	Pages = {614--622},
	Publisher = {American Association of Physics Teachers},
	Title = {Making sense of the Legendre transform},
	Volume = {77},
	Year = {2009}}

@article{Touchette08,
	Author = {Touchette, Hugo},
	Doi = {10.1016/j.physrep.2009.05.002},
	Journal = {Physics Reports},
	Month = {04},
	Title = {Touchette, H.: The large deviation approach to statistical mechanics. Phys. Rep. 478, 1-69},
	Volume = {478},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.physrep.2009.05.002}}

@article{Yoshitsugu89,
	Abstract = {{An attempt to unify statistical mechanics from the large deviation theoretical point of view is described. Theory of large deviations has already been applied to equilibrium statistical mechanics (mainly by mathematicians). However, nonequilibrium statistical physics (or statistical mechanics along the time axis) is a more interesting field to apply large deviation theory. From the large deviation point of view we can unify equilibrium statistical mechanics, statistical theory for chaos, for chaos, theory of multifractals, statistical theory for (non-equilibrium) steady states, and more. The point of view sheds light upon various foundational questions about nonequilibrium as well as equilibrium statistical physics. Although this paper contains expository parts for convenience and for the promotion of this point of view, the main aim is to explore a sound basis for nonequilibrium statistical physics.}},
	Author = {Oono, Yoshitsugu},
	Issn = {0375-9687},
	Journal = {Progress of Theoretical Physics Supplement},
	Month = {06},
	Pages = {165-205},
	Title = {{Large Deviation and Statistical Physics}},
	Volume = {99},
	Year = {1989}
	}

@article{varadhan2008,
	Author = {Varadhan, S. R. S.},
	Doi = {10.1214/07-AOP348},
	Fjournal = {Annals of Probability},
	Journal = {Ann. Probab.},
	Month = {03},
	Number = {2},
	Pages = {397--419},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Large deviations},
	Url = {https://doi.org/10.1214/07-AOP348},
	Volume = {36},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1214/07-AOP348},
	Bdsk-Url-2 = {http://dx.doi.org/10.1214/07-AOP348}}

@article{gartner1977large,
	Author = {G{\"a}rtner, J{\"u}rgen},
	Journal = {Theory of Probability \& Its Applications},
	Number = {1},
	Pages = {24--39},
	Publisher = {SIAM},
	Title = {On large deviations from the invariant measure},
	Volume = {22},
	Year = {1977}}

@article{ellis1984large,
	Author = {Ellis, Richard S and others},
	Journal = {The Annals of Probability},
	Number = {1},
	Pages = {1--12},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Large deviations for a general class of random vectors},
	Volume = {12},
	Year = {1984}}

@article{martin2001statistical,
	Author = {Martin, Olivier C and Monasson, R{\'e}mi and Zecchina, Riccardo},
	Journal = {Theoretical computer science},
	Number = {1-2},
	Pages = {3--67},
	Publisher = {Elsevier},
	Title = {Statistical mechanics methods and phase transitions in optimization problems},
	Volume = {265},
	Year = {2001}}

@book{Mantegna00,
	Author = {Mantegna, Rosario and Stanley, H.},
	Doi = {10.1063/1.1341926},
	Journal = {Nature},
	Month = {12},
	Title = {An Introduction to Econophysics: Correlations and Complexity in Finance},
	Volume = {53},
	Year = {2000},
	Bdsk-Url-1 = {http://dx.doi.org/10.1063/1.1341926}}

@book{bouchaud_potters_2003,
	Author = {Bouchaud, Jean-Philippe and Potters, Marc},
	Doi = {10.1017/CBO9780511753893},
	Edition = {2},
	Place = {Cambridge},
	Publisher = {Cambridge University Press},
	Title = {Theory of Financial Risk and Derivative Pricing: From Statistical Physics to Risk Management},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1017/CBO9780511753893}}

@book{voit2013statistical,
	Author = {Voit, Johannes},
	Publisher = {Springer Science \& Business Media},
	Title = {The statistical mechanics of financial markets},
	Year = {2013}}

@article{opper1991generalization,
	Author = {Opper, Manfred and Haussler, David},
	Journal = {Physical Review Letters},
	Number = {20},
	Pages = {2677},
	Publisher = {APS},
	Title = {Generalization performance of Bayes optimal classification algorithm for learning a perceptron},
	Volume = {66},
	Year = {1991}}

@book{sethna2006statistical,
	Author = {Sethna, James and others},
	Publisher = {Oxford University Press},
	Title = {Statistical mechanics: entropy, order parameters, and complexity},
	Volume = {14},
	Year = {2006}}

@book{kardar2007statistical,
	Author = {Kardar, Mehran},
	Publisher = {Cambridge University Press},
	Title = {Statistical physics of particles},
	Year = {2007}}

@book{toledano1987landau,
	Author = {Toledano, Pierre and Toledano, Jean-claude},
	Publisher = {World Scientific Publishing Company},
	Title = {Landau Theory Of Phase Transitions, The: Application To Structural, Incommensurate, Magnetic And Liquid Crystal Systems},
	Volume = {3},
	Year = {1987}}

@article{imbrie1984lower,
	Author = {Imbrie, John Z},
	Journal = {Physical review letters},
	Number = {18},
	Pages = {1747},
	Publisher = {APS},
	Title = {Lower critical dimension of the random-field Ising model},
	Volume = {53},
	Year = {1984}}

@article{belanger1991random,
	Author = {Belanger, DP and Young, AP},
	Journal = {Journal of magnetism and magnetic materials},
	Number = {1-3},
	Pages = {272--291},
	Publisher = {Elsevier},
	Title = {The random field Ising model},
	Volume = {100},
	Year = {1991}}

@article{guerra2002central,
	Author = {Guerra, Francesco and Lucio Toninelli, Fabio},
	Journal = {Journal of Mathematical Physics},
	Number = {12},
	Pages = {6224--6237},
	Publisher = {American Institute of Physics},
	Title = {Central limit theorem for fluctuations in the high temperature region of the Sherrington--Kirkpatrick spin glass model},
	Volume = {43},
	Year = {2002}}

@article{Orlandini_2002,
	Abstract = {We discuss self-averaging of thermodynamic properties in some random lattice models. In particular, we investigate when self-averaging (in an almost sure sense) of the free energy implies self-averaging of the energy and heat capacity, and we discuss the connection between self-averaging in the almost sure sense, and self-averaging in an Lp sense. Under quite general conditions we show that the average of the finite size heat capacity converges to the second derivative of the limiting quenched average free energy. We consider the application of these ideas to the problem of adsorption of a random copolymer at a surface, and to some related systems.},
	Author = {E Orlandini and M C Tesi and S G Whittington},
	Doi = {10.1088/0305-4470/35/19/303},
	Journal = {Journal of Physics A: Mathematical and General},
	Month = {may},
	Number = {19},
	Pages = {4219--4227},
	Publisher = {{IOP} Publishing},
	Title = {Self-averaging in the statistical mechanics of some lattice models},
	Url = {https://doi.org/10.1088%2F0305-4470%2F35%2F19%2F303},
	Volume = {35},
	Year = 2002,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0305-4470%2F35%2F19%2F303},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4470/35/19/303}}

@misc{cugli2002dynamics,
	Archiveprefix = {arXiv},
	Author = {Leticia F. Cugliandolo},
	Arxivid = {arXiv:0210312},
	Title = {Dynamics of glassy systems},
	Year = {2002}}

@article{bouchaud1998out,
	Author = {Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and Kurchan, Jorge and Mezard, Marc},
	Journal = {Spin glasses and random fields},
	Pages = {161--223},
	Publisher = {World Scientific, Singapore},
	Title = {Out of equilibrium dynamics in spin-glasses and other glassy systems},
	Year = {1998}}

@article{franz1997phase,
	Author = {Franz, Silvio and Parisi, Giorgio},
	Journal = {Physical review letters},
	Number = {13},
	Pages = {2486},
	Publisher = {APS},
	Title = {Phase diagram of coupled glassy systems: A mean-field study},
	Volume = {79},
	Year = {1997}}

@article{biroli2001metastable,
	Author = {Biroli, Giulio and Kurchan, Jorge},
	Journal = {Physical Review E},
	Number = {1},
	Pages = {016101},
	Publisher = {APS},
	Title = {Metastable states in glassy systems},
	Volume = {64},
	Year = {2001}}

@article{franz2011analytical,
	Author = {Franz, Silvio and Semerjian, Guilhem and others},
	Journal = {Dynamical Heterogeneities in Glasses, Colloids, and Granular Media},
	Publisher = {Oxford University Press},
	Title = {Analytical approaches to time-and length scales in models of glasses},
	Volume = {407},
	Year = {2011}}

@article{Berthier11,
	Author = {Berthier, Ludovic and Biroli, Giulio},
	Doi = {10.1103/RevModPhys.83.587},
	Issue = {2},
	Journal = {Rev. Mod. Phys.},
	Month = {Jun},
	Numpages = {0},
	Pages = {587--645},
	Publisher = {American Physical Society},
	Title = {Theoretical perspective on the glass transition and amorphous materials},
	Url = {https://link.aps.org/doi/10.1103/RevModPhys.83.587},
	Volume = {83},
	Year = {2011},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/RevModPhys.83.587},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/RevModPhys.83.587}}

@article{mezard2000statistical,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio},
	Journal = {Journal of Physics: Condensed Matter},
	Number = {29},
	Pages = {6655},
	Publisher = {IOP Publishing},
	Title = {Statistical physics of structural glasses},
	Volume = {12},
	Year = {2000}}

@article{kirkpatrick1987stable,
	Author = {Kirkpatrick, TR and Wolynes, PG},
	Journal = {Physical Review B},
	Number = {16},
	Pages = {8552},
	Publisher = {APS},
	Title = {Stable and metastable states in mean-field Potts and structural glasses},
	Volume = {36},
	Year = {1987}}

@article{lubchenko2007theory,
	Author = {Lubchenko, Vassiliy and Wolynes, Peter G},
	Journal = {Annu. Rev. Phys. Chem.},
	Pages = {235--266},
	Publisher = {Annual Reviews},
	Title = {Theory of structural glasses and supercooled liquids},
	Volume = {58},
	Year = {2007}}

@article{charbonneau2014fractal,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Journal = {Nature communications},
	Number = {1},
	Pages = {1--6},
	Publisher = {Nature Publishing Group},
	Title = {Fractal free energy landscapes in structural glasses},
	Volume = {5},
	Year = {2014}}

@misc{barbier2017adaptive,
	Archiveprefix = {arXiv},
	Author = {Jean Barbier and Nicolas Macris},
	Eprint = {1705.02780},
	Title = {The adaptive interpolation method: A simple scheme to prove replica formulas in Bayesian inference},
	Year = {2017}}

@book{fischer1993spin,
	Author = {Fischer, Konrad H and Hertz, John A},
	Publisher = {Cambridge university press},
	Title = {Spin glasses},
	Volume = {1},
	Year = {1993}}

@article{binder1986spin,
	Author = {Binder, Kurt and Young, A Peter},
	Journal = {Reviews of Modern physics},
	Number = {4},
	Pages = {801},
	Publisher = {APS},
	Title = {Spin glasses: Experimental facts, theoretical concepts, and open questions},
	Volume = {58},
	Year = {1986}}

@article{sompolinsky1982relaxational,
	Author = {Sompolinsky, Haim and Zippelius, Annette},
	Journal = {Physical Review B},
	Number = {11},
	Pages = {6860},
	Publisher = {APS},
	Title = {Relaxational dynamics of the Edwards-Anderson model and the mean-field theory of spin-glasses},
	Volume = {25},
	Year = {1982}}

@book{panchenko2013sherrington,
	Author = {Panchenko, Dmitry},
	Publisher = {Springer Science \& Business Media},
	Title = {The Sherrington-Kirkpatrick model},
	Year = {2013}}

@article{gardner1985spin,
	Author = {Gardner, Elisabeth},
	Journal = {Nuclear Physics B},
	Pages = {747--765},
	Publisher = {Elsevier},
	Title = {Spin glasses with p-spin interactions},
	Volume = {257},
	Year = {1985}}

@article{kirkpatrick1987p,
	Author = {Kirkpatrick, Theodore R and Thirumalai, Devarajan},
	Journal = {Physical Review B},
	Number = {10},
	Pages = {5388},
	Publisher = {APS},
	Title = {p-spin-interaction spin-glass models: Connections with the structural glass problem},
	Volume = {36},
	Year = {1987}}

@article{kirkpatrick1987dynamics,
	Author = {Kirkpatrick, Theodore R and Thirumalai, Devarajan},
	Journal = {Physical review letters},
	Number = {20},
	Pages = {2091},
	Publisher = {APS},
	Title = {Dynamics of the structural glass transition and the p-spin---interaction spin-glass model},
	Volume = {58},
	Year = {1987}}

@article{crisanti1995thouless,
	Author = {Crisanti, Andrea and Sommers, H-J},
	Journal = {Journal de Physique I},
	Number = {7},
	Pages = {805--813},
	Publisher = {EDP Sciences},
	Title = {Thouless-Anderson-Palmer approach to the spherical p-spin spin glass model},
	Volume = {5},
	Year = {1995}}

@article{crisanti1992sphericalp,
	Author = {Crisanti, Andrea and Sommers, H-J},
	Journal = {Zeitschrift f{\"u}r Physik B Condensed Matter},
	Number = {3},
	Pages = {341--354},
	Publisher = {Springer},
	Title = {The sphericalp-spin interaction spin glass model: the statics},
	Volume = {87},
	Year = {1992}}

@incollection{vincent2007ageing,
	Author = {Vincent, Eric},
	Booktitle = {Ageing and the glass transition},
	Pages = {7--60},
	Publisher = {Springer},
	Title = {Ageing, rejuvenation and memory: the example of spin-glasses},
	Year = {2007}}

@article{gauvin2009phase,
	Author = {Gauvin, Laetitia and Vannimenus, Jean and Nadal, J-P},
	Journal = {The European Physical Journal B},
	Number = {2},
	Pages = {293--304},
	Publisher = {Springer},
	Title = {Phase diagram of a Schelling segregation model},
	Volume = {70},
	Year = {2009}}

@article{mackay1996near,
	Author = {MacKay, David JC and Neal, Radford M},
	Journal = {Electronics letters},
	Number = {18},
	Pages = {1645--1646},
	Publisher = {[Stevenage, etc., Institution of Electrical Engineers]},
	Title = {Near Shannon limit performance of low density parity check codes},
	Volume = {32},
	Year = {1996}}

@article{kardar1986dynamic,
	Author = {Kardar, Mehran and Parisi, Giorgio and Zhang, Yi-Cheng},
	Journal = {Physical Review Letters},
	Number = {9},
	Pages = {889},
	Publisher = {APS},
	Title = {Dynamic scaling of growing interfaces},
	Volume = {56},
	Year = {1986}}

@book{jensen2011graph,
	Author = {Jensen, Tommy R and Toft, Bjarne},
	Publisher = {John Wiley \& Sons},
	Title = {Graph coloring problems},
	Volume = {39},
	Year = {2011}}

@article{mulet2002coloring,
	Author = {Mulet, Roberto and Pagnani, Andrea and Weigt, Martin and Zecchina, Riccardo},
	Journal = {Physical review letters},
	Number = {26},
	Pages = {268701},
	Publisher = {APS},
	Title = {Coloring random graphs},
	Volume = {89},
	Year = {2002}}

@article{krzakala2007gibbs,
	Author = {Krzaka{\l}a, Florent and Montanari, Andrea and Ricci-Tersenghi, Federico and Semerjian, Guilhem and Zdeborov{\'a}, Lenka},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {25},
	Pages = {10318--10323},
	Publisher = {National Acad Sciences},
	Title = {Gibbs states and the set of solutions of random constraint satisfaction problems},
	Volume = {104},
	Year = {2007}}

@article{mezard2005clustering,
	Author = {M{\'e}zard, Marc and Mora, Thierry and Zecchina, Riccardo},
	Journal = {Physical Review Letters},
	Number = {19},
	Pages = {197205},
	Publisher = {APS},
	Title = {Clustering of solutions in the random satisfiability problem},
	Volume = {94},
	Year = {2005}}

@article{ricci2001simplest,
	Author = {Ricci-Tersenghi, Federico and Weigt, Martin and Zecchina, Riccardo},
	Journal = {Physical Review E},
	Number = {2},
	Pages = {026702},
	Publisher = {APS},
	Title = {Simplest random k-satisfiability problem},
	Volume = {63},
	Year = {2001}}

@article{decelle2011asymptotic,
	Author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov{\'a}, Lenka},
	Journal = {Physical Review E},
	Number = {6},
	Pages = {066106},
	Publisher = {APS},
	Title = {Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications},
	Volume = {84},
	Year = {2011}}

@article{cugliandolo1994out,
	Author = {Cugliandolo, Leticia F and Kurchan, Jorge},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {17},
	Pages = {5749},
	Publisher = {IOP Publishing},
	Title = {On the out-of-equilibrium relaxation of the Sherrington-Kirkpatrick model},
	Volume = {27},
	Year = {1994}}

@article{talagrand1998sherrington,
	Author = {Talagrand, Michel},
	Journal = {Probability theory and related fields},
	Number = {2},
	Pages = {109--176},
	Publisher = {Springer},
	Title = {The Sherrington--Kirkpatrick model: A challenge for mathematicians},
	Volume = {110},
	Year = {1998}}

@article{mezard1992replica,
	Author = {M{\'e}zard, Marc and Young, Allan Peter},
	Journal = {EPL (Europhysics Letters)},
	Number = {7},
	Pages = {653},
	Publisher = {IOP Publishing},
	Title = {Replica symmetry breaking in the random field Ising model},
	Volume = {18},
	Year = {1992}}

@book{moore2011nature,
	Author = {Moore, Cristopher and Mertens, Stephan},
	Publisher = {OUP Oxford},
	Title = {The nature of computation},
	Year = {2011}}

@article{wilson1983renormalization,
	Author = {Wilson, Kenneth G},
	Journal = {Reviews of Modern Physics},
	Number = {3},
	Pages = {583},
	Publisher = {APS},
	Title = {The renormalization group and critical phenomena},
	Volume = {55},
	Year = {1983}}

@article{Onsager44,
	Author = {Onsager, Lars},
	Doi = {10.1103/PhysRev.65.117},
	Issue = {3-4},
	Journal = {Phys. Rev.},
	Month = {Feb},
	Numpages = {0},
	Pages = {117--149},
	Publisher = {American Physical Society},
	Title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
	Url = {https://link.aps.org/doi/10.1103/PhysRev.65.117},
	Volume = {65},
	Year = {1944},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRev.65.117},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRev.65.117}}

@article{haussler1996rigorous,
	Author = {Haussler, David and Kearns, Michael and Seung, H Sebastian and Tishby, Naftali},
	Journal = {Machine Learning},
	Number = {2-3},
	Pages = {195--236},
	Publisher = {Springer},
	Title = {Rigorous learning curve bounds from statistical mechanics},
	Volume = {25},
	Year = {1996}}

@book{grassberger2012statistical,
	Author = {Grassberger, Peter and Nadal, Jean-Pierre},
	Publisher = {Springer Science \& Business Media},
	Title = {From statistical physics to statistical inference and back},
	Volume = {428},
	Year = {2012}}

@book{nishimori2001statistical,
	Author = {Nishimori, Hidetoshi},
	Number = {111},
	Publisher = {Clarendon Press},
	Title = {Statistical physics of spin glasses and information processing: an introduction},
	Year = {2001}}

@article{rosenblatt1958perceptron,
	Author = {Rosenblatt, Frank},
	Journal = {Psychological review},
	Number = {6},
	Pages = {386},
	Publisher = {American Psychological Association},
	Title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	Volume = {65},
	Year = {1958}}

@incollection{turing2009computing,
	Author = {Turing, Alan M},
	Booktitle = {Parsing the turing test},
	Pages = {23--65},
	Publisher = {Springer},
	Title = {Computing machinery and intelligence},
	Year = {2009}}

@book{minsky2017perceptrons,
	Author = {Minsky, Marvin and Papert, Seymour A},
	Publisher = {MIT press},
	Title = {Perceptrons: An introduction to computational geometry},
	Year = {1969}}

@article{mcculloch1943logical,
	Author = {McCulloch, Warren S and Pitts, Walter},
	Journal = {The bulletin of mathematical biophysics},
	Number = {4},
	Pages = {115--133},
	Publisher = {Springer},
	Title = {A logical calculus of the ideas immanent in nervous activity},
	Volume = {5},
	Year = {1943}}

@techreport{dreyfus1965alchemy,
	Author = {Dreyfus, Hubert L},
	Date-Modified = {2020-09-15 20:42:59 +0000},
	Title = {Alchemy and artificial intelligence},
	Year = {1965}}

@inproceedings{lighthill1973artificial,
	Author = {Lighthill, James},
	Booktitle = {Artificial Intelligence: a paper symposium},
	Organization = {Science Research Council London},
	Pages = {1--21},
	Title = {Artificial intelligence: A general survey},
	Year = {1973}}

@article{rumelhart1986learning,
	Author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	Journal = {nature},
	Number = {6088},
	Pages = {533--536},
	Publisher = {Nature Publishing Group},
	Title = {Learning representations by back-propagating errors},
	Volume = {323},
	Year = {1986}}

@article{burges1998tutorial,
	Author = {Burges, Christopher JC},
	Journal = {Data mining and knowledge discovery},
	Number = {2},
	Pages = {121--167},
	Publisher = {Springer},
	Title = {A tutorial on support vector machines for pattern recognition},
	Volume = {2},
	Year = {1998}}

@article{cortes1995support,
	Author = {Cortes, Corinna and Vapnik, Vladimir},
	Journal = {Machine learning},
	Number = {3},
	Pages = {273--297},
	Publisher = {Springer},
	Title = {Support-vector networks},
	Volume = {20},
	Year = {1995}}

@article{hochreiter1997long,
	Author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	Journal = {Neural computation},
	Number = {8},
	Pages = {1735--1780},
	Publisher = {MIT Press},
	Title = {Long short-term memory},
	Volume = {9},
	Year = {1997}}

@article{lecun1989backpropagation,
	Author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
	Journal = {Neural computation},
	Number = {4},
	Pages = {541--551},
	Publisher = {MIT Press},
	Title = {Backpropagation applied to handwritten zip code recognition},
	Volume = {1},
	Year = {1989}}

@book{sejnowski2018deep,
	Author = {Sejnowski, Terrence J},
	Publisher = {Mit Press},
	Title = {The deep learning revolution},
	Year = {2018}}

@book{skansi2018introduction,
	Author = {Skansi, Sandro},
	Publisher = {Springer},
	Title = {Introduction to Deep Learning: from logical calculus to artificial intelligence},
	Year = {2018}}

@incollection{hecht1989neurocomputer,
	Author = {Hecht-Nielsen, Robert},
	Booktitle = {Neural computers},
	Pages = {445--453},
	Publisher = {Springer},
	Title = {Neurocomputer applications},
	Year = {1989}}

@article{bobrow1964natural,
	Author = {Bobrow, Daniel G},
	Title = {Natural language input for a computer problem solving system},
	Year = {1964}}

@inproceedings{evans1964heuristic,
	Author = {Evans, Thomas G},
	Booktitle = {Proceedings of the April 21-23, 1964, spring joint computer conference},
	Pages = {327--338},
	Title = {A heuristic program to solve geometric-analogy problems},
	Year = {1964}}

@article{quillian1969teachable,
	Author = {Quillian, M Ross},
	Journal = {Communications of the ACM},
	Number = {8},
	Pages = {459--476},
	Publisher = {ACM New York, NY, USA},
	Title = {The teachable language comprehender: A simulation program and theory of language},
	Volume = {12},
	Year = {1969}}

@article{kohonen1977principle,
	Author = {Kohonen, T and Lehti{\"o}, P and Rovamo, J and Hyv{\"a}rinen, J and Bry, K and Vainio, L},
	Journal = {Neuroscience},
	Number = {6},
	Pages = {1065--1076},
	Publisher = {Elsevier},
	Title = {A principle of neural associative memory},
	Volume = {2},
	Year = {1977}}

@article{grossberg1976adaptive,
	Author = {Grossberg, Stephen},
	Journal = {Biological cybernetics},
	Number = {3},
	Pages = {121--134},
	Publisher = {Springer},
	Title = {Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors},
	Volume = {23},
	Year = {1976}}

@book{dreyfus1984intelligence,
	Author = {Dreyfus, Hubert L and Vassallo-Villaneau, Rose-Marie and Andler, Daniel and Perriault, Jacques and Arsac, Jacques and Borillo, Mario},
	Publisher = {Flammarion Paris},
	Title = {Intelligence artificielle: mythes et limites},
	Volume = {198},
	Year = {1984}}

@article{rumelhart1986general,
	Author = {Rumelhart, David E and Hinton, Geoffrey E and McClelland, James L and others},
	Journal = {Parallel distributed processing: Explorations in the microstructure of cognition},
	Number = {45-76},
	Pages = {26},
	Publisher = {Cambridge, MA: MIT Press},
	Title = {A general framework for parallel distributed processing},
	Volume = {1},
	Year = {1986}}

@book{ganascia1993intelligence,
	Author = {Ganascia, Jean-Gabriel},
	Publisher = {Flammarion Paris},
	Title = {L'intelligence artificielle},
	Year = {1993}}

@book{lazard2016histoire,
	Author = {Lazard, Emmanuel and Mounier-Kuhn, Pierre-Eric},
	Publisher = {EDP sciences},
	Title = {Histoire illustr{\'e}e de l'infor matique},
	Year = {2016}}


@article{lecun1998gradient,
	Author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	Journal = {Proceedings of the IEEE},
	Number = {11},
	Pages = {2278--2324},
	Publisher = {Ieee},
	Title = {Gradient-based learning applied to document recognition},
	Volume = {86},
	Year = {1998}}

@article{hopfield1982neural,
	Author = {Hopfield, John J},
	Journal = {Proceedings of the national academy of sciences},
	Number = {8},
	Pages = {2554--2558},
	Publisher = {National Acad Sciences},
	Title = {Neural networks and physical systems with emergent collective computational abilities},
	Volume = {79},
	Year = {1982}}

@inproceedings{tramel2016inferring,
	Author = {Tramel, Eric W and Manoel, Andre and Caltagirone, Francesco and Gabri{\'e}, Marylou and Krzakala, Florent},
	Booktitle = {2016 IEEE Information Theory Workshop (ITW)},
	Organization = {IEEE},
	Pages = {265--269},
	Title = {Inferring sparsity: Compressed sensing using generalized restricted Boltzmann machines},
	Year = {2016}}

@article{cugliandolo1993analytical,
	Author = {Cugliandolo, Leticia F and Kurchan, Jorge},
	Journal = {Physical Review Letters},
	Number = {1},
	Pages = {173},
	Publisher = {APS},
	Title = {Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model},
	Volume = {71},
	Year = {1993}}

@article{arous2006cugliandolo,
	Author = {{Ben Arous}, Gerard and Dembo, Amir and Guionnet, Alice},
	Date-Modified = {2020-09-15 19:54:03 +0000},
	Journal = {Probability theory and related fields},
	Number = {4},
	Pages = {619--660},
	Publisher = {Springer},
	Title = {Cugliandolo-Kurchan equations for dynamics of spin-glasses},
	Volume = {136},
	Year = {2006}}

@article{mannelli2020marvels,
	Author = {Mannelli, Stefano Sarao and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
	Journal = {Physical Review X},
	Number = {1},
	Pages = {011057},
	Publisher = {APS},
	Title = {Marvels and pitfalls of the langevin algorithm in noisy high-dimensional inference},
	Volume = {10},
	Year = {2020}}

@article{mignacco2020dynamical,
	Author = {Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
	Journal = {arXiv preprint arXiv:2006.06098},
	Title = {Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification},
	Year = {2020}}


@article{amit1985spin,
	Author = {Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
	Journal = {Physical Review A},
	Number = {2},
	Pages = {1007},
	Publisher = {APS},
	Title = {Spin-glass models of neural networks},
	Volume = {32},
	Year = {1985}}

@article{amit1985storing,
	Author = {Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
	Journal = {Physical Review Letters},
	Number = {14},
	Pages = {1530},
	Publisher = {APS},
	Title = {Storing infinite numbers of patterns in a spin-glass model of neural networks},
	Volume = {55},
	Year = {1985}}

@article{amit1987statistical,
	Author = {Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
	Journal = {Annals of physics},
	Number = {1},
	Pages = {30--67},
	Publisher = {Elsevier},
	Title = {Statistical mechanics of neural networks near saturation},
	Volume = {173},
	Year = {1987}}

@article{ackley1985learning,
	Author = {Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
	Journal = {Cognitive science},
	Number = {1},
	Pages = {147--169},
	Publisher = {Elsevier},
	Title = {A learning algorithm for Boltzmann machines},
	Volume = {9},
	Year = {1985}}

@article{gardner1987maximum,
	Author = {Gardner, Elizabeth},
	Journal = {EPL (Europhysics Letters)},
	Number = {4},
	Pages = {481},
	Publisher = {IOP Publishing},
	Title = {Maximum storage capacity in neural networks},
	Volume = {4},
	Year = {1987}}

@article{derrida1987exactly,
	Author = {Derrida, Bernard and Gardner, Elizabeth and Zippelius, Anne},
	Journal = {EPL (Europhysics Letters)},
	Number = {2},
	Pages = {167},
	Publisher = {IOP Publishing},
	Title = {An exactly solvable asymmetric neural network model},
	Volume = {4},
	Year = {1987}}

@article{gardner1989three,
	Author = {Gardner, Elizabeth and Derrida, Bernard},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {12},
	Pages = {1983},
	Publisher = {IOP Publishing},
	Title = {Three unfinished works on the optimal storage capacity of networks},
	Volume = {22},
	Year = {1989}}

@inproceedings{boser1992training,
	Author = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
	Booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
	Pages = {144--152},
	Title = {A training algorithm for optimal margin classifiers},
	Year = {1992}}

@article{krauth1987learning,
	Author = {Krauth, Werner and M{\'e}zard, Marc},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {11},
	Pages = {L745},
	Publisher = {IOP Publishing},
	Title = {Learning algorithms with optimal stability in neural networks},
	Volume = {20},
	Year = {1987}}

@article{krauth1989storage,
	Author = {Krauth, Werner and M{\'e}zard, Marc},
	Journal = {Journal de Physique},
	Number = {20},
	Pages = {3057--3066},
	Publisher = {Soci{\'e}t{\'e} fran{\c{c}}aise de physique},
	Title = {Storage capacity of memory networks with binary couplings},
	Volume = {50},
	Year = {1989}}

@inproceedings{tishby2015deep,
	Author = {Tishby, Naftali and Zaslavsky, Noga},
	Booktitle = {2015 IEEE Information Theory Workshop (ITW)},
	Organization = {IEEE},
	Pages = {1--5},
	Title = {Deep learning and the information bottleneck principle},
	Year = {2015}}

@article{agoritsas2018out,
	Author = {Agoritsas, Elisabeth and Biroli, Giulio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {8},
	Pages = {085002},
	Publisher = {IOP Publishing},
	Title = {Out-of-equilibrium dynamical mean-field equations for the perceptron model},
	Volume = {51},
	Year = {2018}}

@article{valiant1984theory,
	Author = {Valiant, Leslie G},
	Journal = {Communications of the ACM},
	Number = {11},
	Pages = {1134--1142},
	Publisher = {ACM New York, NY, USA},
	Title = {A theory of the learnable},
	Volume = {27},
	Year = {1984}}

@book{mcclelland1989explorations,
	Author = {McClelland, James L and Rumelhart, David E},
	Publisher = {MIT press},
	Title = {Explorations in parallel distributed processing: A handbook of models, programs, and exercises},
	Year = {1989}}

@article{gardner1988space,
	Author = {Gardner, Elizabeth},
	Journal = {Journal of physics A: Mathematical and general},
	Number = {1},
	Pages = {257},
	Publisher = {IOP Publishing},
	Title = {The space of interactions in neural network models},
	Volume = {21},
	Year = {1988}}

@article{guerra2002thermodynamic,
	Author = {Guerra, Francesco and Toninelli, Fabio Lucio},
	Journal = {Communications in Mathematical Physics},
	Number = {1},
	Pages = {71--79},
	Publisher = {Springer},
	Title = {The thermodynamic limit in mean field spin glass models},
	Volume = {230},
	Year = {2002}}

@article{vapnik1994measuring,
	Author = {Vapnik, Vladimir and Levin, Esther and Cun, Yann Le},
	Journal = {Neural computation},
	Number = {5},
	Pages = {851--876},
	Publisher = {MIT Press},
	Title = {Measuring the VC-dimension of a learning machine},
	Volume = {6},
	Year = {1994}}

@article{mehta2014exact,
	Author = {Mehta, Pankaj and Schwab, David J},
	Journal = {arXiv preprint arXiv:1410.3831},
	Title = {An exact mapping between the variational renormalization group and deep learning},
	Year = {2014}}

@article{advani2017high,
	Author = {Advani, Madhu S and Saxe, Andrew M},
	Journal = {arXiv preprint arXiv:1710.03667},
	Title = {High-dimensional dynamics of generalization error in neural networks},
	Year = {2017}}

@misc{montanari2012graphical,
	Author = {Montanari, Andrea},
	Title = {Graphical models concepts in compressed sensing.},
	Year = {2012}}

@article{donoho2013information,
	Author = {Donoho, David L and Javanmard, Adel and Montanari, Andrea},
	Journal = {IEEE transactions on information theory},
	Number = {11},
	Pages = {7434--7464},
	Publisher = {IEEE},
	Title = {Information-theoretically optimal compressed sensing via spatial coupling and approximate message passing},
	Volume = {59},
	Year = {2013}}

@article{maleki2011approximate,
	Author = {Maleki, Arian},
	Journal = {a degree of Doctor of Philosophy, Stanford University},
	Publisher = {Citeseer},
	Title = {Approximate message passing algorithms for compressed sensing},
	Year = {2011}}

@inproceedings{yedidia2001generalized,
	Author = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
	Booktitle = {Advances in neural information processing systems},
	Pages = {689--695},
	Title = {Generalized belief propagation},
	Year = {2001}}

@article{yedidia2003understanding,
	Author = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
	Journal = {Exploring artificial intelligence in the new millennium},
	Pages = {236--239},
	Title = {Understanding belief propagation and its generalizations},
	Volume = {8},
	Year = {2003}}

@article{breiman1995reflections,
	Author = {Breiman, Leo},
	Journal = {The Mathematics of Generalization},
	Pages = {11--15},
	Publisher = {Addison Wesley Reading, MA},
	Title = {Reflections after refereeing papers for NIPS},
	Year = {1995}}

@book{barber2012bayesian,
	Author = {Barber, David},
	Publisher = {Cambridge University Press},
	Title = {Bayesian reasoning and machine learning},
	Year = {2012}}

@book{mackay2003information,
	Author = {MacKay, David JC and Mac Kay, David JC},
	Publisher = {Cambridge university press},
	Title = {Information theory, inference and learning algorithms},
	Year = {2003}}

@article{blum1992training,
	Author = {Blum, Avrim L and Rivest, Ronald L},
	Journal = {Neural Networks},
	Number = {1},
	Pages = {117--127},
	Publisher = {Elsevier},
	Title = {Training a 3-node neural network is NP-complete},
	Volume = {5},
	Year = {1992}}

@inproceedings{jacot2018neural,
	Author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	Booktitle = {Advances in neural information processing systems},
	Pages = {8571--8580},
	Title = {Neural tangent kernel: Convergence and generalization in neural networks},
	Year = {2018}}

@inproceedings{chizat2018global,
	Author = {Chizat, Lenaic and Bach, Francis},
	Booktitle = {Advances in neural information processing systems},
	Pages = {3036--3046},
	Title = {On the global convergence of gradient descent for over-parameterized models using optimal transport},
	Year = {2018}}

@article{mei2018mean,
	Author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {33},
	Pages = {E7665--E7671},
	Publisher = {National Acad Sciences},
	Title = {A mean field view of the landscape of two-layer neural networks},
	Volume = {115},
	Year = {2018}}

@article{rotskoff2018neural,
	Author = {Rotskoff, Grant M and Vanden-Eijnden, Eric},
	Journal = {stat},
	Pages = {22},
	Title = {Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error},
	Volume = {1050},
	Year = {2018}}

@article{lecun2015deep,
	Author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	Journal = {nature},
	Number = {7553},
	Pages = {436--444},
	Publisher = {Nature Publishing Group},
	Title = {Deep learning},
	Volume = {521},
	Year = {2015}}

@article{ruder2016overview,
	Author = {Ruder, Sebastian},
	Journal = {arXiv preprint arXiv:1609.04747},
	Title = {An overview of gradient descent optimization algorithms},
	Year = {2016}}

@incollection{bottou2012stochastic,
	Author = {Bottou, L{\'e}on},
	Booktitle = {Neural networks: Tricks of the trade},
	Pages = {421--436},
	Publisher = {Springer},
	Title = {Stochastic gradient descent tricks},
	Year = {2012}}

@inproceedings{biehl2019statistical,
  title={Statistical physics of learning and inference.},
  author={Biehl, Michael and Caticha, Nestor and Opper, Manfred and Villmann, Thomas},
  booktitle={ESANN},
  year={2019}
}


@article{opper2001learning,
	Author = {Opper, Manfred},
	Title = {Learning to generalize}}

@book{tsang2014foundations,
	Author = {Tsang, Edward},
	Publisher = {BoD--Books on Demand},
	Title = {Foundations of constraint satisfaction: the classic text},
	Year = {2014}}

@book{apt2003principles,
	Author = {Apt, Krzysztof},
	Publisher = {Cambridge university press},
	Title = {Principles of constraint programming},
	Year = {2003}}

@article{mezard1986replica,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio},
	Journal = {Journal de Physique},
	Number = {8},
	Pages = {1285--1296},
	Publisher = {Soci{\'e}t{\'e} fran{\c{c}}aise de physique},
	Title = {A replica analysis of the travelling salesman problem},
	Volume = {47},
	Year = {1986}}

@article{mezard1986mean,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio},
	Journal = {EPL (Europhysics Letters)},
	Number = {12},
	Pages = {913},
	Publisher = {IOP Publishing},
	Title = {Mean-field equations for the matching and the travelling salesman problems},
	Volume = {2},
	Year = {1986}}

@article{mezard2002random,
	Author = {M{\'e}zard, Marc and Zecchina, Riccardo},
	Journal = {Physical Review E},
	Number = {5},
	Pages = {056126},
	Publisher = {APS},
	Title = {Random k-satisfiability problem: From an analytic solution to an efficient algorithm},
	Volume = {66},
	Year = {2002}}

@article{decelle2011inference,
	Author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov{\'a}, Lenka},
	Journal = {Physical Review Letters},
	Number = {6},
	Pages = {065701},
	Publisher = {APS},
	Title = {Inference and phase transitions in the detection of modules in sparse networks},
	Volume = {107},
	Year = {2011}}

@article{zdeborova2007phase,
	Author = {Zdeborov{\'a}, Lenka and Krz{\k{a}}ka{\l}a, Florent},
	Journal = {Physical Review E},
	Number = {3},
	Pages = {031131},
	Publisher = {APS},
	Title = {Phase transitions in the coloring of random graphs},
	Volume = {76},
	Year = {2007}}

@article{krzakala2012probabilistic,
	Author = {Krzakala, Florent and M{\'e}zard, Marc and Sausset, Francois and Sun, Yifan and Zdeborov{\'a}, Lenka},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {08},
	Pages = {P08009},
	Publisher = {IOP Publishing},
	Title = {Probabilistic reconstruction in compressed sensing: algorithms, phase diagrams, and threshold achieving matrices},
	Volume = {2012},
	Year = {2012}}

@article{gallager1962low,
	Author = {Gallager, Robert},
	Journal = {IRE Transactions on information theory},
	Number = {1},
	Pages = {21--28},
	Publisher = {IEEE},
	Title = {Low-density parity-check codes},
	Volume = {8},
	Year = {1962}}

@article{bethe1935statistical,
	Author = {Bethe, Hans A},
	Journal = {Proceedings of the Royal Society of London. Series A-Mathematical and Physical Sciences},
	Number = {871},
	Pages = {552--575},
	Publisher = {The Royal Society London},
	Title = {Statistical theory of superlattices},
	Volume = {150},
	Year = {1935}}

@article{peierls1936statistical,
	Author = {Peierls, Rudolf},
	Journal = {Proceedings of the Royal Society of London. Series A-Mathematical and Physical Sciences},
	Number = {881},
	Pages = {207--222},
	Publisher = {The Royal Society London},
	Title = {Statistical theory of superlattices with unequal concentrations of the components},
	Volume = {154},
	Year = {1936}}

@article{charbonneau2017glass,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Journal = {Annual Review of Condensed Matter Physics},
	Pages = {265--288},
	Publisher = {Annual Reviews},
	Title = {Glass and jamming transitions: From exact results to finite-dimensional descriptions},
	Volume = {8},
	Year = {2017}}

@article{parisi2010mean,
	Author = {Parisi, Giorgio and Zamponi, Francesco},
	Journal = {Reviews of Modern Physics},
	Number = {1},
	Pages = {789},
	Publisher = {APS},
	Title = {Mean-field theory of hard sphere glasses and jamming},
	Volume = {82},
	Year = {2010}}

@article{semerjian2008freezing,
	Author = {Semerjian, Guilhem},
	Journal = {Journal of Statistical Physics},
	Number = {2},
	Pages = {251--293},
	Publisher = {Springer},
	Title = {On the freezing of variables in random constraint satisfaction problems},
	Volume = {130},
	Year = {2008}}

@article{shannon1948mathematical,
	Author = {Shannon, Claude E},
	Journal = {The Bell system technical journal},
	Number = {3},
	Pages = {379--423},
	Publisher = {Nokia Bell Labs},
	Title = {A mathematical theory of communication},
	Volume = {27},
	Year = {1948}}

@article{sourlas1989spin,
	Author = {Sourlas, Nicolas},
	Journal = {Nature},
	Number = {6227},
	Pages = {693--695},
	Publisher = {Springer},
	Title = {Spin-glass models as error-correcting codes},
	Volume = {339},
	Year = {1989}}

@article{mezard2001bethe,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio},
	Journal = {The European Physical Journal B-Condensed Matter and Complex Systems},
	Number = {2},
	Pages = {217--233},
	Publisher = {Springer},
	Title = {The Bethe lattice spin glass revisited},
	Volume = {20},
	Year = {2001}}

@article{mezard2003cavity,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio},
	Journal = {Journal of Statistical Physics},
	Number = {1-2},
	Pages = {1--34},
	Publisher = {Springer},
	Title = {The cavity method at zero temperature},
	Volume = {111},
	Year = {2003}}

@article{gordon1985some,
	Author = {Gordon, Yehoram},
	Journal = {Israel Journal of Mathematics},
	Number = {4},
	Pages = {265--289},
	Publisher = {Springer},
	Title = {Some inequalities for Gaussian processes and applications},
	Volume = {50},
	Year = {1985}}

@book{bremaud2017discrete,
	Author = {Br{\'e}maud, Pierre},
	Publisher = {Springer},
	Title = {Discrete probability models and methods},
	Volume = {78},
	Year = {2017}}

@article{franco1983probabilistic,
	Author = {Franco, John and Paull, Marvin},
	Journal = {Discrete Applied Mathematics},
	Number = {1},
	Pages = {77--87},
	Publisher = {Elsevier},
	Title = {Probabilistic analysis of the Davis Putnam procedure for solving the satisfiability problem},
	Volume = {5},
	Year = {1983}}

@article{monasson1999determining,
	Author = {Monasson, R{\'e}mi and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
	Journal = {Nature},
	Number = {6740},
	Pages = {133--137},
	Publisher = {Nature Publishing Group},
	Title = {Determining computational complexity from characteristic `phase transitions'},
	Volume = {400},
	Year = {1999}}

@book{arora2009computational,
	Author = {Arora, Sanjeev and Barak, Boaz},
	Publisher = {Cambridge University Press},
	Title = {Computational complexity: a modern approach},
	Year = {2009}}

@book{percus2006computational,
	Author = {Percus, Allon and Istrate, Gabriel and Moore, Cristopher},
	Publisher = {OUP USA},
	Title = {Computational complexity and statistical physics},
	Year = {2006}}

@article{NatureZdeborova,
	Abstract = {Automated learning from data by means of deep neural networks is finding use in an ever-increasing number of applications, yet key theoretical questions about how it works remain unanswered. A physics-based approach may help to bridge this gap.},
	Author = {Zdeborov{\'a}, Lenka},
	Da = {2020/06/01},
	Date-Added = {2020-07-20 09:21:27 +0000},
	Date-Modified = {2020-07-20 09:21:27 +0000},
	Doi = {10.1038/s41567-020-0929-2},
	Id = {Zdeborov{\'a}2020},
	Isbn = {1745-2481},
	Journal = {Nature Physics},
	Number = {6},
	Pages = {602--604},
	Title = {Understanding deep learning is also a job for physicists},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41567-020-0929-2},
	Volume = {16},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41567-020-0929-2},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/s41567-020-0929-2}}

@article{zdeborova2017machine,
	Author = {Zdeborov{\'a}, Lenka},
	Journal = {Nature Physics},
	Number = {5},
	Pages = {420--421},
	Publisher = {Nature Publishing Group},
	Title = {Machine learning: New tool in the box},
	Volume = {13},
	Year = {2017}}

@book{murphy2012machine,
	Author = {Murphy, Kevin P},
	Publisher = {MIT press},
	Title = {Machine learning: a probabilistic perspective},
	Year = {2012}}

@article{hutchins2001machine,
	Author = {Hutchins, W John},
	Journal = {Histoire epist{\'e}mologie langage},
	Number = {1},
	Pages = {7--31},
	Publisher = {Soci{\'e}t{\'e} d'Histoire et d'{\'E}pist{\'e}mologie des Sciences du Langage},
	Title = {Machine translation over fifty years},
	Volume = {23},
	Year = {2001}}

@article{wu2016google,
	Author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
	Journal = {arXiv preprint arXiv:1609.08144},
	Title = {Google's neural machine translation system: Bridging the gap between human and machine translation},
	Year = {2016}}

@article{oord2016wavenet,
	Author = {Oord, Avd and Dieleman, S and Zen, H and Simonyan, K and Vinyals, O and Graves, A and Kalchbrenner, N and Senior, A and Kavukcuoglu, K},
	Journal = {arXiv preprint arXiv:1609.03499},
	Title = {Wavenet: A generative model for raw audio. arXiv 2016},
	Year = {2016}}

@inproceedings{chung2017lip,
	Author = {Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
	Booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Organization = {IEEE},
	Pages = {3444--3453},
	Title = {Lip reading sentences in the wild},
	Year = {2017}}

@inproceedings{santoro2017simple,
	Author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
	Booktitle = {Advances in neural information processing systems},
	Pages = {4967--4976},
	Title = {A simple neural network module for relational reasoning},
	Year = {2017}}

@article{reed2016generative,
	Author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	Journal = {arXiv preprint arXiv:1605.05396},
	Title = {Generative adversarial text to image synthesis},
	Year = {2016}}

@inproceedings{isola2017image,
	Author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {1125--1134},
	Title = {Image-to-image translation with conditional adversarial networks},
	Year = {2017}}

@inproceedings{ulyanov2018deep,
	Author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	Pages = {9446--9454},
	Title = {Deep image prior},
	Year = {2018}}

@article{madry2017towards,
	Author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	Journal = {arXiv preprint arXiv:1706.06083},
	Title = {Towards deep learning models resistant to adversarial attacks},
	Year = {2017}}

@inproceedings{taigman2014deepface,
	Author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {1701--1708},
	Title = {Deepface: Closing the gap to human-level performance in face verification},
	Year = {2014}}

@article{parkhi2015deep,
	Author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
	Publisher = {British Machine Vision Association},
	Title = {Deep face recognition},
	Year = {2015}}

@inproceedings{zhu2017unpaired,
	Author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
	Booktitle = {Proceedings of the IEEE international conference on computer vision},
	Pages = {2223--2232},
	Title = {Unpaired image-to-image translation using cycle-consistent adversarial networks},
	Year = {2017}}

@inproceedings{vaswani2017attention,
	Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	Booktitle = {Advances in neural information processing systems},
	Pages = {5998--6008},
	Title = {Attention is all you need},
	Year = {2017}}

@inproceedings{sutton2000policy,
	Author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
	Booktitle = {Advances in neural information processing systems},
	Pages = {1057--1063},
	Title = {Policy gradient methods for reinforcement learning with function approximation},
	Year = {2000}}

@article{mnih2013playing,
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	Journal = {arXiv preprint arXiv:1312.5602},
	Title = {Playing atari with deep reinforcement learning},
	Year = {2013}}

@article{silver2016mastering,
	Author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	Journal = {nature},
	Number = {7587},
	Pages = {484--489},
	Publisher = {Nature Publishing Group},
	Title = {Mastering the game of Go with deep neural networks and tree search},
	Volume = {529},
	Year = {2016}}

@article{campbell2002deep,
	Author = {Campbell, Murray and Hoane Jr, A Joseph and Hsu, Feng-hsiung},
	Journal = {Artificial intelligence},
	Number = {1-2},
	Pages = {57--83},
	Publisher = {Elsevier},
	Title = {Deep blue},
	Volume = {134},
	Year = {2002}}

@inproceedings{sutskever2014sequence,
	Author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	Booktitle = {Advances in neural information processing systems},
	Pages = {3104--3112},
	Title = {Sequence to sequence learning with neural networks},
	Year = {2014}}

@inproceedings{graves2013speech,
	Author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	Booktitle = {2013 IEEE international conference on acoustics, speech and signal processing},
	Organization = {IEEE},
	Pages = {6645--6649},
	Title = {Speech recognition with deep recurrent neural networks},
	Year = {2013}}

@article{collobert2011natural,
	Author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	Journal = {Journal of machine learning research},
	Number = {ARTICLE},
	Pages = {2493--2537},
	Title = {Natural language processing (almost) from scratch},
	Volume = {12},
	Year = {2011}}

@article{donoho2006compressed,
	Author = {Donoho, David L},
	Journal = {IEEE Transactions on information theory},
	Number = {4},
	Pages = {1289--1306},
	Publisher = {IEEE},
	Title = {Compressed sensing},
	Volume = {52},
	Year = {2006}}

@article{caruana1997multitask,
	Author = {Caruana, Rich},
	Journal = {Machine learning},
	Number = {1},
	Pages = {41--75},
	Publisher = {Springer},
	Title = {Multitask learning},
	Volume = {28},
	Year = {1997}}

@article{pan2010survey,
	Author = {Pan, Sinno Jialin and Yang, Qiang},
	Journal = {IEEE Transactions on knowledge and data engineering},
	Number = {10},
	Pages = {1345--1359},
	Publisher = {IEEE},
	Title = {A survey on transfer learning},
	Volume = {22},
	Year = {2010}}

@incollection{jolliffe1986principal,
	Author = {Jolliffe, Ian T},
	Booktitle = {Principal component analysis},
	Pages = {129--155},
	Publisher = {Springer},
	Title = {Principal components in regression analysis},
	Year = {1986}}

@book{kaufman2009finding,
	Author = {Kaufman, Leonard and Rousseeuw, Peter J},
	Publisher = {John Wiley \& Sons},
	Title = {Finding groups in data: an introduction to cluster analysis},
	Volume = {344},
	Year = {2009}}

@inproceedings{goodfellow2014generative,
	Author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	Booktitle = {Advances in neural information processing systems},
	Pages = {2672--2680},
	Title = {Generative adversarial nets},
	Year = {2014}}

@article{kingma2013auto,
	Author = {Kingma, Diederik P and Welling, Max},
	Journal = {arXiv preprint arXiv:1312.6114},
	Title = {Auto-encoding variational bayes},
	Year = {2013}}

@article{rezende2014stochastic,
	Author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	Journal = {arXiv preprint arXiv:1401.4082},
	Title = {Stochastic backpropagation and approximate inference in deep generative models},
	Year = {2014}}

@book{sutton1998introduction,
	Author = {Sutton, Richard S and Barto, Andrew G and others},
	Publisher = {MIT press Cambridge},
	Title = {Introduction to reinforcement learning},
	Volume = {135},
	Year = {1998}}

@article{vincent2010stacked,
	Author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
	Journal = {Journal of machine learning research},
	Number = {12},
	Title = {Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
	Volume = {11},
	Year = {2010}}

@article{tishby2000information,
	Author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
	Journal = {arXiv preprint physics/0004057},
	Title = {The information bottleneck method},
	Year = {2000}}

@article{kingma2019introduction,
	Author = {Kingma, Diederik P and Welling, Max},
	Journal = {arXiv preprint arXiv:1906.02691},
	Title = {An introduction to variational autoencoders},
	Year = {2019}}

@article{doersch2016tutorial,
	Author = {Doersch, Carl},
	Journal = {arXiv:1606.05908},
	Title = {Tutorial on variational autoencoders},
	Year = {2016}}

@article{blei2017variational,
	Author = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
	Journal = {Journal of the American statistical Association},
	Number = {518},
	Pages = {859--877},
	Publisher = {Taylor \& Francis},
	Title = {Variational inference: A review for statisticians},
	Volume = {112},
	Year = {2017}}

@book{cherkassky2007learning,
	Author = {Cherkassky, Vladimir and Mulier, Filip M},
	Publisher = {John Wiley \& Sons},
	Title = {Learning from data: concepts, theory, and methods},
	Year = {2007}}

@book{tsybakov2008introduction,
	Author = {Tsybakov, Alexandre B},
	Publisher = {Springer Science \& Business Media},
	Title = {Introduction to nonparametric estimation},
	Year = {2008}}

@incollection{vapnik2015uniform,
	Author = {Vapnik, Vladimir N and Chervonenkis, A Ya},
	Booktitle = {Measures of complexity},
	Pages = {11--30},
	Publisher = {Springer},
	Title = {On the uniform convergence of relative frequencies of events to their probabilities},
	Year = {2015}}

@article{blumer1989learnability,
	Author = {Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
	Journal = {Journal of the ACM (JACM)},
	Number = {4},
	Pages = {929--965},
	Publisher = {ACM New York, NY, USA},
	Title = {Learnability and the Vapnik-Chervonenkis dimension},
	Volume = {36},
	Year = {1989}}

@article{bayes1763lii,
	Author = {Bayes, Thomas},
	Journal = {Philosophical transactions of the Royal Society of London},
	Number = {53},
	Pages = {370--418},
	Publisher = {The Royal Society London},
	Title = {LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFR S},
	Year = {1763}}

@article{aldrich2008ra,
	Author = {Aldrich, John and others},
	Journal = {Bayesian Analysis},
	Number = {1},
	Pages = {161--170},
	Publisher = {International Society for Bayesian Analysis},
	Title = {RA Fisher on Bayes and Bayes' theorem},
	Volume = {3},
	Year = {2008}}

@inproceedings{krizhevsky2012imagenet,
	Author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	Booktitle = {Advances in neural information processing systems},
	Pages = {1097--1105},
	Title = {Imagenet classification with deep convolutional neural networks},
	Year = {2012}}

@inproceedings{he2016deep,
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {770--778},
	Title = {Deep residual learning for image recognition},
	Year = {2016}}

@incollection{lecun1999object,
	Author = {LeCun, Yann and Haffner, Patrick and Bottou, L{\'e}on and Bengio, Yoshua},
	Booktitle = {Shape, contour and grouping in computer vision},
	Pages = {319--345},
	Publisher = {Springer},
	Title = {Object recognition with gradient-based learning},
	Year = {1999}}

@article{schmidhuber2015deep,
	Author = {Schmidhuber, J{\"u}rgen},
	Journal = {Neural networks},
	Pages = {85--117},
	Publisher = {Elsevier},
	Title = {Deep learning in neural networks: An overview},
	Volume = {61},
	Year = {2015}}

@inproceedings{tishby89,
	Abstract = {The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.},
	Author = {Naftali Tishby and Esther Levin and Solla, {Sara A.}},
	Booktitle = {IJCNN Int Jt Conf Neural Network},
	Day = {1},
	Editor = {Anon},
	Language = {English (US)},
	Month = dec,
	Note = {IJCNN International Joint Conference on Neural Networks ; Conference date: 18-06-1989 Through 22-06-1989},
	Pages = {403--409},
	Publisher = {Publ by IEEE},
	Title = {Consistent inference of probabilities in layered networks: Predictions and generalization},
	Year = {1989}}

@article{Patarnello_1987,
	Abstract = {We realized a model which self-organizes to perform a task via a learning-by-example scheme. The system is a network of Boolean operators which, in some of our computations, has been able to achieve an error-free design for addition between integer binary numbers when shown only a small subset of all possible additions. The training procedure, based on optimizing the network on the given sampling using simulated annealing, is completely general and allows in principle to treat any binary mapping. We recognize different regimes in learning, i.e. the system can both memorize patterns (with a capacity which is numerically estimated) and generalize information to construct rules and algorithms. Some scaling relations are conjectured and numerically tested for these different regimes.},
	Author = {S Patarnello and P Carnevali},
	Doi = {10.1209/0295-5075/4/4/020},
	Journal = {Europhysics Letters ({EPL})},
	Month = {aug},
	Number = {4},
	Pages = {503--508},
	Publisher = {{IOP} Publishing},
	Title = {Learning Networks of Neurons with Boolean Logic},
	Url = {https://doi.org/10.1209%2F0295-5075%2F4%2F4%2F020},
	Volume = {4},
	Year = 1987,
	Bdsk-Url-1 = {https://doi.org/10.1209%2F0295-5075%2F4%2F4%2F020},
	Bdsk-Url-2 = {http://dx.doi.org/10.1209/0295-5075/4/4/020}}

@article{kingma2014adam,
	Author = {Kingma, Diederik P and Ba, Jimmy},
	Journal = {arXiv preprint arXiv:1412.6980},
	Title = {Adam: A method for stochastic optimization},
	Year = {2014}}

@article{zeiler2012adadelta,
	Author = {Zeiler, Matthew D},
	Journal = {arXiv preprint arXiv:1212.5701},
	Title = {Adadelta: an adaptive learning rate method},
	Year = {2012}}

@article{duchi2011adaptive,
	Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	Journal = {Journal of machine learning research},
	Number = {7},
	Title = {Adaptive subgradient methods for online learning and stochastic optimization.},
	Volume = {12},
	Year = {2011}}

@inproceedings{sutskever2013importance,
	Author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	Booktitle = {International conference on machine learning},
	Pages = {1139--1147},
	Title = {On the importance of initialization and momentum in deep learning},
	Year = {2013}}

@article{ioffe2015batch,
	Author = {Ioffe, Sergey and Szegedy, Christian},
	Journal = {arXiv preprint arXiv:1502.03167},
	Title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	Year = {2015}}

@article{craiu14,
	Abstract = { Markov chain Monte Carlo (MCMC) algorithms are an indispensable tool for performing Bayesian inference. This review discusses widely used sampling algorithms and illustrates their implementation on a probit regression model for lupus data. The examples considered highlight the importance of tuning the simulation parameters and underscore the important contributions of modern developments such as adaptive MCMC. We then use the theory underlying MCMC to explain the validity of the algorithms considered and to assess the variance of the resulting Monte Carlo estimators. },
	Author = {Craiu, Radu V. and Rosenthal, Jeffrey S.},
	Journal = {Annual Review of Statistics and Its Application},
	Number = {1},
	Pages = {179-201},
	Title = {Bayesian Computation Via Markov Chain Monte Carlo},
	Volume = {1},
	Year = {2014}
	}

@article{rupert13,
	Abstract = {{The posterior probability distribution for a set of model parameters encodes all that the data have to tell us in the context of a given model; it is the fundamental quantity for Bayesian parameter estimation. In order to infer the posterior probability distribution we have to decide how to explore parameter space. Here we compare three prescriptions for how parameter space is navigated, discussing their relative merits. We consider Metropolis--Hasting sampling, nested sampling and affine-invariant ensemble Markov chain Monte Carlo (MCMC) sampling. We focus on their performance on toy-model Gaussian likelihoods and on a real-world cosmological data set. We outline the sampling algorithms themselves and elaborate on performance diagnostics such as convergence time, scope for parallelization, dimensional scaling, requisite tunings and suitability for non-Gaussian distributions. We find that nested sampling delivers high-fidelity estimates for posterior statistics at low computational cost, and should be adopted in favour of Metropolis--Hastings in many cases. Affine-invariant MCMC is competitive when computing clusters can be utilized for massive parallelization. Affine-invariant MCMC and existing extensions to nested sampling naturally probe multimodal and curving distributions.}},
	Author = {Allison, Rupert and Dunkley, Joanna},
	Issn = {0035-8711},
	Journal = {Monthly Notices of the Royal Astronomical Society},
	Month = {12},
	Number = {4},
	Pages = {3918-3928},
	Title = {{Comparison of sampling techniques for Bayesian parameter estimation}},
	Volume = {437},
	Year = {2013}}

@article{andrieu2003introduction,
	Author = {Andrieu, Christophe and De Freitas, Nando and Doucet, Arnaud and Jordan, Michael I},
	Journal = {Machine learning},
	Number = {1-2},
	Pages = {5--43},
	Publisher = {Springer},
	Title = {An introduction to MCMC for machine learning},
	Volume = {50},
	Year = {2003}}

@article{bengio2013representation,
	Author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Number = {8},
	Pages = {1798--1828},
	Publisher = {IEEE},
	Title = {Representation learning: A review and new perspectives},
	Volume = {35},
	Year = {2013}}

@article{gabrie2020mean,
	Author = {Gabri{\'e}, Marylou},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {22},
	Pages = {223002},
	Publisher = {IOP Publishing},
	Title = {Mean-field inference methods for neural networks},
	Volume = {53},
	Year = {2020}}

@article{hansel1990learning,
	Author = {Hansel, David and Sompolinsky, Haim},
	Journal = {EPL (Europhysics Letters)},
	Number = {7},
	Pages = {687},
	Publisher = {IOP Publishing},
	Title = {Learning from examples in a single-layer neural network},
	Volume = {11},
	Year = {1990}}

@article{scholkopf1999input,
	Author = {Scholkopf, Bernhard and Mika, Sebastian and Burges, Chris JC and Knirsch, Philipp and Muller, K-R and Ratsch, Gunnar and Smola, Alexander J},
	Journal = {IEEE transactions on neural networks},
	Number = {5},
	Pages = {1000--1017},
	Publisher = {IEEE},
	Title = {Input space versus feature space in kernel-based methods},
	Volume = {10},
	Year = {1999}}

@inproceedings{williams1996gaussian,
	Author = {Williams, Christopher KI and Rasmussen, Carl Edward},
	Booktitle = {Advances in neural information processing systems},
	Pages = {514--520},
	Title = {Gaussian processes for regression},
	Year = {1996}}

@article{hinton2006fast,
	Author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
	Journal = {Neural computation},
	Number = {7},
	Pages = {1527--1554},
	Publisher = {MIT Press},
	Title = {A fast learning algorithm for deep belief nets},
	Volume = {18},
	Year = {2006}}

@article{aizerman1964theoretical,
	Author = {Aizerman, Mark A},
	Journal = {Automation and remote control},
	Pages = {821--837},
	Title = {Theoretical foundations of the potential function method in pattern recognition learning},
	Volume = {25},
	Year = {1964}}

@article{kabashima1998belief,
	Author = {Kabashima, Yoshiyuki and Saad, David},
	Journal = {EPL (Europhysics Letters)},
	Number = {5},
	Pages = {668},
	Publisher = {IOP Publishing},
	Title = {Belief propagation vs. TAP for decoding corrupted messages},
	Volume = {44},
	Year = {1998}}

@article{boutros2002iterative,
	Author = {Boutros, Joseph and Caire, Giuseppe},
	Journal = {IEEE Transactions on Information Theory},
	Number = {7},
	Pages = {1772--1793},
	Publisher = {IEEE},
	Title = {Iterative multiuser joint decoding: Unified framework and asymptotic analysis},
	Volume = {48},
	Year = {2002}}

@inproceedings{montanari2006analysis,
	Author = {Montanari, Andrea and Tse, David},
	Booktitle = {2006 IEEE Information Theory Workshop-ITW'06 Punta del Este},
	Organization = {IEEE},
	Pages = {160--164},
	Title = {Analysis of belief propagation for non-linear problems: The example of CDMA (or: How to prove Tanaka's formula)},
	Year = {2006}}

@article{guo2008multiuser,
	Author = {Guo, Dongning and Wang, Chih-Chun},
	Journal = {IEEE journal on selected areas in communications},
	Number = {3},
	Pages = {421--431},
	Publisher = {IEEE},
	Title = {Multiuser detection of sparsely spread CDMA},
	Volume = {26},
	Year = {2008}}

@article{tanaka2002statistical,
	Author = {Tanaka, Toshiyuki},
	Journal = {IEEE Transactions on Information theory},
	Number = {11},
	Pages = {2888--2910},
	Publisher = {IEEE},
	Title = {A statistical-mechanics approach to large-system analysis of CDMA multiuser detectors},
	Volume = {48},
	Year = {2002}}

@article{guo2005randomly,
	Author = {Guo, Dongning and Verd{\'u}, Sergio},
	Journal = {IEEE Transactions on Information Theory},
	Number = {6},
	Pages = {1983--2010},
	Publisher = {IEEE},
	Title = {Randomly spread CDMA: Asymptotics via statistical physics},
	Volume = {51},
	Year = {2005}}

@inproceedings{rangan2009asymptotic,
	Author = {Rangan, Sundeep and Goyal, Vivek and Fletcher, Alyson K},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {1545--1553},
	Title = {Asymptotic analysis of map estimation via the replica method and compressed sensing},
	Year = {2009}}

@article{parker2014bilinear,
	Author = {Parker, Jason T and Schniter, Philip and Cevher, Volkan},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {22},
	Pages = {5839--5853},
	Publisher = {IEEE},
	Title = {Bilinear generalized approximate message passing---Part I: Derivation},
	Volume = {62},
	Year = {2014}}

@article{rangan2019vector,
	Author = {Rangan, Sundeep and Schniter, Philip and Fletcher, Alyson K},
	Journal = {IEEE Transactions on Information Theory},
	Number = {10},
	Pages = {6664--6684},
	Publisher = {IEEE},
	Title = {Vector approximate message passing},
	Volume = {65},
	Year = {2019}}

@techreport{widrow1960adaptive,
	Author = {Widrow, Bernard and Hoff, Marcian E},
	Institution = {Stanford Univ Ca Stanford Electronics Labs},
	Title = {Adaptive switching circuits},
	Year = {1960}}

@article{mcclelland1986appeal,
  title={The appeal of parallel distributed processing},
  author={McClelland, James L and Rumelhart, David E and Hinton, Geoffrey E},
  journal={MIT Press, Cambridge MA},
  pages={3--44},
  year={1986}
}


@article{hinton1986learning,
	Author = {Hinton, Geoffrey E and Sejnowski, Terrence J and others},
	Journal = {Parallel distributed processing: Explorations in the microstructure of cognition},
	Number = {282-317},
	Pages = {2},
	Title = {Learning and relearning in Boltzmann machines},
	Volume = {1},
	Year = {1986}}

@article{bengio2007scaling,
	Author = {Bengio, Yoshua and LeCun, Yann and others},
	Journal = {Large-scale kernel machines},
	Number = {5},
	Pages = {1--41},
	Title = {Scaling learning algorithms towards AI},
	Volume = {34},
	Year = {2007}}

@book{james2013introduction,
	Author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	Publisher = {Springer},
	Title = {An introduction to statistical learning},
	Volume = {112},
	Year = {2013}}

@article{sauer1972density,
	Author = {Sauer, Norbert},
	Journal = {Journal of Combinatorial Theory, Series A},
	Number = {1},
	Pages = {145--147},
	Publisher = {Elsevier},
	Title = {On the density of families of sets},
	Volume = {13},
	Year = {1972}}

@article{shelah1972combinatorial,
	Author = {Shelah, Saharon},
	Journal = {Pacific Journal of Mathematics},
	Number = {1},
	Pages = {247--261},
	Publisher = {Mathematical Sciences Publishers},
	Title = {A combinatorial problem; stability and order for models and theories in infinitary languages},
	Volume = {41},
	Year = {1972}}

@book{hastie2015statistical,
	Author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	Publisher = {CRC press},
	Title = {Statistical learning with sparsity: the lasso and generalizations},
	Year = {2015}}

@article{tibshirani1996regression,
	Author = {Tibshirani, Robert},
	Journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	Number = {1},
	Pages = {267--288},
	Publisher = {Wiley Online Library},
	Title = {Regression shrinkage and selection via the lasso},
	Volume = {58},
	Year = {1996}}

@article{hoerl1970ridge,
	Author = {Hoerl, Arthur E and Kennard, Robert W},
	Journal = {Technometrics},
	Number = {1},
	Pages = {55--67},
	Publisher = {Taylor \& Francis Group},
	Title = {Ridge regression: Biased estimation for nonorthogonal problems},
	Volume = {12},
	Year = {1970}}

@article{zou2005regularization,
	Author = {Zou, Hui and Hastie, Trevor},
	Journal = {Journal of the royal statistical society: series B (statistical methodology)},
	Number = {2},
	Pages = {301--320},
	Publisher = {Wiley Online Library},
	Title = {Regularization and variable selection via the elastic net},
	Volume = {67},
	Year = {2005}}

@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin},
  journal={Comp. Rend. Sci. Paris},
  volume={25},
  number={1847},
  pages={536--538},
  year={1847}
}


@book{boyd2004convex,
	Author = {Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
	Publisher = {Cambridge university press},
	Title = {Convex optimization},
	Year = {2004}}

@article{bottou1998online,
	Author = {Bottou, L{\'e}on},
	Journal = {On-line learning in neural networks},
	Number = {9},
	Pages = {142},
	Title = {Online learning and stochastic approximations},
	Volume = {17},
	Year = {1998}}

@inproceedings{neyshabur2017exploring,
	Author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
	Booktitle = {Advances in neural information processing systems},
	Pages = {5947--5956},
	Title = {Exploring generalization in deep learning},
	Year = {2017}}

@article{arora2018stronger,
	Author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	Journal = {arXiv preprint arXiv:1802.05296},
	Title = {Stronger generalization bounds for deep nets via a compression approach},
	Year = {2018}}

@book{koller2009probabilistic,
	Author = {Koller, Daphne and Friedman, Nir},
	Publisher = {MIT press},
	Title = {Probabilistic graphical models: principles and techniques},
	Year = {2009}}

@article{gerace2020generalisation,
	Author = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
	Journal = {arXiv preprint arXiv:2002.09339},
	Title = {Generalisation error in learning with random features and the hidden manifold model},
	Year = {2020}}

@article{belkin2019reconciling,
	Author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {32},
	Pages = {15849--15854},
	Publisher = {National Acad Sciences},
	Title = {Reconciling modern machine-learning practice and the classical bias--variance trade-off},
	Volume = {116},
	Year = {2019}}

@article{mei2019generalization,
	Author = {Mei, Song and Montanari, Andrea},
	Journal = {arXiv preprint arXiv:1908.05355},
	Title = {The generalization error of random features regression: Precise asymptotics and double descent curve},
	Year = {2019}}

@article{d2020double,
	Author = {d'Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
	Journal = {arXiv preprint arXiv:2003.01054},
	Title = {Double trouble in double descent: Bias and variance (s) in the lazy regime},
	Year = {2020}}

@article{braunstein_Survey_propagation,
	Abstract = {Abstract We study the satisfiability of randomly generated formulas formed by M clauses of exactly K literals over N Boolean variables. For a given value of N the problem is known to be most difficult when α = M/N is close to the experimental threshold αc separating the region where almost all formulas are SAT from the region where all formulas are UNSAT. Recent results from a statistical physics analysis suggest that the difficulty is related to the existence of a clustering phenomenon of the solutions when α is close to (but smaller than) αc. We introduce a new type of message passing algorithm which allows to find efficiently a satisfying assignment of the variables in this difficult region. This algorithm is iterative and composed of two main parts. The first is a message-passing procedure which generalizes the usual methods like Sum-Product or Belief Propagation: It passes messages that may be thought of as surveys over clusters of the ordinary messages. The second part uses the detailed probabilistic information obtained from the surveys in order to fix variables and simplify the problem. Eventually, the simplified problem that remains is solved by a conventional heuristic. {\copyright} 2005 Wiley Periodicals, Inc. Random Struct. Alg., 2005},
	Author = {Braunstein, A. and M{\'e}zard, M. and Zecchina, R.},
	Journal = {Random Structures \& Algorithms},
	Number = {2},
	Pages = {201-226},
	Title = {Survey propagation: An algorithm for satisfiability},
	Volume = {27},
	Year = {2005}
	}

@article{kabashima2003cdma,
	Author = {Kabashima, Yoshiyuki},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {43},
	Pages = {11111},
	Publisher = {IOP Publishing},
	Title = {A CDMA multiuser detection algorithm on the basis of belief propagation},
	Volume = {36},
	Year = {2003}}

@article{bolthausen2014iterative,
	Author = {Bolthausen, Erwin},
	Journal = {Communications in Mathematical Physics},
	Number = {1},
	Pages = {333--366},
	Publisher = {Springer},
	Title = {An iterative construction of solutions of the TAP equations for the Sherrington--Kirkpatrick model},
	Volume = {325},
	Year = {2014}}

@inproceedings{barbier2018mutual,
	Author = {Barbier, Jean and Macris, Nicolas and Maillard, Antoine and Krzakala, Florent},
	Booktitle = {2018 IEEE International Symposium on Information Theory (ISIT)},
	Organization = {IEEE},
	Pages = {1390--1394},
	Title = {The mutual information in random linear estimation beyond iid matrices},
	Year = {2018}}

@inproceedings{vila2015adaptive,
	Author = {Vila, Jeremy and Schniter, Philip and Rangan, Sundeep and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	Organization = {IEEE},
	Pages = {2021--2025},
	Title = {Adaptive damping and mean removal for the generalized approximate message passing algorithm},
	Year = {2015}}

@article{rangan2019convergence,
	Author = {Rangan, Sundeep and Schniter, Philip and Fletcher, Alyson K and Sarkar, Subrata},
	Journal = {IEEE Transactions on Information Theory},
	Number = {9},
	Pages = {5339--5351},
	Publisher = {IEEE},
	Title = {On the convergence of approximate message passing with arbitrary matrices},
	Volume = {65},
	Year = {2019}}

@inproceedings{cakmak2014s,
	Author = {Cakmak, Burak and Winther, Ole and Fleury, Bernard H},
	Booktitle = {2014 IEEE Information Theory Workshop (ITW 2014)},
	Organization = {IEEE},
	Pages = {192--196},
	Title = {S-AMP: Approximate message passing for general matrix ensembles},
	Year = {2014}}

@article{tulino2013support,
	Author = {Tulino, Antonia M and Caire, Giuseppe and Verdu, Sergio and Shamai, Shlomo},
	Journal = {IEEE Transactions on Information Theory},
	Number = {7},
	Pages = {4243--4271},
	Publisher = {IEEE},
	Title = {Support recovery with sparsely sampled free random matrices},
	Volume = {59},
	Year = {2013}}

@inproceedings{opper1991calculation,
	Author = {Opper, Manfred and Haussler, David},
	Organization = {Citeseer},
	Title = {Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise},
	Year={1991}
	}

@article{iba1999nishimori,
	Author = {Iba, Yukito},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {21},
	Pages = {3875},
	Publisher = {IOP Publishing},
	Title = {The Nishimori line and Bayesian statistics},
	Volume = {32},
	Year = {1999}}

@article{parisi1995mean,
	Author = {Parisi, Giorgio and Potters, Marc},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {18},
	Pages = {5267},
	Publisher = {IOP Publishing},
	Title = {Mean-field equations for spin models with orthogonal interaction matrices},
	Volume = {28},
	Year = {1995}}

@article{opper2001adaptive,
	Author = {Opper, Manfred and Winther, Ole},
	Journal = {Physical Review E},
	Number = {5},
	Pages = {056131},
	Publisher = {APS},
	Title = {Adaptive and self-averaging Thouless-Anderson-Palmer mean-field theory for probabilistic modeling},
	Volume = {64},
	Year = {2001}}

@article{opper2001tractable,
	Author = {Opper, Manfred and Winther, Ole},
	Journal = {Physical Review Letters},
	Number = {17},
	Pages = {3695},
	Publisher = {APS},
	Title = {Tractable approximations for probabilistic models: The adaptive Thouless-Anderson-Palmer mean field approach},
	Volume = {86},
	Year = {2001}}

@phdthesis{minka2001family,
	Author = {Minka, Thomas Peter},
	School = {Massachusetts Institute of Technology},
	Title = {A family of algorithms for approximate Bayesian inference},
	Year = {2001}}

@article{heskes2012expectation,
	Author = {Heskes, Tom and Zoeter, Onno},
	Journal = {arXiv preprint arXiv:1301.0572},
	Title = {Expectation Propogation for approximate inference in dynamic Bayesian networks},
	Year = {2012}}

@article{heskes2005approximate,
	Author = {Heskes, Tom and Opper, Manfred and Wiegerinck, Wim and Winther, Ole and Zoeter, Onno},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {11},
	Pages = {P11015},
	Publisher = {IOP Publishing},
	Title = {Approximate inference techniques with expectation constraints},
	Volume = {2005},
	Year = {2005}}

@inproceedings{opper2005expectation,
	Author = {Opper, Manfred and Winther, Ole},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {1001--1008},
	Title = {Expectation consistent free energies for approximate inference},
	Year = {2005}}

@article{25,
	Author = {R. Monasson},
	Date-Added = {2018-03-08 16:58:01 +0000},
	Date-Modified = {2018-03-08 16:58:50 +0000},
	Journal = {Physical Review Letter},
	Number = {75 2847},
	Title = {Structural Glass Transition and the Entropy of the Metastable States},
	Year = {1995}}

@book{24,
	Author = {D. Hebb},
	Date-Added = {2018-02-19 23:09:53 +0000},
	Date-Modified = {2018-02-19 23:16:08 +0000},
	Publisher = {Psychology Press},
	Title = {The Organization of Behavior: A Neuropsychological Theory},
	Year = {1949}}

@article{23,
	Author = {Warren S. McCulloch, Walter Pitts},
	Date-Added = {2018-02-19 22:49:33 +0000},
	Date-Modified = {2018-02-19 22:50:29 +0000},
	Journal = {The bulletin of mathematical biophysics},
	Title = {A Logical Calculus of Ideas Immanent in Nervous Activity},
	Year = {1943}}

@article{22,
	Author = {J.R.L de Almeida and D.J Thouless},
	Date-Added = {2018-02-05 14:15:58 +0000},
	Date-Modified = {2018-02-05 14:16:05 +0000},
	Journal = {J. Phys. A: Math. Gen},
	Title = {Stability of the {S}herrington-{K}irkpatrick solution of a spin glass model},
	Year = {1978}}

@article{parisi1980sequence,
	Author = {Parisi, Giorgio},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {4},
	Pages = {L115},
	Publisher = {IOP Publishing},
	Title = {A sequence of approximated solutions to the SK model for spin glasses},
	Volume = {13},
	Year = {1980}}

@article{parisi1980order,
	Author = {Parisi, Giorgio},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {3},
	Pages = {1101},
	Publisher = {IOP Publishing},
	Title = {The order parameter for spin glasses: a function on the interval 0-1},
	Volume = {13},
	Year = {1980}}

@article{parisi1979infinite,
	Author = {Parisi, Giorgio},
	Journal = {Physical Review Letters},
	Number = {23},
	Pages = {1754},
	Publisher = {APS},
	Title = {Infinite number of order parameters for spin-glasses},
	Volume = {43},
	Year = {1979}}

@article{20,
	Author = {S. Franz and G. Parisi and M. Sevelev and P. Urbani and F. Zamponi},
	Date-Added = {2017-06-22 12:37:31 +0000},
	Date-Modified = {2018-03-09 17:01:26 +0000},
	Journal = {SciPost Phys},
	Title = {Universality of the {SAT-UNSAT} (jamming) threshold in non-convex continuous constraint satisfaction problems},
	Year = {2017}}

@article{19,
	Author = {M. M{\'e}zard and G. Parisi and N. Sourlas and G. Toulouse \& M. Virasoro},
	Date-Added = {2017-06-22 09:54:04 +0000},
	Date-Modified = {2017-06-22 09:54:38 +0000},
	Journal = {J. Phys. France},
	Title = {Replica symmetry breaking and the nature of the spin glass phase},
	Year = {1984}}

@article{18,
	Author = {F. Rosenblatt},
	Date-Added = {2017-06-21 11:30:59 +0000},
	Date-Modified = {2017-06-21 11:32:05 +0000},
	Journal = {Psychological Review,},
	Title = {The perceptron: a probabilistic model for information storage and organization in the brain},
	Year = {1958}}

@article{17,
	Author = {N. Brunel, J-P. Nadal \& G. Toulouse},
	Date-Added = {2017-06-21 07:13:13 +0000},
	Date-Modified = {2017-06-21 07:57:53 +0000},
	Journal = {J. Phys. A: Math. and Gen},
	Title = {Information capacity of a perceptron},
	Year = {1992}}

@article{16,
	Author = {H. Huang, K.Y.M Wong \& Y. Kabashima},
	Date-Added = {2017-06-21 07:08:55 +0000},
	Date-Modified = {2017-06-21 07:57:47 +0000},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Title = {Entropy landscape of solutions in the binary perceptron problem},
	Year = {2013}}

@article{huang2014origin,
	Author = {Huang, Haiping and Kabashima, Yoshiyuki},
	Journal = {Physical Review E},
	Number = {5},
	Pages = {052813},
	Publisher = {APS},
	Title = {Origin of the computational hardness for learning with binary synapses},
	Volume = {90},
	Year = {2014}}

@article{15,
	Author = {F. Cousseau, K. Mimura, T. Omori \& M. Okada},
	Date-Added = {2017-06-21 07:05:46 +0000},
	Date-Modified = {2017-06-21 07:54:51 +0000},
	Journal = {Physical Review E},
	Title = {Statistical mechanics of lossy compression for nonmonotonic multilayer perceptrons},
	Year = {2008}}

@article{14,
	Author = {G.J. Bex, R. Serneels, \& C. Van den Broeck},
	Date-Added = {2017-06-21 07:04:40 +0000},
	Date-Modified = {2017-06-21 07:57:37 +0000},
	Journal = {Physical Review E},
	Title = {Storage capacity and generalization error for the reversed-wedge {I}sing perceptron},
	Year = {1995}}

@book{12,
	Author = {H. Nishimori},
	Date-Added = {2017-06-21 06:59:15 +0000},
	Date-Modified = {2017-06-21 07:03:29 +0000},
	Publisher = {Oxford University Press},
	Title = {Statistical physics of spin glasses and information processing},
	Year = {2001}}

@article{11,
	Author = {T. Hosaka \& Y. Kabashima},
	Date-Added = {2017-06-21 06:57:55 +0000},
	Date-Modified = {2017-06-21 07:57:20 +0000},
	Journal = {Physical Review E},
	Title = {Statistical mechanics of lossy data compression using a non-monotonic perceptron},
	Year = {2008}}

@article{10,
	Author = {H.S. Seung \& H. Sompolinsky},
	Date-Added = {2017-06-21 06:56:35 +0000},
	Date-Modified = {2017-06-21 07:57:16 +0000},
	Journal = {Physical Review A},
	Title = {Statistical mechanics of learning from examples},
	Year = {1992}}

@article{9,
	Author = {T. Obuchi \& Y. Kabashima},
	Date-Added = {2017-06-21 06:51:58 +0000},
	Date-Modified = {2017-06-21 07:57:12 +0000},
	Journal = {Journal of Statistical Mechanics : Theory and Experiment},
	Title = {Weight space structure and analysis using a finite replica number in the {I}sing perceptron},
	Year = {2009}}

@article{mossel2015reconstruction,
	Author = {Mossel, Elchanan and Neeman, Joe and Sly, Allan},
	Journal = {Probability Theory and Related Fields},
	Number = {3-4},
	Pages = {431--461},
	Publisher = {Springer},
	Title = {Reconstruction and estimation in the planted partition model},
	Volume = {162},
	Year = {2015}}

@article{6,
	Author = {E. Gardner},
	Date-Added = {2017-06-21 06:24:37 +0000},
	Date-Modified = {2017-06-21 07:08:31 +0000},
	Journal = {J. Phys. A: Math. and Gen},
	Title = {The space of interactions in neural networks},
	Year = {1988}}

@article{5,
	Author = {M. M\'{e}zard},
	Date-Added = {2017-06-21 06:23:20 +0000},
	Date-Modified = {2017-06-21 07:56:31 +0000},
	Journal = {J. Phys. A: Math. and Gen},
	Title = {The space of interactions in neural networks: {G}ardner's computation with the cavity method},
	Year = {1989}}

@booklet{4,
	Author = {M. Advani, S. Lahiri \& S. Ganguli},
	Date-Added = {2017-06-21 06:19:10 +0000},
	Date-Modified = {2017-06-21 08:11:05 +0000},
	Title = {Statistical mechanics of complex neural systems and high dimensional data},
	Year = {2013}}

@article{kim1998covering,
	Author = {Kim, Jeong Han and Roche, James R},
	Journal = {Journal of Computer and System Sciences},
	Number = {2},
	Pages = {223--252},
	Publisher = {Elsevier},
	Title = {Covering cubes by random half cubes, with applications to binary neural networks},
	Volume = {56},
	Year = {1998}}

@article{franz2016simplest,
	Author = {Franz, Silvio and Parisi, Giorgio},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {14},
	Pages = {145001},
	Publisher = {IOP Publishing},
	Title = {The simplest model of jamming},
	Volume = {49},
	Year = {2016}}

@article{opper1995statistical,
	Author = {Opper, Manfred},
	Journal = {Physical Review E},
	Number = {4},
	Pages = {3613},
	Publisher = {APS},
	Title = {Statistical physics estimates for the complexity of feedforward neural networks},
	Volume = {51},
	Year = {1995}}

@article{bex1995storage,
	Author = {Bex, Geert Jan and Serneels, Roger and Van den Broeck, Christian},
	Journal = {Physical Review E},
	Number = {6},
	Pages = {6309},
	Publisher = {APS},
	Title = {Storage capacity and generalization error for the reversed-wedge Ising perceptron},
	Volume = {51},
	Year = {1995}}

@article{hosaka2002statistical,
	Author = {Hosaka, Tadaaki and Kabashima, Yoshiyuki and Nishimori, Hidetoshi},
	Journal = {Physical Review E},
	Number = {6},
	Pages = {066126},
	Publisher = {APS},
	Title = {Statistical mechanics of lossy data compression using a nonmonotonic perceptron},
	Volume = {66},
	Year = {2002}}

@article{stojnic2013another,
	Author = {Stojnic, Mihailo},
	Journal = {arXiv preprint arXiv:1306.3979},
	Title = {Another look at the {G}ardner problem},
	Year = {2013}}

@article{BansalSpencer19,
	Author = {Bansal, Nikhil and Spencer, Joel H.},
	Journal = {arXiv preprint arXiv:1903.06898},
	Title = {On-Line Balancing of Random Inputs},
	Year = {2019}}

@article{baldassi2015subdominant,
	Author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	Journal = {Physical review letters},
	Number = {12},
	Pages = {128101},
	Publisher = {APS},
	Title = {Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses},
	Volume = {115},
	Year = {2015}}

@article{zdeborova2008constraint,
	Author = {Zdeborov{\'a}, Lenka and M{\'e}zard, Marc},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {12},
	Pages = {P12004},
	Publisher = {IOP Publishing},
	Title = {Constraint satisfaction problems with isolated solutions are hard},
	Volume = {2008},
	Year = {2008}}

@article{zdeborova2008locked,
	Author = {Zdeborov{\'a}, Lenka and M{\'e}zard, Marc},
	Journal = {Physical review letters},
	Number = {7},
	Pages = {078702},
	Publisher = {APS},
	Title = {Locked constraint satisfaction problems},
	Volume = {101},
	Year = {2008}}

@article{baldassi2016unreasonable,
	Author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {48},
	Pages = {E7655--E7662},
	Publisher = {National Acad Sciences},
	Title = {Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes},
	Volume = {113},
	Year = {2016}}

@article{zdeborova2011quiet,
	Author = {Zdeborov{\'a}, Lenka and Krzakala, Florent},
	Journal = {SIAM Journal on Discrete Mathematics},
	Number = {2},
	Pages = {750--770},
	Publisher = {SIAM},
	Title = {Quiet planting in the locked constraint satisfaction problems},
	Volume = {25},
	Year = {2011}}

@article{braunstein2006learning,
	Author = {Braunstein, Alfredo and Zecchina, Riccardo},
	Journal = {Physical review letters},
	Number = {3},
	Pages = {030201},
	Publisher = {APS},
	Title = {Learning by message passing in networks of discrete synapses},
	Volume = {96},
	Year = {2006}}

@article{martin2004frozen,
	Author = {Martin, OC and M{\'e}zard, M and Rivoire, O},
	Journal = {Physical review letters},
	Number = {21},
	Pages = {217205},
	Publisher = {APS},
	Title = {Frozen glass phase in the multi-index matching problem},
	Volume = {93},
	Year = {2004}}

@article{shcherbina2003rigorous,
	Author = {Shcherbina, Mariya and Tirozzi, Brunello},
	Journal = {Communications in mathematical physics},
	Number = {3},
	Pages = {383--422},
	Publisher = {Springer},
	Title = {Rigorous solution of the {G}ardner problem},
	Volume = {234},
	Year = {2003}}

@article{cover1965geometrical,
	Author = {Cover, Thomas M},
	Journal = {IEEE transactions on electronic computers},
	Number = {3},
	Pages = {326--334},
	Publisher = {IEEE},
	Title = {Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
	Year = {1965}}

@article{stojnic2013negative,
	Author = {Stojnic, Mihailo},
	Journal = {arXiv preprint arXiv:1306.3980},
	Title = {Negative spherical perceptron},
	Year = {2013}}

@article{stojnic2013discrete,
	Author = {Stojnic, Mihailo},
	Journal = {arXiv preprint arXiv:1306.4375},
	Title = {Discrete perceptrons},
	Year = {2013}}

@article{krzakala2009hiding,
	Author = {Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Journal = {Physical review letters},
	Number = {23},
	Pages = {238701},
	Publisher = {APS},
	Title = {Hiding quiet solutions in random constraint satisfaction problems},
	Volume = {102},
	Year = {2009}}

@inproceedings{achlioptas2008algorithmic,
	Author = {Achlioptas, Dimitris and Coja-Oghlan, Amin},
	Booktitle = {Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on},
	Organization = {IEEE},
	Pages = {793--802},
	Title = {Algorithmic barriers from phase transitions},
	Year = {2008}}

@article{coja2018information,
	Author = {Coja-Oghlan, Amin and Krzakala, Florent and Perkins, Will and Zdeborov\'{a}, Lenka},
	Journal = {Advances in Mathematics},
	Pages = {694--795},
	Publisher = {Elsevier},
	Title = {Information-theoretic thresholds from the cavity method},
	Volume = {333},
	Year = {2018}}

@article{mossel2012stochastic,
	Author = {Mossel, Elchanan and Neeman, Joe and Sly, Allan},
	Journal = {arXiv preprint arXiv:1202.1499},
	Title = {Stochastic block models and reconstruction},
	Year = {2012}}

@article{achlioptas2011solution,
	Author = {Achlioptas, Dimitris and Coja-Oghlan, Amin and Ricci-Tersenghi, Federico},
	Journal = {Random Structures \& Algorithms},
	Number = {3},
	Pages = {251--268},
	Publisher = {Wiley Online Library},
	Title = {On the solution-space geometry of random constraint satisfaction problems},
	Volume = {38},
	Year = {2011}}

@article{panchenko2014parisi,
	Author = {Panchenko, Dmitry},
	Journal = {The Annals of Probability},
	Number = {3},
	Pages = {946--958},
	Publisher = {Institute of Mathematical Statistics},
	Title = {The {P}arisi formula for mixed $p$-spin models},
	Volume = {42},
	Year = {2014}}

@inproceedings{ding2015proof,
	Author = {Ding, Jian and Sly, Allan and Sun, Nike},
	Booktitle = {Proceedings of the forty-seventh annual ACM symposium on Theory of computing},
	Organization = {ACM},
	Pages = {59--68},
	Title = {Proof of the satisfiability conjecture for large k},
	Year = {2015}}

@article{friedgut1999sharp,
	Author = {Friedgut, Ehud},
	Journal = {Journal of the American mathematical Society},
	Number = {4},
	Pages = {1017--1054},
	Title = {Sharp thresholds of graph properties, and the k-{SAT} problem},
	Volume = {12},
	Year = {1999}}

@inproceedings{achlioptas2002asymptotic,
	Author = {Achlioptas, Dimitris and Moore, Cristopher},
	Booktitle = {Foundations of Computer Science, 2002. Proceedings. The 43rd Annual IEEE Symposium on},
	Organization = {IEEE},
	Pages = {779--788},
	Title = {The asymptotic order of the random k-{SAT} threshold},
	Year = {2002}}

@article{wendel1962problem,
	Author = {Wendel, James G},
	Journal = {Math. Scand},
	Pages = {109--111},
	Title = {A problem in geometric probability},
	Volume = {11},
	Year = {1962}}

@article{talagrand2006parisi,
	Author = {Talagrand, Michel},
	Journal = {Annals of mathematics},
	Pages = {221--263},
	Publisher = {JSTOR},
	Title = {The {P}arisi formula},
	Year = {2006}}

@article{Mitchison1989,
	Abstract = {We obtain bounds for the capacity of some multi-layer networks of linear threshold units. In the case of a network having n inputs, a single layer of h hidden units and an output layer of s units, where all the weights in the network are variable and s{\^a}¦h{\^a}¦n, the capacity m satisfies 2n{\^a}¦m{\^a}¦nt logt, where t=1=h/s. We consider in more detail the case where there is a single output that is a fixed boolean function of the hidden units. In this case our upper bound is of order nh logh but the argument which provided the lower bound of 2n no longer applies. However, by explicit computation in low dimensional cases we show that the capacity exceeds 2n but is substantially less than the upper bound. Finally, we describe a learning algorithm for multi-layer networks with a single output unit. This greatly outperforms back propagation at the task of learning random vectors and provides further empirical evidence that the lower bound of 2n can be exceeded.},
	Author = {Mitchison, G. J. and Durbin, R. M.},
	Day = {01},
	Journal = {Biological Cybernetics},
	Month = {Mar},
	Number = {5},
	Pages = {345--365},
	Title = {Bounds on the learning capacity of some multi-layer networks},
	Volume = {60},
	Year = {1989}}

@inproceedings{Sun2018,
	Author = {Ding, Jian and Sun, Nike},
	Booktitle = {Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing},
	Organization = {ACM},
	Pages = {816--827},
	Title = {Capacity lower bound for the {I}sing perceptron},
	Year = {2019}}

@article{Saad1995b,
	Abstract = {We present an analytic solution to the problem of on-line gradient-descent learning for two-layer neural networks with an arbitrary number of hidden units in both teacher and student networks. PACS numbers: 87. 10.+e, 02.50. --- r, 05.20. --- y Layered neural networks are of interest for their abil-ity to implement input-output maps [1]. Classification and regression tasks formulated as a map from an N-dimensional input space g onto a scalar g are real-ized through a map g = fi(g), which can be modified through changes in the internal parameters [J) specifying the strength of the interneuron couplings [2,3]. Learn-ing refers to the modification of these couplings so as to bring the map f{\~{}} implemented by the network as close as possible to a desired map f. The degree of success is monitored through the generalization error, a measure of the dissimilarity between f{\~{}} and f. Learning from examples in layered neural networks is usually formulated as an optimization problem [2,3], based on the minimization of an additive learning er-ror defined over a training set composed of P inde-pendent examples ((t', ("), with (t' = f(g"), 1 {\~{}} p {\~{}} P. Statistical physics tools for investigating the prop-erties of such models, based on the use of the replica method, have been successfully applied to the analysis of single-layer perceptrons [3] and some simplified two-layer structures (e.g., committee machines [4]). Analysis of more complicated multilayer networks is hampered by technical difficulties due to the complex structure of the solutions in a space of order parameters [5], which de-scribe in this case correlations among the various neurons in the trained network, as well as their degree of special-ization toward the implementation of the desired task. A recently introduced alternative approach investigates on line learning -[6]. In this scenario the couplings are adjusted to minimize the error after the presentation of each example. The resulting changes in [J] are de-scribed as a dynamical evolution, with the number of examples playing the role of time. The average that ac-counts for the disorder introduced by the independent random selection of an example at each time step can be performed directly, without invoking the replica method. The resulting equations of motion for the relevant order parameters characterize the structure of the space of so-lutions and allow for a computation of the generalization error. While investigating the on-line learning scenario pro-posed by Biehl and Schwarze [6], we found an unexpected result: The dynamical equations for the order parameters can be obtained analytically for a general two-layer stu-dent network composed of N input units, K hidden units, and a single linear output unit, trained to perform a task defined through a teacher network of similar architecture, except that its number M of hidden units is not necessarily equal to K. Two-layer networks with an arbitrary number of hidden units have been shown to be universal approximators [1] for N-to-one dimensional maps. Our results thus describe the learning of tasks of arbitrary complexity (general M). The complexity of the student network is also arbitrary (general K, independent of M), providing a tool to investigate realizable (K = M), overrealizable (K) M), and unrealizable (K (M) learning scenarios. Such capabilities are to be contrasted with previously available results; the equations provided in [6] can only describe a committee rnachine with K = 2 hidden units learning a linearly separable task (M = 1). In this Letter we limit our discussion to the case of the soft-committee machine [6], in which all the hid-den units are connected to the output unit with positive couplings of unit strength, and only the input-to-hidden couplings are adaptive. Consider the student network: hid-den unit i receives information from input unit r through the weight J; " and its activation under presentation of an input pattern g = (gi, . . . , g{\~{}}) is x; = J; g, with J; = (J;i, . . . , J,{\~{}}) defined as the vector of incoming weights onto the ith hidden unit. The output of the student network is tr(J, g) = g, , g(J; g), where g is the activation func-tion of the hidden units, taken here to be the error function g(x) = --- erf (x/{\~{}}2), and J --- = [J;)i; tc is the set of input-to-hidden adaptive weights. Training examples are of the form (gt', gt'). The components of the independently drawn input vectors g{\&} are uncorrelated random variables with zero mean and unit variance. The corresponding output gP is given by a deterministic teacher whose internal structure is that of a network similar to the student except for a possible difference in the number M of hidden units. Hidden unit n in the teacher network receives input information through the weight vector B, = (B " i, . . . , B " tv), and its activation under presentation of the input pattern g{\&} is yn The corresponding output is g{\&} = g " ,g(B, gt").},
	Author = {Saad, David and Solla, Sara A.},
	Doi = {10.1103/PhysRevLett.74.4337},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.74.4337.pdf:pdf},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {21},
	Pages = {4337--4340},
	Title = {{Exact solution for on-line learning in multilayer neural networks}},
	Volume = {74},
	Year = {1995},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.74.4337}}

@article{Rattray1998a,
	Abstract = {Natural gradient descent is an on-line variable-metric optimization algorithm which utilizes an underlying Riemannian parameter space. We analyze the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks. For a realizable learning scenario we find significant improvements over standard gradient descent for both the transient and asymptotic stages of learning, with a slower power law increase in learning time as task complexity grows. [S0031-9007(98)07950-2].},
	Author = {Rattray, M and Saad, D and Amari, S},
	Doi = {10.1103/PhysRevLett.81.5461},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.81.5461.pdf:pdf},
	Isbn = {0031-9007},
	Issn = {0031-9007},
	Journal = {Physical Review Letters},
	Keywords = {multilayer neural networks,online},
	Number = {24},
	Pages = {5461--5464},
	Title = {{Natural gradient descent for on-line learning}},
	Volume = {81},
	Year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.81.5461}}

@article{Saad1995c,
	Author = {Saad, David and Solla, Sara A},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/56. On-line learning in soft committee machine.pdf:pdf},
	Number = {2},
	Pages = {3--4},
	Title = {{( F ) = Μ}},
	Volume = {52},
	Year = {1995}}

@article{Mei2018a,
	Abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearlyideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
	Archiveprefix = {arXiv},
	Arxivid = {1804.06561},
	Author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	Eprint = {1804.06561},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1804.06561.pdf:pdf},
	Title = {{A Mean Field View of the Landscape of Two-Layers Neural Networks}},
	Url = {http://arxiv.org/abs/1804.06561},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1804.06561}}

@article{Safran2017,
	Abstract = {We consider the optimization problem associated with training simple ReLU neural networks of the form {\$}\backslashmathbf{\{}x{\}}\backslashmapsto \backslashsum{\_}{\{}i=1{\}}{\^{}}{\{}k{\}}\backslashmax\backslash{\{}0,\backslashmathbf{\{}w{\}}{\_}i{\^{}}\backslashtop \backslashmathbf{\{}x{\}}\backslash{\}}{\$} with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is unrestricted, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once {\$}k\backslashgeq 6{\$}. By a continuity argument, this implies that in high dimensions, $\backslash$emph{\{}nearly all{\}} target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.},
	Archiveprefix = {arXiv},
	Arxivid = {1712.08968},
	Author = {Safran, Itay and Shamir, Ohad},
	Eprint = {1712.08968},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1712.08968.pdf:pdf},
	Number = {1},
	Title = {{Spurious Local Minima are Common in Two-Layer ReLU Neural Networks}},
	Url = {http://arxiv.org/abs/1712.08968},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1712.08968}}

@article{Kabashima1994,
	Author = {Kabashima, Y.},
	Doi = {10.1088/0305-4470/27/6/017},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/36. Perfect loss of generalization due to nise in K=2 parity machines.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {6},
	Pages = {1917--1927},
	Title = {{Perfect loss of generalization due to noise in K=2 parity machines}},
	Volume = {27},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/27/6/017}}

@article{Gualdi2015a,
	Abstract = {The aim of this work is to explore the possible types of phenomena that simple macroeconomic Agent-Based models (ABMs) can reproduce. We propose a methodology, inspired by statistical physics, that characterizes a model through its "phase diagram" in the space of parameters. Our first motivation is to understand the large macro-economic fluctuations observed in the "Mark I" ABM devised by Delli Gatti and collaborators. In this regard, our major finding is the generic existence of a phase transition between a "good economy" where unemployment is low, and a "bad economy" where unemployment is high. We then introduce a simpler framework that allows us to show that this transition is robust against many modifications of the model, and is generically induced by an asymmetry between the rate of hiring and the rate of firing of the firms. The unemployment level remains small until a tipping point, beyond which the economy suddenly collapses. If the parameters are such that the system is close to this transition, any small fluctuation is amplified as the system jumps between the two equilibria. We have explored several natural extensions of the model. One is to introduce a bankruptcy threshold, limiting the firms maximum level of debt-to-sales ratio. This leads to a rich phase diagram with, in particular, a region where acute endogenous crises occur, during which the unemployment rate shoots up before the economy can recover. We also introduce simple wage policies. This leads to inflation (in the "good" phase) or deflation (in the "bad" phase), but leaves the overall phase diagram of the model essentially unchanged. We have also explored the effect of simple monetary policies that attempt to contain rising unemployment and defang crises. We end the paper with general comments on the usefulness of ABMs to model macroeconomic phenomena, in particular in view of the time needed to reach a steady state that raises the issue of ergodicity in these models.},
	Archiveprefix = {arXiv},
	Arxivid = {1307.5319},
	Author = {Gualdi, Stanislao and Tarzia, Marco and Zamponi, Francesco and Bouchaud, Jean Philippe},
	Doi = {10.1016/j.jedc.2014.08.003},
	Eprint = {1307.5319},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1307.5319v4.pdf:pdf},
	Issn = {01651889},
	Journal = {Journal of Economic Dynamics and Control},
	Keywords = {Agent-based computational economics,Aggregative models,Cycles},
	Pages = {29--61},
	Title = {{Tipping points in macroeconomic agent-based models}},
	Volume = {50},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.jedc.2014.08.003}}

@article{Kabashima1994a,
	Author = {Kabashima, Y.},
	Doi = {10.1088/0305-4470/27/6/017},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/53. Perfect loss of generalization due to noise in K=2 parity machines.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {6},
	Pages = {1917--1927},
	Title = {{Perfect loss of generalization due to noise in K=2 parity machines}},
	Volume = {27},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/27/6/017}}

@article{Shinzato2008,
	Abstract = {In this paper, we address the problem of how many randomly labeled patterns$\backslash$ncan be correctly classified by a single-layer perceptron when the patterns are$\backslash$ncorrelated with each other. In order to solve this problem, two analytical$\backslash$nschemes are developed based on the replica method and Thouless-Anderson-Palmer$\backslash$n(TAP) approach by utilizing an integral formula concerning random rectangular$\backslash$nmatrices. The validity and relevance of the developed methodologies are shown$\backslash$nfor one known result and two example problems. A message-passing algorithm to$\backslash$nperform the TAP scheme is also presented.},
	Archiveprefix = {arXiv},
	Arxivid = {0712.4050v1},
	Author = {Shinzato, Takashi and Kabashima, Yoshiyuki},
	Doi = {10.1088/1751-8113/41/32/324013},
	Eprint = {0712.4050v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Shinzato{\_}2008{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Theor.{\_}41{\_}324013.pdf:pdf},
	Issn = {17518113},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {32},
	Title = {{Perceptron capacity revisited: Classification ability for correlated patterns}},
	Volume = {41},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8113/41/32/324013}}

@article{Arora2018,
	Abstract = {Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that're orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified $\backslash$textquotedblleft noise stability$\backslash$textquotedblright properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1802.05296v4},
	Author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	Eprint = {arXiv:1802.05296v4},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1802.05296.pdf:pdf},
	Isbn = {9781510867963},
	Journal = {35th International Conference on Machine Learning, ICML 2018},
	Pages = {390--418},
	Title = {{Stronger generalization bounds for deep nets via a compression approach}},
	Volume = {1},
	Year = {2018}}

@article{Krzakala2011,
	Abstract = {Compressed sensing is triggering a major evolution in signal acquisition. It consists in sampling a sparse signal at low rate and later using computational power for its exact reconstruction, so that only the necessary information is measured. Currently used reconstruction techniques are, however, limited to acquisition rates larger than the true density of the signal. We design a new procedure which is able to reconstruct exactly the signal with a number of measurements that approaches the theoretical limit in the limit of large systems. It is based on the joint use of three essential ingredients: a probabilistic approach to signal reconstruction, a message-passing algorithm adapted from belief propagation, and a careful design of the measurement matrix inspired from the theory of crystal nucleation. The performance of this new algorithm is analyzed by statistical physics methods. The obtained improvement is confirmed by numerical studies of several cases.},
	Archiveprefix = {arXiv},
	Arxivid = {1109.4424},
	Author = {Krzakala, Florent and M{\'{e}}zard, Marc and Sausset, Fran{\c{c}}ois and Sun, Yifan and Zdeborov{\'{a}}, Lenka},
	Doi = {10.1103/PhysRevX.2.021005},
	Eprint = {1109.4424},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1109.4424.pdf:pdf},
	Isbn = {2160-3308},
	Issn = {2160-3308},
	Pages = {1--21},
	Title = {{Statistical physics-based reconstruction in compressed sensing}},
	Url = {http://arxiv.org/abs/1109.4424{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.2.021005},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevX.2.021005}}

@article{Dong2018,
	Abstract = {We introduce a generalized version of phase retrieval called multiplexed phase retrieval. We want to recover the phase of amplitude-only measurements from linear combinations of them. This corresponds to the case in which multiple incoherent sources are sampled jointly, and one would like to recover their individual contributions. We show that a recent spectral method developed for phase retrieval can be generalized to this setting, and that its performance follows a phase transition behavior. We apply this new technique to light focusing at depth in a complex medium. Experimentally, although we only have access to the sum of the intensities on multiple targets, we are able to separately focus on each ones, thus opening potential applications in deep fluorescence imaging and light delivery},
	Archiveprefix = {arXiv},
	Arxivid = {1810.13038},
	Author = {Dong, Jonathan and Krzakala, Florent and Gigan, Sylvain},
	Eprint = {1810.13038},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1810.13038.pdf:pdf},
	Title = {{Spectral Method for Multiplexed Phase Retrieval and Application in Optical Imaging in Complex Media}},
	Url = {http://arxiv.org/abs/1810.13038},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1810.13038}}

@article{Chung2017b,
	Abstract = {We consider the problem of classifying data manifolds where each manifold represents invariances that are parameterized by continuous degrees of freedom. Conventional data augmentation methods rely upon sampling large numbers of training examples from these manifolds; instead, we propose an iterative algorithm called M{\_}{\{}CP{\}} based upon a cutting-plane approach that efficiently solves a quadratic semi-infinite programming problem to find the maximum margin solution. We provide a proof of convergence as well as a polynomial bound on the number of iterations required for a desired tolerance in the objective function. The efficiency and performance of M{\_}{\{}CP{\}} are demonstrated in high-dimensional simulations and on image manifolds generated from the ImageNet dataset. Our results indicate that M{\_}{\{}CP{\}} is able to rapidly learn good classifiers and shows superior generalization performance compared with conventional maximum margin methods using data augmentation methods.},
	Archiveprefix = {arXiv},
	Arxivid = {1705.09944},
	Author = {Chung, SueYeon and Cohen, Uri and Sompolinsky, Haim and Lee, Daniel D.},
	Eprint = {1705.09944},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1705.09944.pdf:pdf},
	Title = {{Learning Data Manifolds with a Cutting Plane Method}},
	Url = {http://arxiv.org/abs/1705.09944},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.09944}}

@article{Baldassi2015a,
	Abstract = {We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance.We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here,we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings.We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1509.05753v1},
	Author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	Doi = {10.1103/PhysRevLett.115.128101},
	Eprint = {arXiv:1509.05753v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1509.05753.pdf:pdf},
	Issn = {10797114},
	Journal = {Physical Review Letters},
	Number = {12},
	Pages = {1--11},
	Pmid = {26431018},
	Title = {{Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses}},
	Volume = {115},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.115.128101}}

@article{Koch-Janusz2018,
	Abstract = {Physical systems differring in their microscopic details often display strikingly similar behaviour when probed at macroscopic scales. Those universal properties, largely determining their physical characteristics, are revealed by the powerful renormalization group (RG) procedure, which systematically retains "slow" degrees of freedom and integrates out the rest. However, the important degrees of freedom may be difficult to identify. Here we demonstrate a machine learning algorithm capable of identifying the relevant degrees of freedom and executing RG steps iteratively without any prior knowledge about the system. We introduce an artificial neural network based on a model-independent, information-theoretic characterization of a real-space RG procedure, performing this task. We apply the algorithm to classical statistical physics problems in one and two dimensions. We demonstrate RG flow and extract the Ising critical exponent. Our results demonstrate that machine learning techniques can extract abstract physical concepts and consequently become an integral part of theory- and model-building.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1704.06279v2},
	Author = {Koch-Janusz, Maciej and Ringel, Zohar},
	Doi = {10.1038/s41567-018-0081-4},
	Eprint = {arXiv:1704.06279v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1704.06279.pdf:pdf},
	Issn = {17452481},
	Journal = {Nature Physics},
	Number = {6},
	Pages = {578--582},
	Title = {{Mutual information, neural networks and the renormalization group}},
	Volume = {14},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/s41567-018-0081-4}}

@article{Thouless1977,
	Author = {Thouless, D J and Anderson, P W and Palmer, R G},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/2. Solution of Solvable model of a spin glass $\backslash$: Thouless - Anderson - Palmer.pdf:pdf},
	Journal = {Phil. Mag.},
	Number = {3},
	Pages = {593--601},
	Title = {{Solution of `Solvable Model of a Spin-Glass'}},
	Volume = {35},
	Year = {1977}}


@article{Higgins2018,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1812.02230v1},
	Author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
	Eprint = {arXiv:1812.02230v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Towards a Definition of Disentangled Representations.pdf:pdf},
	Pages = {1--29},
	Title = {{Towards a Definition of Disentangled Representations}},
	Year = {2018}}

@misc{Sherrington1975,
	Author = {Sherrington, D and Kirkpatrick, S},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/3. Solvable model of a spin glass - D.Sherrington S.Kirkpatrick.pdf:pdf},
	Pages = {1792--1796},
	Title = {{Solvable Model of a Spin Glass}},
	Volume = {35},
	Year = {1975}}

@article{Crisanti2015,
	Abstract = {The Replica Fourier Transform is the generalization of the discrete Fourier Transform to quantities defined on an ultrametric tree. It finds use in conjunction of the replica method used to study thermodynamics properties of disordered systems such as spin glasses. Its definition is presented in a systematic and simple form and its use illustrated with some representative examples. In particular we give a detailed discussion of the diagonalization in the Replica Fourier Space of the Hessian matrix of the Gaussian fluctuations about the mean field saddle point of spin glass theory. The general results are finally discussed for a generic spherical spin glass model, where the Hessian can be computed analytically.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1410.4023v1},
	Author = {Crisanti, A. and {De Dominicis}, C.},
	Doi = {10.1016/j.nuclphysb.2014.12.002},
	Eprint = {arXiv:1410.4023v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1410.4023.pdf:pdf},
	Issn = {05503213},
	Journal = {Nuclear Physics B},
	Keywords = {09,11 2014,18 ac,3,v 1},
	Pages = {73--105},
	Title = {{Replica Fourier Transform: Properties and applications}},
	Volume = {891},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.nuclphysb.2014.12.002}}

@article{Huang2013a,
	Abstract = {The statistical picture of the solution space for a binary perceptron is studied. The binary perceptron learns a random classification of input random patterns by a set of binary synaptic weights. The learning of this network is difficult especially when the pattern (constraint) density is close to the capacity, which is supposed to be intimately related to the structure of the solution space. The geometrical organization is elucidated by the entropy landscape from a reference configuration and of solution-pairs separated by a given Hamming distance in the solution space. We evaluate the entropy at the annealed level as well as replica symmetric level and the mean field result is confirmed by the numerical simulations on single instances using the proposed message passing algorithms. From the first landscape (a random configuration as a reference), we see clearly how the solution space shrinks as more constraints are added. From the second landscape of solution-pairs, we deduce the coexistence of clustering and freezing in the solution space.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1304.2850v2},
	Author = {Huang, Haiping and Wong, K. Y.Michael and Kabashima, Yoshiyuki},
	Doi = {10.1088/1751-8113/46/37/375002},
	Eprint = {arXiv:1304.2850v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/34. Entropy landscape of solutions in the binary perceptron problem.pdf:pdf},
	Issn = {17518113},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {37},
	Title = {{Entropy landscape of solutions in the binary perceptron problem}},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8113/46/37/375002}}

@article{Hosaka2002a,
	Abstract = {The performance of a lossy data compression scheme for uniformly biased Boolean messages is investigated via methods of statistical mechanics. Inspired by a formal similarity to the storage capacity problem in neural network research, we utilize a perceptron of which the transfer function is appropriately designed in order to compress and decode the messages. Employing the replica method, we analytically show that our scheme can achieve the optimal performance known in the framework of lossy compression in most cases when the code length becomes infinite. The validity of the obtained results is numerically confirmed.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0207356},
	Author = {Hosaka, Tadaaki and Kabashima, Yoshiyuki and Nishimori, Hidetoshi},
	Doi = {10.1103/PhysRevE.66.066126},
	Eprint = {0207356},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0207356.pdf:pdf},
	Issn = {1063651X},
	Journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
	Number = {6},
	Pages = {8},
	Pmid = {12513366},
	Primaryclass = {cond-mat},
	Title = {{Statistical mechanics of lossy data compression using a nonmonotonic perceptron}},
	Volume = {66},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.66.066126}}

@article{Gualdi2015,
	Abstract = {We propose a simple framework to understand commonly observed crisis waves in macroeconomic Agent Based models, that is also relevant to a variety of other physical or biological situations where synchronization occurs. We compute exactly the phase diagram of the model and the location of the synchronization transition in parameter space. Many modifications and extensions can be studied, confirming that the synchronization transition is extremely robust against various sources of noise or imperfections. },
	Archiveprefix = {arXiv},
	Arxivid = {1409.3296},
	Author = {Gualdi, Stanislao and Bouchaud, Jean Philippe and Cencetti, Giulia and Tarzia, Marco and Zamponi, Francesco},
	Doi = {10.1103/PhysRevLett.114.088701},
	Eprint = {1409.3296},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1409.3296v1.pdf:pdf},
	Issn = {10797114},
	Journal = {Physical Review Letters},
	Number = {8},
	Pages = {1--5},
	Title = {{Endogenous crisis waves: Stochastic model with synchronized collective Behavior}},
	Volume = {114},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.114.088701}}

@article{Hosaka2002,
	Abstract = {The performance of a lossy data compression scheme for uniformly biased Boolean messages is investigated via methods of statistical mechanics. Inspired by a formal similarity to the storage capacity problem in neural network research, we utilize a perceptron of which the transfer function is appropriately designed in order to compress and decode the messages. Employing the replica method, we analytically show that our scheme can achieve the optimal performance known in the framework of lossy compression in most cases when the code length becomes infinite. The validity of the obtained results is numerically confirmed.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0207356},
	Author = {Hosaka, Tadaaki and Kabashima, Yoshiyuki and Nishimori, Hidetoshi},
	Doi = {10.1103/PhysRevE.66.066126},
	Eprint = {0207356},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.66.066126.pdf:pdf},
	Issn = {1063651X},
	Journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
	Number = {6},
	Pages = {8},
	Pmid = {12513366},
	Primaryclass = {cond-mat},
	Title = {{Statistical mechanics of lossy data compression using a nonmonotonic perceptron}},
	Volume = {66},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.66.066126}}

@article{Bouchaud2013,
	Abstract = {Financial and economic history is strewn with bubbles and crashes, booms and busts, crises and upheavals of all sorts. Understanding the origin of these events is arguably one of the most important problems in economic theory. In this paper, we review recent efforts to include heterogeneities and interactions in models of decision. We argue that the Random Field Ising model (RFIM) indeed provides a unifying framework to account for many collective socio-economic phenomena that lead to sudden ruptures and crises. We discuss different models that can capture potentially destabilising self-referential feedback loops, induced either by herding, i.e. reference to peers, or trending, i.e. reference to the past, and account for some of the phenomenology missing in the standard models. We discuss some empirically testable predictions of these models, for example robust signatures of RFIM-like herding effects, or the logarithmic decay of spatial correlations of voting patterns. One of the most striking result, inspired by statistical physics methods, is that Adam Smith's invisible hand can badly fail at solving simple coordination problems. We also insist on the issue of time-scales, that can be extremely long in some cases, and prevent socially optimal equilibria to be reached. As a theoretical challenge, the study of so-called "detailed-balance" violating decision rules is needed to decide whether conclusions based on current models (that all assume detailed-balance) are indeed robust and generic.},
	Archiveprefix = {arXiv},
	Arxivid = {1209.0453},
	Author = {Bouchaud, Jean Philippe},
	Doi = {10.1007/s10955-012-0687-3},
	Eprint = {1209.0453},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/JStatPhys.pdf:pdf},
	Isbn = {0022-4715},
	Issn = {00224715},
	Journal = {Journal of Statistical Physics},
	Keywords = {Avalanches,Collective phenomena,Crises,Random field Ising model},
	Number = {3-4},
	Pages = {567--606},
	Title = {{Crises and Collective Socio-Economic Phenomena: Simple Models and Challenges}},
	Volume = {151},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10955-012-0687-3}}

@article{Diffusion1995,
	Author = {Diffusion, Atomic},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/48. Weight space structure and representations.pdf:pdf},
	Number = {12},
	Pages = {2364--2367},
	Title = {{Physical review letters 18}},
	Volume = {75},
	Year = {1995}}

@article{Barbier2019a,
  title={Concentration of multi-overlaps for random ferromagnetic spin models},
  author={Barbier, J and Chan, CL and Macris, N},
  journal={arXiv preprint arXiv:1901.06521},
  year={2019}
}

@article{Gabrie2019,
	Archiveprefix = {arXiv},
	Arxivid = {1910.00285},
	Author = {Gabri{\'{e}}, Marylou and Barbier, Jean and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
	Eprint = {1910.00285},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1910.00285.pdf:pdf},
	Number = {1},
	Pages = {1--27},
	Title = {{Blind calibration for compressed sensing: State evolution and an online algorithm}},
	Url = {http://arxiv.org/abs/1910.00285},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1910.00285}}

@article{Gyorgyi2001,
	Abstract = {In this article we review the framework for spontaneous replica symmetry breaking. Subsequently that is applied to the example of the statistical mechanical description of the storage properties of a McCulloch-Pitts neuron, i.e., simple perceptron. It is shown that in the neuron problem, the general formula that is at the core of all problems admitting Parisi's replica symmetry breaking ansatz with a one-component order parameter appears. The details of Parisi's method are reviewed extensively, with regard to the wide range of systems where the method may be applied. Parisi's partial differential equation and related differential equations are discussed, and the Green function technique is introduced for the calculation of replica averages, the key to determining the averages of physical quantities. The Green function of the Fokker-Planck equation due to Sompolinsky turns out to play the role of the statistical mechanical Green function in the graph rules for replica correlators. The subsequently obtained graph rules involve only tree graphs, as appropriate for a mean-field-like model. The lowest order Ward-Takahashi identity is recovered analytically and shown to lead to the Goldstone modes in continuous replica symmetry breaking phases. The need for a replica symmetry breaking theory in the storage problem of the neuron has arisen due to the thermodynamical instability of formerly given solutions. Variational forms for the neuron's free energy are derived in terms of the order parameter function x(q), for different prior distribution of synapses. Analytically in the high temperature limit and numerically in generic cases various phases are identified, among them is one similar to the Parisi phase in long-range interaction spin glasses. Extensive quantities like the error per pattern change slightly with respect to the known unstable solutions, but there is a significant difference in the distribution of non-extensive quantities like the synaptic overlaps and the pattern storage stability parameter. A simulation result is also reviewed and compared with the prediction of the theory. {\textcopyright} 2001 Elsevier Science B.V.},
	Author = {Gy{\"{o}}rgyi, G.},
	Isbn = {0370-1573},
	Issn = {03701573},
	Journal = {Physics Report},
	Keywords = {07.05.Mh,61.43. - j,75.10Nr,84.35.+i,Neural networks,Pattern storage,Replica symmetry breaking,Spin glasses},
	Number = {4-5},
	Pages = {263--392},
	Primaryclass = {cond-mat},
	Title = {{Techniques of replica symmetry breaking and the storage problem of the McCulloch-Pitts neuron}},
	Volume = {342},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/S0370-1573(00)00073-9}}

@article{Biroli2018,
	Abstract = {In this paper and in the companion one we address the problem of identifying the effective theory that describes the statistics of the fluctuations of what is thought to be the relevant order parameter for glassy systems---the overlap field with an equilibrium reference configuration---close to the putative thermodynamic glass transition. Our starting point is the mean-field theory of glass formation which relies on the existence of a complex free-energy landscape with a multitude of metastable states. In this paper, we focus on archetypal mean-field models possessing this type of free-energy landscape and set up the framework to determine the exact effective theory. We show that the effective theory at the mean-field level is generically of the random-field + random-bond Ising type. We also discuss what are the main issues concerning the extension of our result to finite-dimensional systems. This extension is addressed in detail in the companion paper.},
	Archiveprefix = {arXiv},
	Arxivid = {1807.06293},
	Author = {Biroli, G. and Cammarota, C. and Tarjus, G. and Tarzia, M.},
	Eprint = {1807.06293},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1807.06293.pdf:pdf},
	Title = {{Random-Field Ising like effective theory of the glass transition: I Mean-Field Models}},
	Url = {http://arxiv.org/abs/1807.06293},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1807.06293}}

@article{Ros2018a,
	Abstract = {We analyze the energy barriers that allow escapes from a given local minimum in a mean-field model of glasses. We perform this study by using the Kac-Rice method and computing the typical number of critical points of the energy function at a given distance from the minimum. We analyze their Hessian in terms of random matrix theory and show that for a certain regime of energies and distances critical points are index-one saddles and are associated to barriers. We find that the lowest barrier, important for activated dynamics at low temperature, is strictly lower than the "threshold" level above which saddles proliferate. We characterize how the quenched complexity of barriers, important for activated process at finite temperature, depends on the energy of the barrier, the energy of the initial minimum, and the distance between them. The overall picture gained from this study is expected to hold generically for mean-field models of the glass transition.},
	Archiveprefix = {arXiv},
	Arxivid = {1809.05440},
	Author = {Ros, Valentina and Biroli, Giulio and Cammarota, Chiara},
	Eprint = {1809.05440},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1809.05440.pdf:pdf},
	Title = {{Complexity of energy barriers in mean-field glassy systems}},
	Url = {http://arxiv.org/abs/1809.05440},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1809.05440}}

@article{Mei2018,
	Abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearlyideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
	Archiveprefix = {arXiv},
	Arxivid = {1804.06561},
	Author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	Eprint = {1804.06561},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1804.06561.pdf:pdf},
	Title = {{A Mean Field View of the Landscape of Two-Layers Neural Networks}},
	Url = {http://arxiv.org/abs/1804.06561},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1804.06561}}

@article{Obuchi2009b,
	Abstract = {The weight space of the Ising perceptron in which a set of random patterns is stored is examined using the generating function of the partition function {\$}\backslashphi(n)=(1/N)\backslashlog [Z{\^{}}n]{\$} as the dimension of the weight vector {\$}N{\$} tends to infinity, where {\$}Z{\$} is the partition function and {\$}[ ... ]{\$} represents the configurational average. We utilize {\$}\backslashphi(n){\$} for two purposes, depending on the value of the ratio {\$}\backslashalpha=M/N{\$}, where {\$}M{\$} is the number of random patterns. For {\$}\backslashalpha {\textless} \backslashalpha{\_}{\{}\backslashrm s{\}}=0.833 ...{\$}, we employ {\$}\backslashphi(n){\$}, in conjunction with Parisi's one-step replica symmetry breaking scheme in the limit of {\$}n \backslashto 0{\$}, to evaluate the complexity that characterizes the number of disjoint clusters of weights that are compatible with a given set of random patterns, which indicates that, in typical cases, the weight space is equally dominated by a single large cluster of exponentially many weights and exponentially many small clusters of a single weight. For {\$}\backslashalpha {\textgreater} \backslashalpha{\_}{\{}\backslashrm s{\}}{\$}, on the other hand, {\$}\backslashphi(n){\$} is used to assess the rate function of a small probability that a given set of random patterns is atypically separable by the Ising perceptrons. We show that the analyticity of the rate function changes at {\$}\backslashalpha = \backslashalpha{\_}{\{}\backslashrm GD{\}}=1.245 ... {\$}, which implies that the dominant configuration of the atypically separable patterns exhibits a phase transition at this critical ratio. Extensive numerical experiments are conducted to support the theoretical predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {0910.2281},
	Author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
	Doi = {10.1088/1742-5468/2009/12/P12014},
	Eprint = {0910.2281},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Obuchi{\_}2009{\_}J.{\_}Stat.{\_}Mech.{\_}2009{\_}P12014.pdf:pdf},
	Issn = {1742-5468},
	Title = {{Weight space structure and analysis using a finite replica number in the Ising perceptron}},
	Url = {http://arxiv.org/abs/0910.2281{\%}0Ahttp://dx.doi.org/10.1088/1742-5468/2009/12/P12014},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2009/12/P12014}}

@article{Kabashima2016,
	Abstract = {We analyse the matrix factorization problem. Given a noisy measurement of a product of two matrices, the problem is to estimate back the original matrices. It arises in many applications such as dictionary learning, blind matrix calibration, sparse principal component analysis, blind source separation, low rank matrix completion, robust principal component analysis or factor analysis. It is also important in machine learning: unsupervised representation learning can often be studied through matrix factorization. We use the tools of statistical mechanics - the cavity and replica methods - to analyze the achievability and computational tractability of the inference problems in the setting of Bayes-optimal inference, which amounts to assuming that the two matrices have random independent elements generated from some known distribution, and this information is available to the inference algorithm. In this setting, we compute the minimal mean-squared-error achievable in principle in any computational time, and the error that can be achieved by an efficient approximate message passing algorithm. The computation is based on the asymptotic state-evolution analysis of the algorithm. The performance that our analysis predicts, both in terms of the achieved mean-squared-error, and in terms of sample complexity, is extremely promising and motivating for a further development of the algorithm.},
	Archiveprefix = {arXiv},
	Arxivid = {1402.1298},
	Author = {Kabashima, Yoshiyuki and Krzakala, Florent and Mezard, Marc and Sakata, Ayaka and Zdeborova, Lenka},
	Doi = {10.1109/TIT.2016.2556702},
	Eprint = {1402.1298},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/30. Phase transitions and sample complexity in Bayes-optimal matrix factorization.pdf:pdf},
	Isbn = {1402.1298},
	Issn = {00189448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Statistical inference,computational barriers,dictionary learning,message passing algorithms,phase transitions,probabilistic matrix factorization,sparse coding,statistical and computational tradeoff,statistical physics},
	Number = {7},
	Pages = {4228--4265},
	Title = {{Phase Transitions and Sample Complexity in Bayes-Optimal Matrix Factorization}},
	Volume = {62},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIT.2016.2556702}}

@article{Watkin1993,
	Abstract = {A summary is presented of the statistical mechanical theory of learning$\backslash$na rule with a neural network, a rapidly advancing area which is$\backslash$nclosely related to other inverse problems frequently encountered$\backslash$nby physicists. By emphasizing the relationship between neural networks$\backslash$nand strongly interacting physical systems, such as spin glasses,$\backslash$nthe authors show how learning theory has provided a workshop in$\backslash$nwhich to develop new, exact analytical techniques.},
	Author = {Watkin, Timothy L H and Rau, Albrecht},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/49. The statistical mechanics of learning a rule.pdf:pdf},
	Journal = {Review of Modern Physics},
	Keywords = {reviews},
	Pages = {499--556},
	Title = {{The statistical mechanics of learning a rule}},
	Volume = {65},
	Year = {1993}}

@article{Huang2014a,
	Abstract = {Supervised learning in a binary perceptron is able to classify an extensive number of random patterns by a proper assignment of binary synaptic weights. However, to find such assignments in practice, is quite a nontrivial task. The relation between the weight space structure and the algorithmic hardness has not yet been fully understood. To this end, we analytically derive the Franz-Parisi potential for the binary preceptron problem, by starting from an equilibrium solution of weights and exploring the weight space structure around it. Our result reveals the geometrical organization of the weight space$\backslash$textemdash the weight space is composed of isolated solutions, rather than clusters of exponentially many close-by solutions. The point-like clusters far apart from each other in the weight space explain the previously observed glassy behavior of stochastic local search heuristics.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1408.1784v1},
	Author = {Huang, Haiping and Kabashima, Yoshiyuki},
	Doi = {10.1103/PhysRevE.90.052813},
	Eprint = {arXiv:1408.1784v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/35. Origin of the computational hardness for learning.pdf:pdf},
	Issn = {15502376},
	Journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	Number = {5},
	Title = {{Origin of the computational hardness for learning with binary synapses}},
	Volume = {90},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.90.052813}}

@article{Sokolov2012,
	Abstract = {A suspension of microswimmers, the simplest realization of active matter, exhibits novel material properties: the emergence of collective motion, reduction in viscosity, increase in diffusivity, and extraction of useful energy. Bacterial dynamics in dilute suspensions suggest that hydrodynamic interactions and collisions between the swimmers lead to collective motion at higher concentrations. On the example of aerobic bacteria Bacillus subtilis, we report on spatial and temporal correlation functions measurements of collective state for various swimming speeds and concentrations. The experiments produced a puzzling result: while the energy injection rate is proportional to the swimming speed and concentration, the correlation length remains practically constant upon small speeds where random tumbling of bacteria dominates. It highlights two fundamental mechanisms: hydrodynamic interactions and collisions; for both of these mechanisms, the change of the swimming speed or concentration alters an overall time scale.},
	Archiveprefix = {arXiv},
	Arxivid = {0710.3256v2},
	Author = {Sokolov, Andrey and Aranson, Igor S.},
	Doi = {10.1103/PhysRevLett.109.248109},
	Eprint = {0710.3256v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0710.3256.pdf:pdf},
	Isbn = {0066-4189},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {24},
	Pages = {1--58},
	Pmid = {23368392},
	Title = {{Physical properties of collective motion in suspensions of bacteria}},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.109.248109}}

@article{Kinzel2003,
	Author = {{Biehl, Michael; Caticha, Nestor; Opper, Manfred; Villmann}, Thomas Published},
	Doi = {10.1007/978-3-662-05594-6_8},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/ESANN2019{\_}Biehl{\_}tutorial(2).pdf:pdf},
	Journal = {Adaptivity and Learning},
	Pages = {77--88},
	Title = {{Statistical Physics of Learning and Generalization}},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-662-05594-6_8}}

@article{Montanari2018,
	Author = {Montanari, Andrea},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Montanari.pdf:pdf},
	Pages = {1--4},
	Title = {{Lecture on neural networks}},
	Volume = {2},
	Year = {2018}}

@article{Sakata2012,
	Abstract = {Finding a basis matrix (dictionary) by which objective signals are represented sparsely is of major relevance in various scientific and technological fields. We consider a problem to learn a dictionary from a set of training signals. We employ techniques of statistical mechanics of disordered systems to evaluate the size of the training set necessary to typically succeed in the dictionary learning. The results indicate that the necessary size is much smaller than previously estimated, which theoretically supports and/or encourages the use of dictionary learning in practical situations.},
	Archiveprefix = {arXiv},
	Arxivid = {1203.6178v2},
	Author = {Sakata, Ayaka and Kabashima, Yoshiyuki},
	Doi = {10.1209/0295-5075/103/28008},
	Eprint = {1203.6178v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Sakata{\_}2013{\_}EPL{\_}103{\_}28008.pdf:pdf},
	Issn = {02955075},
	Journal = {Europhysics Letters},
	Pages = {2--5},
	Title = {{Statistical Mechanics of Dictionary Learning}},
	Url = {http://arxiv.org/abs/1203.6178},
	Volume = {28008},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1203.6178},
	Bdsk-Url-2 = {http://dx.doi.org/10.1209/0295-5075/103/28008}}

@article{Crisanti1992,
	Abstract = {The relaxational dynamics for local spin autocor- relations of the spherical p-spin interaction spin-glass model is studied in the mean field limit. In the high temperature and high external field regime, the dynamics is ergodic and simi- lar to the behaviour in known liquid-glass transition models. In the static limit, we recover the replica symmetric solution for the long time correlation. This phase becomes unstable on a critical line in the (T, h) plane, where critical slowing down is observed with a cross-over to power law decay of the correlation function o( t-{\~{}}, with an exponent u varying along the critical line. For low temperatures and low fields, ergodicity in phase space is broken. For small fields the tran- sition is discontinuous, and approaching this transition from above, two tong time scales are seen to emerge. This dynam- ical transition lies at a somewhat higher temperature than the one obtained within replica theory. For larger fields the tran- sition becomes continuous at some tricritical point. The low temperature phase with broken ergodicity is studied within a modified equilibrium theory and alternatively for adiabatic cooling across the transition line. This latter scheme yields rather detailed insight into the formation and structure of the ergodic components.},
	Author = {Crisanti, A. and Sommers, H. J.},
	Doi = {10.1007/BF01309287},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/10.1007-BF01309287.pdf:pdf},
	Issn = {07223277},
	Journal = {Zeitschrift f{\"{u}}r Physik B Condensed Matter},
	Number = {3},
	Pages = {341--354},
	Title = {{The spherical p-spin interaction spin glass model: the statics}},
	Volume = {87},
	Year = {1992},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/BF01309287}}

@article{Saad1995,
	Author = {Saad, David and Solla, Sara A},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/56. On-line learning in soft committee machine.pdf:pdf},
	Number = {2},
	Pages = {3--4},
	Title = {{Online learning in soft committee machines}},
	Volume = {52},
	Year = {1995}}

@article{Doersch2016,
	Abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	Archiveprefix = {arXiv},
	Arxivid = {1606.05908},
	Author = {Doersch, Carl},
	Eprint = {1606.05908},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1606.05908.pdf:pdf},
	Keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
	Pages = {1--23},
	Title = {{Tutorial on Variational Autoencoders}},
	Url = {http://arxiv.org/abs/1606.05908},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1606.05908}}

@article{Minka2013,
	Abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
	Author = {Minka, Thomas P.},
	Pages = {362--369},
	Booktitle = {Proceedings of the UAI},
	Title = {{Expectation Propagation for approximate Bayesian inference}},
	Year = {2001}
	}

@article{Obuchi2009,
	Abstract = {The weight space of the Ising perceptron in which a set of random patterns is stored is examined using the generating function of the partition function {\$}\backslashphi(n)=(1/N)\backslashlog [Z{\^{}}n]{\$} as the dimension of the weight vector {\$}N{\$} tends to infinity, where {\$}Z{\$} is the partition function and {\$}[ ... ]{\$} represents the configurational average. We utilize {\$}\backslashphi(n){\$} for two purposes, depending on the value of the ratio {\$}\backslashalpha=M/N{\$}, where {\$}M{\$} is the number of random patterns. For {\$}\backslashalpha {\textless} \backslashalpha{\_}{\{}\backslashrm s{\}}=0.833 ...{\$}, we employ {\$}\backslashphi(n){\$}, in conjunction with Parisi's one-step replica symmetry breaking scheme in the limit of {\$}n \backslashto 0{\$}, to evaluate the complexity that characterizes the number of disjoint clusters of weights that are compatible with a given set of random patterns, which indicates that, in typical cases, the weight space is equally dominated by a single large cluster of exponentially many weights and exponentially many small clusters of a single weight. For {\$}\backslashalpha {\textgreater} \backslashalpha{\_}{\{}\backslashrm s{\}}{\$}, on the other hand, {\$}\backslashphi(n){\$} is used to assess the rate function of a small probability that a given set of random patterns is atypically separable by the Ising perceptrons. We show that the analyticity of the rate function changes at {\$}\backslashalpha = \backslashalpha{\_}{\{}\backslashrm GD{\}}=1.245 ... {\$}, which implies that the dominant configuration of the atypically separable patterns exhibits a phase transition at this critical ratio. Extensive numerical experiments are conducted to support the theoretical predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {0910.2281},
	Author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
	Doi = {10.1088/1742-5468/2009/12/P12014},
	Eprint = {0910.2281},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Obuchi{\_}2009{\_}J.{\_}Stat.{\_}Mech.{\_}2009{\_}P12014(2).pdf:pdf},
	Issn = {1742-5468},
	Title = {{Weight space structure and analysis using a finite replica number in the Ising perceptron}},
	Url = {http://arxiv.org/abs/0910.2281{\%}0Ahttp://dx.doi.org/10.1088/1742-5468/2009/12/P12014},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2009/12/P12014}}

@article{Lempitsky2018,
	Abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep{\_}image{\_}prior .},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1711.10925v3},
	Author = {Lempitsky, Victor and Vedaldi, Andrea and Ulyanov, Dmitry},
	Doi = {10.1109/CVPR.2018.00984},
	Eprint = {arXiv:1711.10925v3},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1711.10925.pdf:pdf},
	Isbn = {9781538664209},
	Issn = {10636919},
	Journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	Pages = {9446--9454},
	Title = {{Deep Image Prior}},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2018.00984}}

@article{Manoel2017a,
	Author = {Manoel, Andre},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Andre.pdf:pdf},
	Title = {{Inference in Multi-Layer Graphical Models}},
	Year = {2017}}

@article{Majer1993a,
	Author = {Majer, P. and Engel, A. and Zippelius, A.},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {24},
	Pages = {7405--7416},
	Title = {{Perceptrons above saturation}},
	Volume = {26},
	Year = {1993}}

@article{Chung2017,
	Abstract = {Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals associated with different physical features (e.g., orientation, pose, scale, location, and intensity) of the same perceptual object. Object recognition and discrimination requires classifying the manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems give rise to invariant object classification and recognition is a fundamental problem in brain theory as well as in machine learning. Here we study the ability of a readout network to classify objects from their perceptual manifold representations. We develop a statistical mechanical theory for the linear classification of manifolds with arbitrary geometry revealing a remarkable relation to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold dimension are introduced which can explain the classification capacity for manifolds of various geometries. The general theory is demonstrated on a number of representative manifolds, including L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting of finite sample points, and orientation manifolds which arise from neurons tuned to respond to a continuous angle variable, such as object orientation. The effects of label sparsity on the classification capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and manifold radius. Theoretical predictions are corroborated by numerical simulations using recently developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory and its extensions provide a powerful and rich framework for applying statistical mechanics of linear classification to data arising from neuronal responses to object stimuli, as well as to artificial deep networks trained for object recognition tasks.},
	Archiveprefix = {arXiv},
	Arxivid = {1710.06487},
	Author = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
	Doi = {10.1103/PhysRevX.8.031003},
	Eprint = {1710.06487},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1710.06487.pdf:pdf},
	Issn = {2160-3308},
	Pages = {1--27},
	Title = {{Classification and Geometry of General Perceptual Manifolds}},
	Url = {http://arxiv.org/abs/1710.06487{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.8.031003},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevX.8.031003}}

@article{Richardson2018a,
	Abstract = {A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution ("mode collapse") and that using the learned models for anything other than generating samples may be very difficult. In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images.},
	Author = {Richardson, Eitan and Weiss, Yair},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/7826-on-gans-and-gmms(2).pdf:pdf},
	Issn = {10495258},
	Journal = {Advances in Neural Information Processing Systems},
	Number = {NeurIPS},
	Pages = {5847--5858},
	Title = {{On GANs and GMMs}},
	Volume = {2018-Decem},
	Year = {2018}}

@article{Bakshi,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1811.01885v1},
	Author = {Bakshi, Ainesh and Jayaram, Rajesh and Woodruff, David P},
	Eprint = {arXiv:1811.01885v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1811.01885(2).pdf:pdf},
	Title = {{Learning Two Layer Rectified Neural Networks in Polynomial Time}}}

@article{Mezard1986,
	Author = {M{\'{e}}zard, M. and Parisi, G. and Virasoro, M. A.},
	Journal = {Epl},
	Number = {2},
	Pages = {77--82},
	Title = {{SK model: The replica solution without replicas}},
	Volume = {1},
	Year = {1986}}

@article{Cavagna2003,
	Abstract = {We revisit two classic Thouless-Anderson-Palmer (TAP) studies of the Sherrington-Kirkpatrick model [Bray A J and Moore M A 1980 J. Phys. C 13, L469; De Dominicis C and Young A P, 1983 J. Phys. A 16, 2063]. By using the Becchi-Rouet-Stora-Tyutin (BRST) supersymmetry, we prove the general equivalence of TAP and replica partition functions, and show that the annealed calculation of the TAP complexity is formally identical to the quenched thermodynamic calculation of the free energy at one step level of replica symmetry breaking. The complexity we obtain by means of the BRST symmetry turns out to be considerably smaller than the previous non-symmetric value.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0210665},
	Author = {Cavagna, Andrea and Giardina, Irene and Parisi, Giorgio and M{\'{e}}zard, Marc},
	Eprint = {0210665},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/3. On the formal equivalence of the TAP and thermodynamic methods in the SK model $\backslash$: Cavagna - Giardina - Parisi - M{\'{e}}zard.pdf:pdf},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {5},
	Pages = {1175--1194},
	Primaryclass = {cond-mat},
	Title = {{On the formal equivalence of the TAP and thermodynamic methods in the SK model}},
	Volume = {36},
	Year = {2003}}

@article{Montanari2007,
	Author = {Montanari, Andrea},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/lecture-11.pdf:pdf},
	Journal = {Stat},
	Number = {5},
	Pages = {1--4},
	Title = {{Bethe-Peierls approximation : an informal introduction Bethe equations Bethe free entropy}},
	Volume = {1},
	Year = {2007}}

@article{Louart2017,
	Abstract = {This article studies the Gram random matrix model {\$}G=\backslashfrac1T\backslashSigma{\^{}}{\{}\backslashrm T{\}}\backslashSigma{\$}, {\$}\backslashSigma=\backslashsigma(WX){\$}, classically found in the analysis of random feature maps and random neural networks, where {\$}X=[x{\_}1,\backslashldots,x{\_}T]\backslashin{\{}\backslashmathbb R{\}}{\^{}}{\{}p\backslashtimes T{\}}{\$} is a (data) matrix of bounded norm, {\$}W\backslashin{\{}\backslashmathbb R{\}}{\^{}}{\{}n\backslashtimes p{\}}{\$} is a matrix of independent zero-mean unit variance entries, and {\$}\backslashsigma:{\{}\backslashmathbb R{\}}\backslashto{\{}\backslashmathbb R{\}}{\$} is a Lipschitz continuous (activation) function --- {\$}\backslashsigma(WX){\$} being understood entry-wise. By means of a key concentration of measure lemma arising from non-asymptotic random matrix arguments, we prove that, as {\$}n,p,T{\$} grow large at the same rate, the resolvent {\$}Q=(G+\backslashgamma I{\_}T){\^{}}{\{}-1{\}}{\$}, for {\$}\backslashgamma{\textgreater}0{\$}, has a similar behavior as that met in sample covariance matrix models, involving notably the moment {\$}\backslashPhi=\backslashfrac{\{}T{\}}n{\{}\backslashmathbb E{\}}[G]{\$}, which provides in passing a deterministic equivalent for the empirical spectral measure of {\$}G{\$}. Application-wise, this result enables the estimation of the asymptotic performance of single-layer random neural networks. This in turn provides practical insights into the underlying mechanisms into play in random neural networks, entailing several unexpected consequences, as well as a fast practical means to tune the network hyperparameters.},
	Archiveprefix = {arXiv},
	Arxivid = {1702.05419},
	Author = {Louart, Cosme and Liao, Zhenyu and Couillet, Romain},
	Eprint = {1702.05419},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1702.05419.pdf:pdf},
	Keywords = {60B20, 62M45},
	Title = {{A Random Matrix Approach to Neural Networks}},
	Url = {http://arxiv.org/abs/1702.05419},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1702.05419}}

@article{Gueudre2014,
	Abstract = {Finding a good compromise between the exploitation of known resources and the exploration of unknown, but potentially more profitable choices, is a general problem, which arises in many different scientific disciplines. We propose a stylized model for these exploration-exploitation situations, including population or economic growth, portfolio optimisation, evolutionary dynamics, or the problem of optimal pinning of vortices or dislocations in disordered materials. We find the exact growth rate of this model for tree-like geometries and prove the existence of an optimal migration rate in this case. Numerical simulations in the one-dimensional case confirm the generic existence of an optimum.},
	Archiveprefix = {arXiv},
	Arxivid = {1310.5114},
	Author = {Gueudr{\'{e}}, Thomas and Dobrinevski, Alexander and Bouchaud, Jean Philippe},
	Doi = {10.1103/PhysRevLett.112.050602},
	Eprint = {1310.5114},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1310.5114.pdf:pdf},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {5},
	Pmid = {24580581},
	Title = {{Explore or exploit? A generic model and an exactly solvable case}},
	Volume = {112},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.112.050602}}

@article{Zdeborova2009,
	Abstract = {Optimization is fundamental in many areas of science, from computer science and information theory to engineering and statistical physics, as well as to biology or social sciences. It typically involves a large number of variables and a cost function depending on these variables. Optimization problems in the NP-complete class are particularly difficult, it is believed that the number of operations required to minimize the cost function is in the most difficult cases exponential in the system size. However, even in an NP-complete problem the practically arising instances might, in fact, be easy to solve. The principal question we address in this thesis is: How to recognize if an NP-complete constraint satisfaction problem is typically hard and what are the main reasons for this? We adopt approaches from the statistical physics of disordered systems, in particular the cavity method developed originally to describe glassy systems. We describe new properties of the space of solutions in two of the most studied constraint satisfaction problems - random satisfiability and random graph coloring. We suggest a relation between the existence of the so-called frozen variables and the algorithmic hardness of a problem. Based on these insights, we introduce a new class of problems which we named "locked" constraint satisfaction, where the statistical description is easily solvable, but from the algorithmic point of view they are even more challenging than the canonical satisfiability.},
	Archiveprefix = {arXiv},
	Arxivid = {0806.4112},
	Author = {Zdeborov{\'{a}}, Lenka},
	Doi = {10.2478/v10155-010-0096-6},
	Eprint = {0806.4112},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0806.4112.pdf:pdf},
	Issn = {03230465},
	Journal = {Acta Physica Slovaca},
	Keywords = {Average computational complexity,Belief propagation,Cavity method,Clustering of solutions,Constraint satisfaction problems,Random graphs coloring problem,Reconstruction on trees,Replica symmetry breaking,Satisfiability threshold,Spin glasses},
	Number = {3},
	Pages = {169--303},
	Title = {{Statistical physics of hard optimization problems}},
	Volume = {59},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.2478/v10155-010-0096-6}}

@article{Bouchaud2013a,
	Abstract = {Financial and economic history is strewn with bubbles and crashes, booms and busts, crises and upheavals of all sorts. Understanding the origin of these events is arguably one of the most important problems in economic theory. In this paper, we review recent efforts to include heterogeneities and interactions in models of decision. We argue that the Random Field Ising model (RFIM) indeed provides a unifying framework to account for many collective socio-economic phenomena that lead to sudden ruptures and crises. We discuss different models that can capture potentially destabilising self-referential feedback loops, induced either by herding, i.e. reference to peers, or trending, i.e. reference to the past, and account for some of the phenomenology missing in the standard models. We discuss some empirically testable predictions of these models, for example robust signatures of RFIM-like herding effects, or the logarithmic decay of spatial correlations of voting patterns. One of the most striking result, inspired by statistical physics methods, is that Adam Smith's invisible hand can badly fail at solving simple coordination problems. We also insist on the issue of time-scales, that can be extremely long in some cases, and prevent socially optimal equilibria to be reached. As a theoretical challenge, the study of so-called "detailed-balance" violating decision rules is needed to decide whether conclusions based on current models (that all assume detailed-balance) are indeed robust and generic.},
	Archiveprefix = {arXiv},
	Arxivid = {1209.0453},
	Author = {Bouchaud, Jean Philippe},
	Doi = {10.1007/s10955-012-0687-3},
	Eprint = {1209.0453},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Crises+and+collective+socio-economic+phenomena.pdf:pdf},
	Isbn = {0022-4715},
	Issn = {00224715},
	Journal = {Journal of Statistical Physics},
	Keywords = {Avalanches,Collective phenomena,Crises,Random field Ising model},
	Number = {3-4},
	Pages = {567--606},
	Title = {{Crises and Collective Socio-Economic Phenomena: Simple Models and Challenges}},
	Volume = {151},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10955-012-0687-3}}

@article{Yedidia2002,
	Abstract = {This is an updated and expanded version of TR2000-26, but it is still in draft form. Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. We show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. More importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved.},
	Author = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/TR2001-16.pdf:pdf},
	Number = {TR2002-35},
	Title = {{Bethe free energy Kikuchi approximations and BP algorithms}},
	Url = {http://www.merl.com},
	Year = {2002},
	Bdsk-Url-1 = {http://www.merl.com}}

@article{Kamilov2014,
	Archiveprefix = {arXiv},
	Arxivid = {1207.3859},
	Author = {Kamilov, Ulugbek S and Member, Student and Rangan, Sundeep and Fletcher, Alyson K and Unser, Michael},
	Eprint = {1207.3859},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1207.3859.pdf:pdf},
	Number = {5},
	Pages = {2969--2985},
	Title = {{Parameter Estimation and Applications to Sparse Learning}},
	Volume = {60},
	Year = {2014}}

@article{Rangan,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1010.5141v2},
	Author = {Rangan, Sundeep},
	Eprint = {arXiv:1010.5141v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1010.5141.pdf:pdf},
	Pages = {1--22},
	Title = {{Generalized Approximate Message Passing for Estimation with Random Linear Mixing}}}

@article{Zamponi2010,
	Abstract = {These lecture notes focus on the mean field theory of spin glasses, with particular emphasis on the presence of a very large number of metastable states in these systems. This phenomenon, and some of its physical consequences, will be discussed in details for fully-connected models and for models defined on random lattices. This will be done using the replica and cavity methods. These notes have been prepared for a course of the PhD program in Statistical Mechanics at SISSA, Trieste and at the University of Rome "Sapienza". Part of the material is reprinted from other lecture notes, and when this is done a reference is obviously provided to the original.},
	Archiveprefix = {arXiv},
	Arxivid = {1008.4844},
	Author = {Zamponi, Francesco},
	Isbn = {1478643770823},
	Title = {{Mean field theory of spin glasses}},
	Year = {2010}
	}

@article{Schwaeze1992,
	Author = {Schwarze, H. and Hertz, J.},
	Doi = {10.1209/0295-5075/20/4/015},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/44. Generalization in a large committee machine.pdf:pdf},
	Issn = {12864854},
	Journal = {Epl},
	Number = {4},
	Pages = {375--380},
	Title = {{Generalization in a large committee machine}},
	Volume = {20},
	Year = {1992},
	Bdsk-Url-1 = {http://dx.doi.org/10.1209/0295-5075/20/4/015}}

@article{Model1989,
	Author = {Model, Minimum Torque-change},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Bounds on the learning capacity of some multi-layer networks.pdf:pdf},
	Pages = {89--101},
	Title = {{Biological Cybernetics}},
	Volume = {101},
	Year = {1989}}

@article{Park1996,
	Author = {Park, Kibeom and Kwon, C. and Park, Youngah},
	Doi = {10.1088/0305-4470/29/7/012},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Kibeom{\_}Park{\_}1996{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}29{\_}012.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {7},
	Pages = {1397--1409},
	Title = {{One-step replica-symmetry-breaking solution for a perceptron learning with weight mismatch}},
	Volume = {29},
	Year = {1996},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/29/7/012}}

@article{Franz2017,
	Abstract = {Random constraint satisfaction problems (CSP) have been studied extensively using statistical physics techniques. They provide a benchmark to study average case scenarios instead of the worst case one. The interplay between statistical physics of disordered systems and computer science has brought new light into the realm of computational complexity theory, by introducing the notion of clustering of solutions, related to replica symmetry breaking. However, the class of problems in which clustering has been studied often involve discrete degrees of freedom: standard random CSPs are random K-SAT (aka disordered Ising models) or random coloring problems (aka disordered Potts models). In this work we consider instead problems that involve continuous degrees of freedom. The simplest prototype of these problems is the perceptron. Here we discuss in detail the full phase diagram of the model. In the regions of parameter space where the problem is non-convex, leading to multiple disconnected clusters of solutions, the solution is critical at the SAT/UNSAT threshold and lies in the same universality class of the jamming transition of soft spheres. We show how the critical behavior at the satisfiability threshold emerges, and we compute the critical exponents associated to the approach to the transition from both the SAT and UNSAT phase. We conjecture that there is a large universality class of non-convex continuous CSPs whose SAT-UNSAT threshold is described by the same scaling solution.},
	Archiveprefix = {arXiv},
	Arxivid = {1702.06919},
	Author = {Franz, Silvio and Parisi, Giorgio and Sevelev, Maksim and Urbani, Pierfrancesco and Zamponi, Francesco},
	Doi = {10.21468/SciPostPhys.2.3.019},
	Eprint = {1702.06919},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/33. Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems.pdf:pdf},
	Issn = {2542-4653},
	Pages = {1--28},
	Title = {{Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems}},
	Url = {http://arxiv.org/abs/1702.06919{\%}0Ahttp://dx.doi.org/10.21468/SciPostPhys.2.3.019},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.21468/SciPostPhys.2.3.019}}

@article{Panchenko2012,
	Abstract = {The goal of this paper is to review some of the main ideas that emerged from the attempts to confirm mathematically the predictions of the celebrated Parisi ansatz in the Sherrington-Kirkpatrick model. We try to focus on the big picture while sketching the proofs of only a few selected results, but an interested reader can find most of the missing details in [31] and [44].},
	Archiveprefix = {arXiv},
	Arxivid = {1211.1094},
	Author = {Panchenko, Dmitry},
	Doi = {10.1007/s10955-012-0586-7},
	Eprint = {1211.1094},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1211.1094.pdf:pdf},
	Keywords = {2010,60k35,82b44,mathematics subject classification,parisi ansatz,sherrington-kirkpatrick model},
	Number = {2},
	Pages = {1--25},
	Title = {{The Sherrington-Kirkpatrick model: an overview}},
	Url = {http://arxiv.org/abs/1211.1094{\%}0Ahttp://dx.doi.org/10.1007/s10955-012-0586-7},
	Volume = {2},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10955-012-0586-7}}

@article{Crisanti2011,
	Abstract = {To test the stability of the Parisi solution near T=0, we study the spectrum of the Hessian of the Sherrington-Kirkpatrick model near T=0, whose eigenvalues are the masses of the bare propagators in the expansion around the mean-field solution. In the limit {\$}T\backslashll 1{\$} two regions can be identified. In the first region, for {\$}x{\$} close to 0, where {\$}x{\$} is the Parisi replica symmetry breaking scheme parameter, the spectrum of the Hessian is not trivial and maintains the structure of the full replica symmetry breaking state found at higher temperatures. In the second region {\$}T\backslashll x \backslashleq 1{\$} as {\$}T\backslashto 0{\$}, the components of the Hessian become insensitive to changes of the overlaps and the bands typical of the full replica symmetry breaking state collapse. In this region only two eigenvalues are found: a null one and a positive one, ensuring stability for {\$}T\backslashll 1{\$}. In the limit {\$}T\backslashto 0{\$} the width of the first region shrinks to zero and only the positive and null eigenvalues survive. As byproduct we enlighten the close analogy between the static Parisi replica symmetry breaking scheme and the multiple time-scales approach of dynamics, and compute the static susceptibility showing that it equals the static limit of the dynamic susceptibility computed via the modified fluctuation dissipation theorem.},
	Archiveprefix = {arXiv},
	Arxivid = {1101.5233},
	Author = {Crisanti, A and {De Dominicis}, C},
	Doi = {10.1088/1751-8113/44/11/115006},
	Eprint = {1101.5233},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1101.5233.pdf:pdf},
	Issn = {1751-8113},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {11},
	Pages = {115006},
	Title = {{Stability of the Parisi solution for the Sherrington--Kirkpatrick model near {\textless}i{\textgreater}T{\textless}/i{\textgreater} = 0}},
	Url = {http://stacks.iop.org/1751-8121/44/i=11/a=115006?key=crossref.cd8e9b19bfc5371977c75151104002a5},
	Volume = {44},
	Year = {2011},
	Bdsk-Url-1 = {http://stacks.iop.org/1751-8121/44/i=11/a=115006?key=crossref.cd8e9b19bfc5371977c75151104002a5},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1751-8113/44/11/115006}}

@article{Plefka2002,
	Abstract = {The stability of the TAP mean-field equations is reanalyzed with the conclusion that the exclusive reason for the breakdown at the spin glass instability is an inconsistency for the value of the local susceptibility. A new alternative approach leads to modified equations which are in complete agreement with the original ones above the instability. Essentially altered results below the instability are presented and the consequences for the dynamical mean-field equations are discussed.},
	Author = {Plefka, T.},
	Doi = {10.1209/epl/i2002-00457-7},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/T.{\_}Plefka{\_}2002{\_}EPL{\_}58{\_}892.pdf:pdf},
	Issn = {02955075},
	Journal = {Europhysics Letters},
	Number = {6},
	Pages = {892--898},
	Title = {{Modified TAP equations for the SK spin glass}},
	Volume = {58},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1209/epl/i2002-00457-7}}

@article{Ricci-Tersenghi2018,
	Abstract = {Many inference problems, notably the stochastic block model (SBM) that generates a random graph with a hidden community structure, undergo phase transitions as a function of the signal-to-noise ratio, and can exhibit hard phases in which optimal inference is information-theoretically possible but computationally challenging. In this paper we refine this description in two ways. In a qualitative perspective we emphasize the existence of more generic phase diagrams with a hybrid-hard phase in which it is computationally easy to reach a non-trivial inference accuracy, but computationally hard to match the information theoretically optimal one. We support this discussion by quantitative expansions of the functional cavity equations that describe inference problems on sparse graphs. These expansions shed light on the existence of hybrid-hard phases, for a large class of planted constraint satisfaction problems, and on the question of the tightness of the Kesten-Stigum (KS) bound for the associated tree reconstruction problem. Our results show that the instability of the trivial fixed point is not a generic evidence for the Bayes-optimality of the message passing algorithms. We clarify in particular the status of the symmetric SBM with 4 communities and of the tree reconstruction of the associated Potts model: in the assortative (ferromagnetic) case the KS bound is always tight, whereas in the disassortative (antiferromagnetic) case we exhibit an explicit criterion involving the degree distribution that separates a large degree regime where the KS bound is tight and a low degree regime where it is not. We also investigate the SBM with 2 communities of different sizes, a.k.a. the asymmetric Ising model, and describe quantitatively its computational gap as a function of its asymmetry. We complement this study with numerical simulations of the Belief Propagation iterative algorithm.},
	Archiveprefix = {arXiv},
	Arxivid = {1806.11013},
	Author = {Ricci-Tersenghi, Federico and Semerjian, Guilhem and Zdeborova, Lenka},
	Eprint = {1806.11013},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1806.11013.pdf:pdf},
	Pages = {1--63},
	Title = {{Typology of phase transitions in Bayesian inference problems}},
	Url = {http://arxiv.org/abs/1806.11013},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1806.11013}}

@article{Lei2019,
	Abstract = {We study the problem of inverting a deep generative model with ReLU activations. Inversion corresponds to finding a latent code vector that explains observed measurements as much as possible. In most prior works this is performed by attempting to solve a non-convex optimization problem involving the generator. In this paper we obtain several novel theoretical results for the inversion problem. We show that for the realizable case, single layer inversion can be performed exactly in polynomial time, by solving a linear program. Further, we show that for multiple layers, inversion is NP-hard and the pre-image set can be non-convex. For generative models of arbitrary depth, we show that exact recovery is possible in polynomial time with high probability, if the layers are expanding and the weights are randomly selected. Very recent work analyzed the same problem for gradient descent inversion. Their analysis requires significantly higher expansion (logarithmic in the latent dimension) while our proposed algorithm can provably reconstruct even with constant factor expansion. We also provide provable error bounds for different norms for reconstructing noisy observations. Our empirical validation demonstrates that we obtain better reconstructions when the latent dimension is large.},
	Archiveprefix = {arXiv},
	Arxivid = {1906.07437},
	Author = {Lei, Qi and Jalal, Ajil and Dhillon, Inderjit S. and Dimakis, Alexandros G.},
	Eprint = {1906.07437},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1906.07437.pdf:pdf},
	Number = {2},
	Pages = {1--16},
	Title = {{Inverting Deep Generative models, One layer at a time}},
	Url = {http://arxiv.org/abs/1906.07437},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1906.07437}}

@article{Marquardt2017,
	Author = {Marquardt, Florian},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/machine-learning-physicists.pdf:pdf},
	Title = {{Machine Learning for Physicists}},
	Year = {2017}}

@article{Cherrier2003,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:cond-mat/0211695v1},
	Author = {Cherrier, R. and Dean, D. S. and Lef{\`{e}}vre, A.},
	Doi = {10.1103/PhysRevE.67.046112},
	Eprint = {0211695v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0211695.pdf:pdf},
	Issn = {1063651X},
	Journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
	Number = {4},
	Pages = {10},
	Primaryclass = {arXiv:cond-mat},
	Title = {{Role of the interaction matrix in mean-field spin glass models}},
	Volume = {67},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.67.046112}}

@article{Rangan2017a,
	Abstract = {The standard linear regression (SLR) problem is to recover a vector {\$}\backslashmathbf{\{}x{\}}{\^{}}0{\$} from noisy linear observations {\$}\backslashmathbf{\{}y{\}}=\backslashmathbf{\{}Ax{\}}{\^{}}0+\backslashmathbf{\{}w{\}}{\$}. The approximate message passing (AMP) algorithm recently proposed by Donoho, Maleki, and Montanari is a computationally efficient iterative approach to SLR that has a remarkable property: for large i.i.d.$\backslash$ sub-Gaussian matrices {\$}\backslashmathbf{\{}A{\}}{\$}, its per-iteration behavior is rigorously characterized by a scalar state-evolution whose fixed points, when unique, are Bayes optimal. The AMP algorithm, however, is fragile in that even small deviations from the i.i.d.$\backslash$ sub-Gaussian model can cause the algorithm to diverge. This paper considers a "vector AMP" (VAMP) algorithm and shows that VAMP has a rigorous scalar state-evolution that holds under a much broader class of large random matrices {\$}\backslashmathbf{\{}A{\}}{\$}: those that are right-orthogonally invariant. After performing an initial singular value decomposition (SVD) of {\$}\backslashmathbf{\{}A{\}}{\$}, the per-iteration complexity of VAMP can be made similar to that of AMP. In addition, the fixed points of VAMP's state evolution are consistent with the replica prediction of the minimum mean-squared error recently derived by Tulino, Caire, Verd$\backslash$'u, and Shamai. Numerical experiments are used to confirm the effectiveness of VAMP and its consistency with state-evolution predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1610.03082v2},
	Author = {Rangan, Sundeep and Schniter, Philip and Fletcher, Alyson K.},
	Doi = {10.1109/ISIT.2017.8006797},
	Eprint = {arXiv:1610.03082v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1610.03082(2).pdf:pdf},
	Isbn = {9781509040964},
	Issn = {21578095},
	Journal = {IEEE International Symposium on Information Theory - Proceedings},
	Keywords = {Belief propagation,Compressive sensing,Inference algorithms,Message passing,Random matrices},
	Number = {8},
	Pages = {1588--1592},
	Title = {{Vector approximate message passing}},
	Volume = {1},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISIT.2017.8006797}}

@article{Mezard2017a,
	Abstract = {Motivated by recent progress in using restricted Boltzmann machines as preprocessing algorithms for deep neural network, we revisit the mean-field equations (belief-propagation and TAP equations) in the best understood such machine, namely the Hopfield model of neural networks, and we explicit how they can be used as iterative message-passing algorithms, providing a fast method to compute the local polarizations of neurons. In the "retrieval phase" where neurons polarize in the direction of one memorized pattern, we point out a major difference between the belief propagation and TAP equations : the set of belief propagation equations depends on the pattern which is retrieved, while one can use a unique set of TAP equations. This makes the latter method much better suited for applications in the learning process of restricted Boltzmann machines. In the case where the patterns memorized in the Hopfield model are not independent, but are correlated through a combinatorial structure, we show that the TAP equations have to be modified. This modification can be seen either as an alteration of the reaction term in TAP equations, or, more interestingly, as the consequence of message passing on a graphical model with several hidden layers, where the number of hidden layers depends on the depth of the correlations in the memorized patterns. This layered structure is actually necessary when one deals with more general restricted Boltzmann machines.},
	Archiveprefix = {arXiv},
	Arxivid = {1608.01558},
	Author = {M{\'{e}}zard, Marc},
	Doi = {10.1103/PhysRevE.95.022117},
	Eprint = {1608.01558},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1608.01558.pdf:pdf},
	Issn = {24700053},
	Journal = {Physical Review E},
	Number = {2},
	Title = {{Mean-field message-passing equations in the Hopfield model and its generalizations}},
	Volume = {95},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.95.022117}}

@article{Barbier2019_interpolation,
	Author = {Jean Barbier and Nicolas Macris},
	Date-Modified = {2020-09-15 19:34:55 +0000},
	Doi = {10.1088/1751-8121/ab2735},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {29},
	Pages = {294002},
	Publisher = {{IOP} Publishing},
	Title = {The adaptive interpolation method for proving replica formulas. Applications to the Curie{\textendash}Weiss and Wigner spike models},
	Volume = {52},
	Year = 2019,
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8121/ab2735}}

@article{Rattray1999,
	Abstract = {Natural gradient descent is a principled method for adapting the parameters of a statistical model on-line using an underlying Riemannian parameter space to redefine the direction of steepest descent. The algorithm is examined via methods of statistical physics which accurately characterize both transient and asymptotic behavior. A solution of the learning dynamics is obtained for the case of multilayer neural network training in the limit of large input dimension. We find that natural gradient learning leads to optimal asymptotic performance and outperforms gradient descent in the transient, significantly shortening or even removing plateaus in the transient generalization performance which typically hamper gradient descent training. 87.10.+e, 02.50.-r, 05.20.-y},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:cond-mat/9901212v1},
	Author = {Rattray, Magnus and Saad, David},
	Eprint = {9901212v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.59.4523.pdf:pdf},
	Number = {4},
	Pages = {4523--4532},
	Primaryclass = {arXiv:cond-mat},
	Title = {{Analysis of Natural Gradient Descent for Multilayer Neural Networks}},
	Volume = {59},
	Year = {1999}}

@article{Yaida2018,
	Abstract = {The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.},
	Archiveprefix = {arXiv},
	Arxivid = {1810.00004},
	Author = {Yaida, Sho},
	Doi = {10.1515/cog-2013-0031},
	Eprint = {1810.00004},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1810.00004.pdf:pdf},
	Isbn = {1810.00004v1},
	Issn = {09365907},
	Pages = {1--12},
	Title = {{Fluctuation-dissipation relations for stochastic gradient descent}},
	Url = {http://arxiv.org/abs/1810.00004},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1810.00004},
	Bdsk-Url-2 = {http://dx.doi.org/10.1515/cog-2013-0031}}

@article{Hand2018,
	Abstract = {The phase retrieval problem asks to recover a natural signal {\$}y{\_}0 \backslashin \backslashmathbb{\{}R{\}}{\^{}}n{\$} from {\$}m{\$} quadratic observations, where {\$}m{\$} is to be minimized. As is common in many imaging problems, natural signals are considered sparse with respect to a known basis, and the generic sparsity prior is enforced via {\$}\backslashell{\_}1{\$} regularization. While successful in the realm of linear inverse problems, such {\$}\backslashell{\_}1{\$} methods have encountered possibly fundamental limitations, as no computationally efficient algorithm for phase retrieval of a {\$}k{\$}-sparse signal has been proven to succeed with fewer than {\$}O(k{\^{}}2\backslashlog n){\$} generic measurements, exceeding the theoretical optimum of {\$}O(k \backslashlog n){\$}. In this paper, we propose a novel framework for phase retrieval by 1) modeling natural signals as being in the range of a deep generative neural network {\$}G : \backslashmathbb{\{}R{\}}{\^{}}k \backslashrightarrow \backslashmathbb{\{}R{\}}{\^{}}n{\$} and 2) enforcing this prior directly by optimizing an empirical risk objective over the domain of the generator. Our formulation has provably favorable global geometry for gradient methods, as soon as {\$}m = O(kd{\^{}}2\backslashlog n){\$}, where {\$}d{\$} is the depth of the network. Specifically, when suitable deterministic conditions on the generator and measurement matrix are met, we construct a descent direction for any point outside of a small neighborhood around the unique global minimizer and its negative multiple, and show that such conditions hold with high probability under Gaussian ensembles of multilayer fully-connected generator networks and measurement matrices. This formulation for structured phase retrieval thus has two advantages over sparsity based methods: 1) deep generative priors can more tightly represent natural signals and 2) information theoretically optimal sample complexity. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms sparse phase retrieval methods.},
	Author = {Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/8127-phase-retrieval-under-a-generative-prior.pdf:pdf},
	Issn = {10495258},
	Journal = {Advances in Neural Information Processing Systems},
	Number = {NeurIPS},
	Pages = {9136--9146},
	Title = {{Phase retrieval under a generative prior}},
	Volume = {2018-December},
	Year = {2018}}

@article{Schniter2016,
	Abstract = {The generalized linear model (GLM), where a random vector {\$}\backslashboldsymbol{\{}x{\}}{\$} is observed through a noisy, possibly nonlinear, function of a linear transform output {\$}\backslashboldsymbol{\{}z{\}}=\backslashboldsymbol{\{}Ax{\}}{\$}, arises in a range of applications such as robust regression, binary classification, quantized compressed sensing, phase retrieval, photon-limited imaging, and inference from neural spike trains. When {\$}\backslashboldsymbol{\{}A{\}}{\$} is large and i.i.d. Gaussian, the generalized approximate message passing (GAMP) algorithm is an efficient means of MAP or marginal inference, and its performance can be rigorously characterized by a scalar state evolution. For general {\$}\backslashboldsymbol{\{}A{\}}{\$}, though, GAMP can misbehave. Damping and sequential-updating help to robustify GAMP, but their effects are limited. Recently, a "vector AMP" (VAMP) algorithm was proposed for additive white Gaussian noise channels. VAMP extends AMP's guarantees from i.i.d. Gaussian {\$}\backslashboldsymbol{\{}A{\}}{\$} to the larger class of rotationally invariant {\$}\backslashboldsymbol{\{}A{\}}{\$}. In this paper, we show how VAMP can be extended to the GLM. Numerical experiments show that the proposed GLM-VAMP is much more robust to ill-conditioning in {\$}\backslashboldsymbol{\{}A{\}}{\$} than damped GAMP.},
	Archiveprefix = {arXiv},
	Arxivid = {1612.01186},
	Author = {Schniter, Philip and Rangan, Sundeep and Fletcher, Alyson K.},
	Eprint = {1612.01186},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1612.01186.pdf:pdf},
	Title = {{Vector Approximate Message Passing for the Generalized Linear Model}},
	Url = {http://arxiv.org/abs/1612.01186},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1612.01186}}

@article{Arora2019,
	Abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
	Archiveprefix = {arXiv},
	Arxivid = {1901.08584},
	Author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	Eprint = {1901.08584},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1901.08584.pdf:pdf},
	Title = {{Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks}},
	Url = {http://arxiv.org/abs/1901.08584},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1901.08584}}

@article{Bouchaud2016,
	Author = {Bouchaud, M Jean-philippe and Saad, M David},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1610.04337.pdf:pdf},
	Title = {{de l ' Universit{\'{e}} de recherche Paris Sciences et Lettres Spectral Inference Methods on Sparse Graphs : Theory and Applications M{\'{e}}thodes spectrales d ' inf{\'{e}}rence sur les graphes parcimonieux Th{\'{e}}orie et applications Soutenue par Alaa Saade le 3 octobre 2016}},
	Year = {2016}}

@article{Chung2017a,
	Abstract = {Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals associated with different physical features (e.g., orientation, pose, scale, location, and intensity) of the same perceptual object. Object recognition and discrimination requires classifying the manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems give rise to invariant object classification and recognition is a fundamental problem in brain theory as well as in machine learning. Here we study the ability of a readout network to classify objects from their perceptual manifold representations. We develop a statistical mechanical theory for the linear classification of manifolds with arbitrary geometry revealing a remarkable relation to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold dimension are introduced which can explain the classification capacity for manifolds of various geometries. The general theory is demonstrated on a number of representative manifolds, including L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting of finite sample points, and orientation manifolds which arise from neurons tuned to respond to a continuous angle variable, such as object orientation. The effects of label sparsity on the classification capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and manifold radius. Theoretical predictions are corroborated by numerical simulations using recently developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory and its extensions provide a powerful and rich framework for applying statistical mechanics of linear classification to data arising from neuronal responses to object stimuli, as well as to artificial deep networks trained for object recognition tasks.},
	Archiveprefix = {arXiv},
	Arxivid = {1710.06487},
	Author = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
	Doi = {10.1103/PhysRevX.8.031003},
	Eprint = {1710.06487},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1710.06487(2).pdf:pdf},
	Issn = {2160-3308},
	Pages = {1--24},
	Title = {{Classification and Geometry of General Perceptual Manifolds}},
	Url = {http://arxiv.org/abs/1710.06487{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.8.031003},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevX.8.031003}}

@article{Fyodorov2018,
	Abstract = {An encryption of a signal {\$}{\{}\backslashbf s{\}}\backslashin\backslashmathbb{\{}R{\^{}}N{\}}{\$} is a random mapping {\$}{\{}\backslashbf s{\}}\backslashmapsto \backslashtextbf{\{}y{\}}=(y{\_}1,\backslashldots,y{\_}M){\^{}}T\backslashin \backslashmathbb{\{}R{\}}{\^{}}M{\$} which can be corrupted by an additive noise. Given the Encryption Redundancy Parameter (ERP) {\$}\backslashmu=M/N\backslashge 1{\$}, the signal strength parameter {\$}R=\backslashsqrt{\{}\backslashsum{\_}i s{\_}i{\^{}}2/N{\}}{\$}, and the ('bare') noise-to-signal ratio (NSR) {\$}\backslashgamma\backslashge 0{\$}, we consider the problem of reconstructing {\$}{\{}\backslashbf s{\}}{\$} from its corrupted image by a Least Square Scheme for a certain class of random Gaussian mappings. The problem is equivalent to finding the configuration of minimal energy in a certain version of spherical spin glass model, with squared Gaussian-distributed random potential. We use the Parisi replica symmetry breaking scheme to evaluate the mean overlap {\$}p{\_}{\{}\backslashinfty{\}}\backslashin [0,1]{\$} between the original signal and its recovered image (known as 'estimator') as {\$}N\backslashto \backslashinfty{\$}, which is a measure of the quality of the signal reconstruction. We explicitly analyze the general case of linear-quadratic family of random mappings and discuss the full {\$}p{\_}{\{}\backslashinfty{\}} (\backslashgamma){\$} curve. When nonlinearity exceeds a certain threshold but redundancy is not yet too big, the replica symmetric solution is necessarily broken in some interval of NSR. We show that encryptions with a nonvanishing linear component permit reconstructions with {\$}p{\_}{\{}\backslashinfty{\}}{\textgreater}0{\$} for any {\$}\backslashmu{\textgreater}1{\$} and any {\$}\backslashgamma{\textless}\backslashinfty{\$}, with {\$}p{\_}{\{}\backslashinfty{\}}\backslashsim \backslashgamma{\^{}}{\{}-1/2{\}}{\$} as {\$}\backslashgamma\backslashto \backslashinfty{\$}. In contrast, for the case of purely quadratic nonlinearity, for any ERP {\$}\backslashmu{\textgreater}1{\$} there exists a threshold NSR value {\$}\backslashgamma{\_}c(\backslashmu){\$} such that {\$}p{\_}{\{}\backslashinfty{\}}=0{\$} for {\$}\backslashgamma{\textgreater}\backslashgamma{\_}c(\backslashmu){\$} making the reconstruction impossible. The behaviour close to the threshold is given by {\$}p{\_}{\{}\backslashinfty{\}}\backslashsim (\backslashgamma{\_}c-\backslashgamma){\^{}}{\{}3/4{\}}{\$} and is controlled by the replica symmetry breaking mechanism.},
	Archiveprefix = {arXiv},
	Arxivid = {1805.06982},
	Author = {Fyodorov, Yan V},
	Eprint = {1805.06982},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1805.06982.pdf:pdf},
	Pages = {1--33},
	Title = {{A spin glass model for reconstructing nonlinearly encrypted signals corrupted by noise}},
	Url = {http://arxiv.org/abs/1805.06982},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1805.06982}}

@article{Obuchi2009a,
	Abstract = {The weight space of the Ising perceptron in which a set of random patterns is stored is examined using the generating function of the partition function {\$}\backslashphi(n)=(1/N)\backslashlog [Z{\^{}}n]{\$} as the dimension of the weight vector {\$}N{\$} tends to infinity, where {\$}Z{\$} is the partition function and {\$}[ ... ]{\$} represents the configurational average. We utilize {\$}\backslashphi(n){\$} for two purposes, depending on the value of the ratio {\$}\backslashalpha=M/N{\$}, where {\$}M{\$} is the number of random patterns. For {\$}\backslashalpha {\textless} \backslashalpha{\_}{\{}\backslashrm s{\}}=0.833 ...{\$}, we employ {\$}\backslashphi(n){\$}, in conjunction with Parisi's one-step replica symmetry breaking scheme in the limit of {\$}n \backslashto 0{\$}, to evaluate the complexity that characterizes the number of disjoint clusters of weights that are compatible with a given set of random patterns, which indicates that, in typical cases, the weight space is equally dominated by a single large cluster of exponentially many weights and exponentially many small clusters of a single weight. For {\$}\backslashalpha {\textgreater} \backslashalpha{\_}{\{}\backslashrm s{\}}{\$}, on the other hand, {\$}\backslashphi(n){\$} is used to assess the rate function of a small probability that a given set of random patterns is atypically separable by the Ising perceptrons. We show that the analyticity of the rate function changes at {\$}\backslashalpha = \backslashalpha{\_}{\{}\backslashrm GD{\}}=1.245 ... {\$}, which implies that the dominant configuration of the atypically separable patterns exhibits a phase transition at this critical ratio. Extensive numerical experiments are conducted to support the theoretical predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {0910.2281},
	Author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
	Doi = {10.1088/1742-5468/2009/12/P12014},
	Eprint = {0910.2281},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1. Weight space structure and analysis using ad finite replica number in the Ising perceptron.pdf:pdf},
	Issn = {1742-5468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Month = {dec},
	Number = {12},
	Pages = {P12014},
	Title = {{Weight space structure and analysis using a finite replica number in the Ising perceptron}},
	Url = {http://stacks.iop.org/1742-5468/2009/i=12/a=P12014?key=crossref.a275d866de9c7571ee60474f8fca9af5},
	Volume = {2009},
	Year = {2009},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-5468/2009/i=12/a=P12014?key=crossref.a275d866de9c7571ee60474f8fca9af5},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-5468/2009/12/P12014}}

@inproceedings{Baity-Jesi2018,
	Author = {Baity-Jest, Marco and Sagun, Levcnt and Mario, Geiger and Spiglery, Stefano and Arous, Gerard Ben and Cammarota, Chiara and LeCun, Yann and Vvyart, Matthieu and Biroli, Giu Jio},
	Booktitle = {35th International Conference on Machine Learning, ICML 2018},
	Organization = {International Machine Learning Society (IMLS)},
	Pages = {526--535},
	Title = {Comparing Dynamics: Deep Neural Networks versus Glassy Systems},
	Year = {2018}}

@article{Giudice1989,
	Author = {Giudice, P Del and Franz, S and Virasoro, M A},
	Doi = {10.1051/jphys:01989005002012100},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/ajp-jphys{\_}1989{\_}50{\_}2{\_}121{\_}0.pdf:pdf},
	Isbn = {Preprint No. 612},
	Issn = {0302-0738},
	Journal = {J. Phys. France},
	Number = {2},
	Pages = {121--134},
	Title = {{Perceptron beyond the limit of capacity}},
	Volume = {50},
	Year = {1989},
	Bdsk-Url-1 = {http://dx.doi.org/10.1051/jphys:01989005002012100}}

@article{Rotskoff2018,
	Abstract = {Neural networks, a central tool in machine learning, have demonstrated remarkable, high fidelity performance on image recognition and classification tasks. These successes evince an ability to accurately represent high dimensional functions, potentially of great use in computational and applied mathematics. That said, there are few rigorous results about the representation error and trainability of neural networks. Here we characterize both the error and the scaling of the error with the size of the network by reinterpreting the standard optimization algorithm used in machine learning applications, stochastic gradient descent, as the evolution of a particle system with interactions governed by a potential related to the objective or "loss" function used to train the network. We show that, when the number {\$}n{\$} of parameters is large, the empirical distribution of the particles descends on a convex landscape towards a minimizer at a rate independent of {\$}n{\$}. We establish a Law of Large Numbers and a Central Limit Theorem for the empirical distribution, which together show that the approximation error of the network universally scales as {\$}O(n{\^{}}{\{}-1{\}}){\$}. Remarkably, these properties do not depend on the dimensionality of the domain of the function that we seek to represent. Our analysis also quantifies the scale and nature of the noise introduced by stochastic gradient descent and provides guidelines for the step size and batch size to use when training a neural network. We illustrate our findings on examples in which we train neural network to learn the energy function of the continuous 3-spin model on the sphere. The approximation error scales as our analysis predicts in as high a dimension as {\$}d=25{\$}.},
	Archiveprefix = {arXiv},
	Arxivid = {1805.00915},
	Author = {Rotskoff, Grant M. and Vanden-Eijnden, Eric},
	Eprint = {1805.00915},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1805.00915.pdf:pdf},
	Pages = {1--35},
	Title = {{Neural Networks as Interacting Particle Systems: Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error}},
	Url = {http://arxiv.org/abs/1805.00915},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1805.00915}}

@article{Rangan2017,
	Abstract = {The standard linear regression (SLR) problem is to recover a vector {\$}\backslashmathbf{\{}x{\}}{\^{}}0{\$} from noisy linear observations {\$}\backslashmathbf{\{}y{\}}=\backslashmathbf{\{}Ax{\}}{\^{}}0+\backslashmathbf{\{}w{\}}{\$}. The approximate message passing (AMP) algorithm recently proposed by Donoho, Maleki, and Montanari is a computationally efficient iterative approach to SLR that has a remarkable property: for large i.i.d.$\backslash$ sub-Gaussian matrices {\$}\backslashmathbf{\{}A{\}}{\$}, its per-iteration behavior is rigorously characterized by a scalar state-evolution whose fixed points, when unique, are Bayes optimal. The AMP algorithm, however, is fragile in that even small deviations from the i.i.d.$\backslash$ sub-Gaussian model can cause the algorithm to diverge. This paper considers a "vector AMP" (VAMP) algorithm and shows that VAMP has a rigorous scalar state-evolution that holds under a much broader class of large random matrices {\$}\backslashmathbf{\{}A{\}}{\$}: those that are right-orthogonally invariant. After performing an initial singular value decomposition (SVD) of {\$}\backslashmathbf{\{}A{\}}{\$}, the per-iteration complexity of VAMP can be made similar to that of AMP. In addition, the fixed points of VAMP's state evolution are consistent with the replica prediction of the minimum mean-squared error recently derived by Tulino, Caire, Verd$\backslash$'u, and Shamai. Numerical experiments are used to confirm the effectiveness of VAMP and its consistency with state-evolution predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1610.03082v2},
	Author = {Rangan, Sundeep and Schniter, Philip and Fletcher, Alyson K.},
	Doi = {10.1109/ISIT.2017.8006797},
	Eprint = {arXiv:1610.03082v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1610.03082.pdf:pdf},
	Isbn = {9781509040964},
	Issn = {21578095},
	Journal = {IEEE International Symposium on Information Theory - Proceedings},
	Keywords = {Belief propagation,Compressive sensing,Inference algorithms,Message passing,Random matrices},
	Number = {8},
	Pages = {1588--1592},
	Title = {{Vector approximate message passing}},
	Volume = {1},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISIT.2017.8006797}}

@article{Woodworth2019,
	Abstract = {A recent line of work studies overparametrized neural networks in the ``kernel regime,'' i.e.{\~{}}when the network behaves during training as a kernelized linear predictor, and thus training with gradient descent has the effect of finding the minimum RKHS norm solution. This stands in contrast to other studies which demonstrate how gradient descent on overparametrized multilayer networks can induce rich implicit biases that are not RKHS norms. Building on an observation by Chizat and Bach, we show how the scale of the initialization controls the transition between the ``kernel'' (aka lazy) and ``deep'' (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a simple two-layer model that already exhibits an interesting and meaningful transition between the kernel and deep regimes, and we demonstrate the transition for more complex matrix factorization models.},
	Archiveprefix = {arXiv},
	Arxivid = {1906.05827},
	Author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
	Eprint = {1906.05827},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1906.05827.pdf:pdf},
	Pages = {1--16},
	Title = {{Kernel and Deep Regimes in Overparametrized Models}},
	Url = {http://arxiv.org/abs/1906.05827},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1906.05827}}

@article{Hansel1992,
	Abstract = {The supervised learning of a rule that can be realized by a multilayer network (the teacher) functioning as a parity machine with K = 2 hidden units and nonoverlapping receptive fields is studied. The student network is supposed to have the same architecture as the teacher. Application of statistical mechanics shows that when the number of examples is smaller than a critical value P* the trained network is unable to generalize the rule from the examples. Numerical simulations exhibiting this phenomenon are discussed.},
	Author = {Hansel, D. and Mato, G. and Meunier, C.},
	Doi = {10.1209/0295-5075/20/5/015},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/54. Memorization Without Generalization in a Multilayered Neural Network.pdf:pdf},
	Issn = {12864854},
	Journal = {Epl},
	Number = {5},
	Pages = {471--476},
	Title = {{Memorization without generalization in a multilayered neural network}},
	Volume = {20},
	Year = {1992},
	Bdsk-Url-1 = {http://dx.doi.org/10.1209/0295-5075/20/5/015}}

@article{Heckel2018,
	Abstract = {Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.},
	Archiveprefix = {arXiv},
	Arxivid = {1810.03982},
	Author = {Heckel, Reinhard and Hand, Paul},
	Eprint = {1810.03982},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1810.03982.pdf:pdf},
	Title = {{Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks}},
	Url = {http://arxiv.org/abs/1810.03982},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1810.03982}}

@article{Krzakala2012a,
	Abstract = {Compressed sensing is a signal processing method that acquires data directly in a compressed form. This allows one to make less measurements than what was considered necessary to record a signal, enabling faster or more precise measurement protocols in a wide range of applications. Using an interdisciplinary approach, we have recently proposed in [arXiv:1109.4424] a strategy that allows compressed sensing to be performed at acquisition rates approaching to the theoretical optimal limits. In this paper, we give a more thorough presentation of our approach, and introduce many new results. We present the probabilistic approach to reconstruction and discuss its optimality and robustness. We detail the derivation of the message passing algorithm for reconstruction and expectation max- imization learning of signal-model parameters. We further develop the asymptotic analysis of the corresponding phase diagrams with and without measurement noise, for different distribution of signals, and discuss the best possible reconstruction performances regardless of the algorithm. We also present new efficient seeding matrices, test them on synthetic data and analyze their performance asymptotically.},
	Archiveprefix = {arXiv},
	Arxivid = {1206.3953},
	Author = {Krzakala, Florent and M{\'{e}}zard, Marc and Sausset, Francois and Sun, Yifan and Zdeborov{\'{a}}, Lenka},
	Doi = {10.1088/1742-5468/2012/08/P08009},
	Eprint = {1206.3953},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/publijournalstatmech.pdf:pdf},
	Isbn = {1742-5468},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {cavity and replica method,error correcting codes,message-passing algorithms,statistical inference},
	Number = {8},
	Title = {{Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices}},
	Volume = {2012},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2012/08/P08009}}

@article{Sarkar2018,
	Abstract = {We consider the problem of jointly recovering the vector {\$}\backslashboldsymbol{\{}b{\}}{\$} and the matrix {\$}\backslashboldsymbol{\{}C{\}}{\$} from noisy measurements {\$}\backslashboldsymbol{\{}Y{\}} = \backslashboldsymbol{\{}A{\}}(\backslashboldsymbol{\{}b{\}})\backslashboldsymbol{\{}C{\}} + \backslashboldsymbol{\{}W{\}}{\$}, where {\$}\backslashboldsymbol{\{}A{\}}(\backslashcdot){\$} is a known affine linear function of {\$}\backslashboldsymbol{\{}b{\}}{\$} (i.e., {\$}\backslashboldsymbol{\{}A{\}}(\backslashboldsymbol{\{}b{\}})=\backslashboldsymbol{\{}A{\}}{\_}0+\backslashsum{\_}{\{}i=1{\}}{\^{}}Q b{\_}i \backslashboldsymbol{\{}A{\}}{\_}i{\$} with known matrices {\$}\backslashboldsymbol{\{}A{\}}{\_}i{\$}). This problem has applications in matrix completion, robust PCA, dictionary learning, self-calibration, blind deconvolution, joint-channel/symbol estimation, compressive sensing with matrix uncertainty, and many other tasks. To solve this bilinear recovery problem, we propose the Bilinear Adaptive Vector Approximate Message Passing (BAd-VAMP) algorithm. We demonstrate numerically that the proposed approach is competitive with other state-of-the-art approaches to bilinear recovery, including lifted VAMP and Bilinear GAMP.},
	Archiveprefix = {arXiv},
	Arxivid = {1809.00024},
	Author = {Sarkar, Subrata and Fletcher, Alyson K. and Rangan, Sundeep and Schniter, Philip},
	Eprint = {1809.00024},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Bilinear Recovery using Adaptive Vector-AMP.pdf:pdf},
	Number = {2},
	Pages = {1--12},
	Title = {{Bilinear Recovery using Adaptive Vector-AMP}},
	Url = {http://arxiv.org/abs/1809.00024},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1809.00024}}

@article{Barbier2017,
	Abstract = {We study the approximate message-passing decoder for sparse superposition coding on the additive white Gaussian noise channel and extend our preliminary work [1]. We use heuristic statistical-physics-based tools such as the cavity and the replica methods for the statistical analysis of the scheme. While superposition codes asymptotically reach the Shannon capacity, we show that our iterative decoder is limited by a phase transition similar to the one that happens in Low Density Parity check codes. We consider two solutions to this problem, that both allow to reach the Shannon capacity: i) a power allocation strategy and ii) the use of spatial coupling, a novelty for these codes that appears to be promising. We present in particular simulations suggesting that spatial coupling is more robust and allows for better reconstruction at finite code lengths. Finally, we show empirically that the use of a fast Hadamard-based operator allows for an efficient reconstruction, both in terms of computational time and memory, and the ability to deal with very large messages.},
	Archiveprefix = {arXiv},
	Arxivid = {1503.08040},
	Author = {Barbier, Jean and Krzakala, Florent},
	Doi = {10.1109/TIT.2017.2713833},
	Eprint = {1503.08040},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1503.08040.pdf:pdf},
	Issn = {00189448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Additive white Gaussian noise channel,Approximate message-passing,Capacity achieving,Compressed sensing,Error-correcting codes,Fast Hadamard operator,Power allocation,Replica analysis,Sparse superposition codes,Spatial coupling,State evolution},
	Number = {8},
	Pages = {4894--4927},
	Title = {{Approximate Message-Passing Decoder and Capacity Achieving Sparse Superposition Codes}},
	Volume = {63},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIT.2017.2713833}}

@article{Whyte1996,
	Author = {Whyte, W. and Sherrington, D.},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {12},
	Pages = {3063--3073},
	Title = {{Replica-symmetry breaking in perceptrons}},
	Volume = {29},
	Year = {1996}}

@article{Venturi2018,
	Abstract = {Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may be responsible for such success. In particular, several authors have noted that over-parametrization appears to act as a remedy against non-convexity. In this paper, we address this phenomenon by studying key topological properties of the loss, such as the presence or absence of "spurious valleys", defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of two-layer neural networks defined by smooth (but generally non-linear) activation functions, our main contribution is to prove that as soon as the hidden layer size matches the intrinsic dimension of the reproducing space, defined as the linear functional space generated by the activations, no spurious valleys exist, thus allowing the existence of descent directions. Our setup includes smooth activations such as polynomials, both in the empirical and population risk, and generic activations in the empirical risk case.},
	Archiveprefix = {arXiv},
	Arxivid = {1802.06384},
	Author = {Venturi, Luca and Bandeira, Afonso S. and Bruna, Joan},
	Eprint = {1802.06384},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1802.06384.pdf:pdf},
	Pages = {1--25},
	Title = {{Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys}},
	Url = {http://arxiv.org/abs/1802.06384},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1802.06384}}

@article{Franz2001,
	Abstract = {We introduce a finite-connectivity ferromagnetic model with a three-spin interaction which has a crystalline (ferromagnetic) phase as well as a glass phase. The model is not frustrated, it has a ferromagnetic equilibrium phase at low temperature which is not reached dynamically in a quench from the high-temperature phase. Instead it shows a glass transition which can be studied in detail by a one step replica-symmetry broken calculation. This spin model exhibits the main properties of the structural glass transition at a solvable mean-field level.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0103026},
	Author = {Franz, S. and M{\'{e}}zard, M. and Ricci-Tersenghi, F. and Weigt, M. and Zecchina, R.},
	Doi = {10.1209/epl/i2001-00438-4},
	Eprint = {0103026},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/01{\_}FMRTWZ{\_}EPL.pdf:pdf},
	Issn = {02955075},
	Journal = {Europhysics Letters},
	Number = {4},
	Pages = {465--471},
	Primaryclass = {cond-mat},
	Title = {{A ferromagnet with a glass transition}},
	Volume = {55},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1209/epl/i2001-00438-4}}

@article{RemiMonassonandRiccardo1996,
	Author = {{R{\'{e}}mi, Monasson and Riccardo}, Zecchina},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/48. Weight space structure and representations{\_}2.pdf:pdf},
	Number = {6},
	Pages = {9007},
	Title = {{Weight Space Structure and Internal Representations : A Direct Approach to Learning and Generalization in Multilayer Neural Networks}},
	Volume = {76},
	Year = {1996}}

@article{Shinzato2009,
	Abstract = {Learning behavior of simple perceptrons is analyzed for a teacher-student scenario in which output labels are provided by a teacher network for a set of possibly correlated input patterns, and such that the teacher and student networks are of the same type. Our main concern is the effect of statistical correlations among the input patterns on learning performance. For this purpose, we extend to the teacher-student scenario a methodology for analyzing randomly labeled patterns recently developed in Shinzato and Kabashima 2008 J. Phys. A: Math. Theor. 41 324013. This methodology is used for analyzing situations in which orthogonality of the input patterns is enhanced in order to optimize the learning performance. {\textcopyright} 2009 IOP Publishing Ltd.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:0809.1978v1},
	Author = {Shinzato, Takashi and Kabashima, Yoshiyuki},
	Doi = {10.1088/1751-8113/42/1/015005},
	Eprint = {arXiv:0809.1978v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Shinzato{\_}2009{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Theor.{\_}42{\_}015005.pdf:pdf},
	Issn = {17518113},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {1},
	Title = {{Learning from correlated patterns by simple perceptrons}},
	Volume = {42},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8113/42/1/015005}}

@article{Miolane2018,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1702.00473v3},
	Author = {{Leo Miolane}},
	Eprint = {arXiv:1702.00473v3},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1702.00473.pdf:pdf},
	Pages = {1--34},
	Title = {{Fundamental limits of low-rank matrix estimation : the non-symmetric case}}}

@article{Bouten1994,
	Abstract = {It is demonstrated that the replica symmetric saddle point is unstable$\backslash$nwhen the distribution of aligned fields displays a gap.},
	Author = {Bouten, M.},
	Doi = {10.1088/0305-4470/27/17/033},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/32. Replica symmetry instability in perceptron models.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {17},
	Pages = {6021--6023},
	Title = {{Replica symmetry instability in perceptron models}},
	Volume = {27},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/27/17/033}}

@article{MezardMarc;N1984,
	Author = {{Mezard, Marc ; N}, Soulas; G. Toulouse; M. Virasoro},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/84{\_}MPSTV{\_}JDP.pdf:pdf},
	Isbn = {0198400450508},
	Pages = {843--854},
	Title = {{Replica symmetry breaking and the nature of the spin glass phase}},
	Volume = {206},
	Year = {1984}}

@article{Maillard2019,
	Abstract = {Improved mean-field technics are a central theme of statistical physics methods applied to inference and learning. We revisit here some of these methods using high-temperature expansions for disordered systems initiated by Plefka, Georges and Yedidia. We derive the Gibbs free entropy and the subsequent self-consistent equations for a generic class of statistical models with correlated matrices and show in particular that many classical approximation schemes, such as adaptive TAP, Expectation-Consistency, or the approximations behind the Vector Approximate Message Passing algorithm all rely on the same assumptions, that are also at the heart of high-temperature expansions. We focus on the case of rotationally invariant random coupling matrices in the `high-dimensional' limit in which the number of samples and the dimension are both large, but with a fixed ratio. This encapsulates many widely studied models, such as Restricted Boltzmann Machines or Generalized Linear Models with correlated data matrices. In this general setting, we show that all the approximation schemes described before are equivalent, and we conjecture that they are exact in the thermodynamic limit in the replica symmetric phases. We achieve this conclusion by resummation of the infinite perturbation series, which generalizes a seminal result of Parisi and Potters. A rigorous derivation of this conjecture is an interesting mathematical challenge. On the way to these conclusions, we uncover several diagrammatical results in connection with free probability and random matrix theory, that are interesting independently of the rest of our work.},
	Archiveprefix = {arXiv},
	Arxivid = {1906.08479},
	Author = {Maillard, Antoine and Foini, Laura and Castellanos, Alejandro Lage and Krzakala, Florent and M{\'{e}}zard, Marc and Zdeborov{\'{a}}, Lenka},
	Eprint = {1906.08479},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1906.08479.pdf:pdf},
	Pages = {1--59},
	Title = {{High-temperature Expansions and Message Passing Algorithms}},
	Url = {http://arxiv.org/abs/1906.08479},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1906.08479}}

@article{Georges1991,
	Abstract = {Abstraet. High - temperature expansions performed at a fired-order parameter provide a simple and systematic way to derive and correct mean - field theories for statistical mechanical models. For models like spin glasses which have general couplings between ... $\backslash$n},
	Author = {Georges, Antoine and Yedidia, Jonathan S},
	Doi = {10.1088/0305-4470/24/9/024},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/A{\_}Georges{\_}1991{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}24{\_}024.pdf:pdf},
	Isbn = {0305-4470},
	Issn = {0305-4470},
	Journal = {Journal of Physics A: Mathematical and General},
	Pages = {2173--2192},
	Title = {{How to expand around mean-field theory using high-temperature expansions}},
	Volume = {24},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/24/9/024}}

@article{Diffusion1995a,
	Author = {Diffusion, Atomic},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/41. Weight space structure and representations.pdf:pdf},
	Number = {12},
	Pages = {2364--2367},
	Title = {{Physical review letters 18}},
	Volume = {75},
	Year = {1995}}

@article{Baldassi2015,
	Abstract = {We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance.We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here,we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings.We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1509.05753v1},
	Author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	Doi = {10.1103/PhysRevLett.115.128101},
	Eprint = {arXiv:1509.05753v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/SM.pdf:pdf},
	Issn = {10797114},
	Journal = {Physical Review Letters},
	Number = {12},
	Pmid = {26431018},
	Title = {{Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses}},
	Volume = {115},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.115.128101}}

@article{Hertz1993,
	Abstract = {We study generalization in large committee machines. For a model with nonoverlapping receptive fields a full replica calculation yields results qualitatively similar to those for single-layer machines. For a fully connected architecture, within the annealed approximation we find a transition from a symmetric state to one with specialized hidden units, accompanied by a discontinuous drop in the generalization error, for both binary and continuous weights. The poorly generalizing symmetric states are metastable for arbitrarily large training sets. {\textcopyright} 1993.},
	Author = {Hertz, John and Schwarze, Holm},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/55.Dicontinuous Generalization in large comiittee machines.pdf:pdf},
	Journal = {Physica A: Statistical Mechanics and its Applications},
	Number = {1-4},
	Pages = {563--569},
	Title = {{Generalization in large committee machines}},
	Volume = {200},
	Year = {1993}}

@article{Saad1995a,
	Abstract = {We present an analytic solution to the problem of on-line gradient-descent learning for two-layer neural networks with an arbitrary number of hidden units in both teacher and student networks. PACS numbers: 87. 10.+e, 02.50. --- r, 05.20. --- y Layered neural networks are of interest for their abil-ity to implement input-output maps [1]. Classification and regression tasks formulated as a map from an N-dimensional input space g onto a scalar g are real-ized through a map g = fi(g), which can be modified through changes in the internal parameters [J) specifying the strength of the interneuron couplings [2,3]. Learn-ing refers to the modification of these couplings so as to bring the map f{\~{}} implemented by the network as close as possible to a desired map f. The degree of success is monitored through the generalization error, a measure of the dissimilarity between f{\~{}} and f. Learning from examples in layered neural networks is usually formulated as an optimization problem [2,3], based on the minimization of an additive learning er-ror defined over a training set composed of P inde-pendent examples ((t', ("), with (t' = f(g"), 1 {\~{}} p {\~{}} P. Statistical physics tools for investigating the prop-erties of such models, based on the use of the replica method, have been successfully applied to the analysis of single-layer perceptrons [3] and some simplified two-layer structures (e.g., committee machines [4]). Analysis of more complicated multilayer networks is hampered by technical difficulties due to the complex structure of the solutions in a space of order parameters [5], which de-scribe in this case correlations among the various neurons in the trained network, as well as their degree of special-ization toward the implementation of the desired task. A recently introduced alternative approach investigates on line learning -[6]. In this scenario the couplings are adjusted to minimize the error after the presentation of each example. The resulting changes in [J] are de-scribed as a dynamical evolution, with the number of examples playing the role of time. The average that ac-counts for the disorder introduced by the independent random selection of an example at each time step can be performed directly, without invoking the replica method. The resulting equations of motion for the relevant order parameters characterize the structure of the space of so-lutions and allow for a computation of the generalization error. While investigating the on-line learning scenario pro-posed by Biehl and Schwarze [6], we found an unexpected result: The dynamical equations for the order parameters can be obtained analytically for a general two-layer stu-dent network composed of N input units, K hidden units, and a single linear output unit, trained to perform a task defined through a teacher network of similar architecture, except that its number M of hidden units is not necessarily equal to K. Two-layer networks with an arbitrary number of hidden units have been shown to be universal approximators [1] for N-to-one dimensional maps. Our results thus describe the learning of tasks of arbitrary complexity (general M). The complexity of the student network is also arbitrary (general K, independent of M), providing a tool to investigate realizable (K = M), overrealizable (K) M), and unrealizable (K (M) learning scenarios. Such capabilities are to be contrasted with previously available results; the equations provided in [6] can only describe a committee rnachine with K = 2 hidden units learning a linearly separable task (M = 1). In this Letter we limit our discussion to the case of the soft-committee machine [6], in which all the hid-den units are connected to the output unit with positive couplings of unit strength, and only the input-to-hidden couplings are adaptive. Consider the student network: hid-den unit i receives information from input unit r through the weight J; " and its activation under presentation of an input pattern g = (gi, . . . , g{\~{}}) is x; = J; g, with J; = (J;i, . . . , J,{\~{}}) defined as the vector of incoming weights onto the ith hidden unit. The output of the student network is tr(J, g) = g, , g(J; g), where g is the activation func-tion of the hidden units, taken here to be the error function g(x) = --- erf (x/{\~{}}2), and J --- = [J;)i; tc is the set of input-to-hidden adaptive weights. Training examples are of the form (gt', gt'). The components of the independently drawn input vectors g{\&} are uncorrelated random variables with zero mean and unit variance. The corresponding output gP is given by a deterministic teacher whose internal structure is that of a network similar to the student except for a possible difference in the number M of hidden units. Hidden unit n in the teacher network receives input information through the weight vector B, = (B " i, . . . , B " tv), and its activation under presentation of the input pattern g{\&} is yn The corresponding output is g{\&} = g " ,g(B, gt").},
	Author = {Saad, David and Solla, Sara A.},
	Doi = {10.1103/PhysRevLett.74.4337},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.74.4337.pdf:pdf},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {21},
	Pages = {4337--4340},
	Title = {{Exact solution for on-line learning in multilayer neural networks}},
	Volume = {74},
	Year = {1995},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.74.4337}}

@article{Mezard2003,
	Abstract = {In this note we explain the use of the cavity method directly at zero temperature, in the case of the spin glass on a Bethe lattice. The computation is done explicitly in the formalism equivalent to 'one step replica symmetry breaking'; we compute the energy of the global ground state, as well as the complexity of equilibrium states at a given energy. Full results are presented for a Bethe lattice with connectivity equal to three.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0207121},
	Author = {M{\'{e}}zard, Marc and Parisi, Giorgio},
	Doi = {10.1023/A:1022221005097},
	Eprint = {0207121},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0207121.pdf:pdf},
	Issn = {00224715},
	Journal = {Journal of Statistical Physics},
	Keywords = {Bethe lattice,Cavity method,Spin glass},
	Number = {1-2},
	Pages = {1--34},
	Pmid = {180806300001},
	Primaryclass = {cond-mat},
	Title = {{The Cavity Method at Zero Temperature}},
	Volume = {111},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1022221005097}}

@article{Aubin2019_storage,
  url       = {https://doi.org/10.1088%2F1751-8121%2Fab227a},
  year      = 2019,
  publisher = {{IOP} Publishing},
  volume    = {52},
  number    = {29},
  pages     = {294003},
  author    = {Benjamin Aubin and Will Perkins and Lenka Zdeborov{\'{a}}},
  title     = {Storage capacity in symmetric binary perceptrons},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  abstract  = {We study the problem of determining the capacity of the binary perceptron for two variants of the problem where the corresponding constraint is symmetric. We call these variants the rectangle-binary-perceptron (RPB) and the u-function-binary-perceptron (UBP). We show that, unlike for the usual step-function-binary-perceptron, the critical capacity in these symmetric cases is given by the annealed computation in a large region of parameter space (for all rectangular constraints and for narrow enough u-function constraints, K  <  K
*). We prove this fact (under two natural assumptions) using the first and second moment methods. We further use the second moment method to conjecture that solutions of the symmetric binary perceptrons are organized in a so-called frozen-1RSB structure, without using the replica method. We then use the replica method to estimate the capacity threshold for the UBP case when the u-function is wide K  >  K
*. We conclude that full-step-replica-symmetry breaking would have to be evaluated in order to obtain the exact capacity in this case.},
  Arxivid={1901.00314},
  Eprint={https://arxiv.org/abs/1901.00314}
}


@article{Poncet2015,
	Author = {Poncet, Alexis},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Links between Jamming, Glasses and Constraint Satisfaction Problems.pdf:pdf},
	Number = {January},
	Pages = {1--18},
	Title = {{Links between Jamming , Glasses and Constraint Satisfaction Problems}},
	Year = {2015}}

@article{Bandeira2018,
	Abstract = {In these notes we describe heuristics to predict computational-to-statistical gaps in certain statistical problems. These are regimes in which the underlying statistical problem is information-theoretically possible although no efficient algorithm exists, rendering the problem essentially unsolvable for large instances. The methods we describe here are based on mature, albeit non-rigorous, tools from statistical physics. These notes are based on a lecture series given by the authors at the Courant Institute of Mathematical Sciences in New York City, on May 16th, 2017.},
	Archiveprefix = {arXiv},
	Arxivid = {1803.11132},
	Author = {Bandeira, Afonso S. and Perry, Amelia and Wein, Alexander S.},
	Eprint = {1803.11132},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1803.11132.pdf:pdf},
	Pages = {1--22},
	Title = {{Notes on computational-to-statistical gaps: predictions using statistical physics}},
	Url = {http://arxiv.org/abs/1803.11132},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1803.11132}}

@article{Saxe2019,
	Abstract = {An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: What are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep-learning dynamics to give rise to these regularities.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1810.10531v1},
	Author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	Doi = {10.1073/pnas.1820226116},
	Eprint = {arXiv:1810.10531v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1810.10531.pdf:pdf},
	Issn = {10916490},
	Journal = {Proceedings of the National Academy of Sciences of the United States of America},
	Keywords = {Deep learning,Generative models,Neural networks,Semantic cognition},
	Number = {23},
	Pages = {11537--11546},
	Title = {{A mathematical theory of semantic development in deep neural networks}},
	Volume = {166},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1073/pnas.1820226116}}

@article{Alemi2017,
	Abstract = {A fundamental aspect of limitations in learning any computation in neural architectures is characterizing their optimal capacities. An important, widely-used neural architecture is known as autoencoders where the network reconstructs the input at the output layer via a representation at a hidden layer. Even though capacities of several neural architectures have been addressed using statistical physics methods, the capacity of autoencoder neural networks is not well-explored. Here, we analytically show that an autoencoder network of binary neurons with a hidden layer can achieve a capacity that grows exponentially with network size. The network has fixed random weights encoding a set of dense input patterns into a dense, expanded (or $\backslash$emph{\{}overcomplete{\}}) hidden layer representation. A set of learnable weights decodes the input patters at the output layer. We perform a mean-field approximation of the model to reduce the model to a perceptron problem with an input-output dependency. Carrying out Gardner's $\backslash$emph{\{}replica{\}} calculation, we show that as the expansion ratio, defined as the number of hidden units over the number of input units, increases, the autoencoding capacity grows exponentially even when the sparseness or the coding level of the hidden layer representation is changed. The replica-symmetric solution is locally stable and is in good agreement with simulation results obtained using a local learning rule. In addition, the degree of symmetry between the encoding and decoding weights monotonically increases with the expansion ratio.},
	Archiveprefix = {arXiv},
	Arxivid = {1705.07441},
	Author = {Alemi, Alireza and Abbara, Alia},
	Eprint = {1705.07441},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1705.07441.pdf:pdf},
	Title = {{Exponential Capacity in an Autoencoder Neural Network with a Hidden Layer}},
	Url = {http://arxiv.org/abs/1705.07441},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.07441}}

@article{Mei2019,
	Abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. The neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the generalization error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the generalization error is found in this overparametrized regime, often when the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the {\$}d{\$}-dimensional sphere {\$}\backslashmathbb S{\^{}}{\{}d-1{\}}{\$}, from {\$}n{\$} i.i.d. samples {\$}(\backslashboldsymbol x{\_}i, y{\_}i) \backslashin \backslashmathbb S{\^{}}{\{}d-1{\}} \backslashtimes \backslashmathbb R{\$}, {\$}i \backslashle n{\$}. We perform ridge regression on {\$}N{\$} random features of the form {\$}\backslashsigma(\backslashboldsymbol w{\_}a{\^{}}{\{}\backslashmathsf T{\}}\backslashboldsymbol x){\$}, {\$}a \backslashle N{\$}. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the generalization error, in the limit {\$}N, n, d \backslashto \backslashinfty{\$} with {\$}N/d{\$} and {\$}n/d{\$} fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon.},
	Archiveprefix = {arXiv},
	Arxivid = {1908.05355},
	Author = {Mei, Song and Montanari, Andrea},
	Eprint = {1908.05355},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1908.05355.pdf:pdf},
	Title = {{The generalization error of random features regression: Precise asymptotics and double descent curve}},
	Url = {http://arxiv.org/abs/1908.05355},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1908.05355}}

@article{Mezard1986a,
	Abstract = {(Re{\c{c}}u le 27 janvier, accepti le 23 avril 1986) R{\'{e}}sum{\'{e}}. 2014 Nous proposons et analysons une solution sym{\'{e}}trique dans les r{\'{e}}pliques des probl{\`{e}}mes de voyageurs de commerce {\`{a}} liens ind{\'{e}}pendants. Elle fournit des estimations analytiques raisonnables pour les quantit{\'{e}}s thermo-dynamiques comme la longueur du chemin le plus court. Abstract. 2014 We propose and analyse a replica symmetric solution for random link travelling salesman problems. This gives reasonable analytical estimates for thermodynamic quantities such as the length of the shortest path.},
	Author = {M{\'{e}}zard, M and Parisi, G},
	Doi = {10.1051/jphys:019860047080128500},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/86{\_}MP{\_}JDP.pdf:pdf},
	Isbn = {0198600470801},
	Issn = {0302-0738},
	Journal = {J. Physique Classification Physics Abstracts},
	Number = {1986},
	Pages = {1285--1296},
	Title = {{A replica analysis of the travelling salesman problem}},
	Volume = {47},
	Year = {1986},
	Bdsk-Url-1 = {http://dx.doi.org/10.1051/jphys:019860047080128500}}

@article{Heskes2005,
	Abstract = {This paper discusses inference problems in probabilistic graphical models that often occur in a machine learning setting. In particular it presents a unified view of several recently proposed approximation schemes. Expectation consistent approximations and expectation propagation are both shown to be related to Bethe free energies with weak consistency constraints, i.e. free energies where local approximations are only required to agree on certain statistics instead of full marginals.},
	Author = {Heskes, Tom and Opper, Manfred and Wiegerinck, Wim and Winther, Ole and Zoeter, Onno},
	Doi = {10.1088/1742-5468/2005/11/P11015},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Heskes{\_}2005{\_}J.{\_}Stat.{\_}Mech.{\_}2005{\_}P11015.pdf:pdf},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {Analysis of algorithms,Message-passing algorithms},
	Number = {11},
	Pages = {277--300},
	Title = {{Approximate inference techniques with expectation constraints}},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2005/11/P11015}}

@article{Ge2019,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1810.06793v2},
	Author = {Ge, Rong},
	Eprint = {arXiv:1810.06793v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1810.06793.pdf:pdf},
	Number = {2017},
	Pages = {1--56},
	Title = {{Learning Two-layer Neural Networks with Symmetric Inputs}},
	Year = {2019}}

@book{Neri2010,
	Abstract = {Statistical mechanics of finitely-connected systems is the study of spin models on random graphs. The methods developed to study spin models on graphs find their origins in spin-glass theory and are applicable to the description of emergent phenomena in coding theory, computer science, statistical inference in molecular biology, etc. The cornerstone of spin-glass theory is the Sherrington-Kirkpatrick model. This is a fully-connected model of spin variables interacting through random interactions drawn from a Gaussian distribution. The Sherrington-Kirkpatrick model is universal in the sense that the phase diagram of every fully-connected model with a finite variance is the same as the phase diagram of the Sherrington-Kirkpatrick model. When the variance of the distribution of couplings is infinitely large we speak of the L{\'{e}}vy spin glass. We show how this fully-connected model L{\'{e}}vy spin glass can be decomposed in a finitely-connected backbone of strong bonds interacting with the background of weak bonds. On the basis of this decomposition we derive the phase diagram of the L{\'{e}}vy spin glass. We show how to extend the tools of statistical mechanics of finitely-connected systems to non-equilibrium models where the steady state is not known a priori. Our aim is the development of an efficient and accurate message-passing algorithm for the study of non-equilibrium models, analogous to the belief-propagation algorithm for equilibrium models. We construct such an algorithm through the analysis of some simple models: models of binary spins evolving through Glauber dynamics on partially asymmetric graphs which are local tree like. Non- equilibrium systems evolving on a complex network are e.g. neurons in biological organisms, the spreading of epidemics and traffic networks. We discuss the influence of the graph structure on the retrieval properties of a neural network. Spin models on graphs can be used to study the performance of low-density parity-check codes. Low-density parity-check codes are efficient codes reaching the Shannon limit for reliable communication over memoryless symmetric channels. An important question is whether the low-density parity-check codes can also reach the Shannon limit in asymmetric channels are finite-state Markov channels. Finite v vi state Markov-channels or channels with memory in the noise. These channels are more realistic models for optical and wireless communication. For asymmetric channels the code performance depends on the codeword sent while for channels with memory the performance depends on the state during the use of the channel. The decoding of low-density parity-check code for channels with memory is realized through a message passing algorithm on a small-world hypergraph.},
	Author = {Neri, Izaak},
	Booktitle = {Status: Published},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/phd.pdf:pdf},
	Isbn = {9789086493456},
	Number = {June},
	Title = {{Statistical mechanics of spin models on graphs.}},
	Url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+mechanics+of+spin+models+on+graphs{\#}1},
	Year = {2010},
	Bdsk-Url-1 = {http://scholar.google.com/scholar?hl=en%7B%5C&%7DbtnG=Search%7B%5C&%7Dq=intitle:Statistical+mechanics+of+spin+models+on+graphs%7B%5C#%7D1}}

@article{Wilhelm2010,
	Abstract = {In this article we present tmvtnorm, an R package implementation for the truncated multivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densi-ties as well as computation of the mean and co-variance of the truncated variables. This contri-bution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.},
	Author = {Wilhelm, Stefan and BG, BGM},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/RJournal{\_}2010-1{\_}Wilhelm+Manjunath.pdf:pdf},
	Issn = {20734859},
	Journal = {Sigma},
	Number = {June},
	Pages = {25--29},
	Title = {{tmvtnorm: A package for the truncated multivariate normal distribution}},
	Url = {http://journal.r-project.org/archive/2010-1/RJournal{\_}2010-1{\_}Wilhelm+Manjunath{~}B{~}G.pdf},
	Volume = {2},
	Year = {2010},
	Bdsk-Url-1 = {http://journal.r-project.org/archive/2010-1/RJournal%7B%5C_%7D2010-1%7B%5C_%7DWilhelm+Manjunath%7B~%7DB%7B~%7DG.pdf}}

@article{BenArous2017,
	Abstract = {We consider the problem of estimating a large rank-one tensor {\$}{\{}\backslashboldsymbol u{\}}{\^{}}{\{}\backslashotimes k{\}}\backslashin({\{}\backslashmathbb R{\}}{\^{}}{\{}n{\}}){\^{}}{\{}\backslashotimes k{\}}{\$}, {\$}k\backslashge 3{\$} in Gaussian noise. Earlier work characterized a critical signal-to-noise ratio {\$}\backslashlambda{\_}{\{}Bayes{\}}= O(1){\$} above which an ideal estimator achieves strictly positive correlation with the unknown vector of interest. Remarkably no polynomial-time algorithm is known that achieved this goal unless {\$}\backslashlambda\backslashge C n{\^{}}{\{}(k-2)/4{\}}{\$} and even powerful semidefinite programming relaxations appear to fail for {\$}1\backslashll \backslashlambda\backslashll n{\^{}}{\{}(k-2)/4{\}}{\$}. In order to elucidate this behavior, we consider the maximum likelihood estimator, which requires maximizing a degree-{\$}k{\$} homogeneous polynomial over the unit sphere in {\$}n{\$} dimensions. We compute the expected number of critical points and local maxima of this objective function and show that it is exponential in the dimensions {\$}n{\$}, and give exact formulas for the exponential growth rate. We show that (for {\$}\backslashlambda{\$} larger than a constant) critical points are either very close to the unknown vector {\$}{\{}\backslashboldsymbol u{\}}{\$}, or are confined in a band of width {\$}\backslashTheta(\backslashlambda{\^{}}{\{}-1/(k-1){\}}){\$} around the maximum circle that is orthogonal to {\$}{\{}\backslashboldsymbol u{\}}{\$}. For local maxima, this band shrinks to be of size {\$}\backslashTheta(\backslashlambda{\^{}}{\{}-1/(k-2){\}}){\$}. These `uninformative' local maxima are likely to cause the failure of optimization algorithms.},
	Archiveprefix = {arXiv},
	Arxivid = {1711.05424},
	Author = {{Ben Arous}, Gerard and Mei, Song and Montanari, Andrea and Nica, Mihai},
	Eprint = {1711.05424},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1711.05424.pdf:pdf},
	Pages = {1--40},
	Title = {{The landscape of the spiked tensor model}},
	Url = {http://arxiv.org/abs/1711.05424},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1711.05424}}

@article{Richardson2018,
	Abstract = {A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution ("mode collapse") and that using the learned models for anything other than generating samples may be very difficult. In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images.},
	Archiveprefix = {arXiv},
	Arxivid = {1805.12462},
	Author = {Richardson, Eitan and Weiss, Yair},
	Eprint = {1805.12462},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/7826-on-gans-and-gmms.pdf:pdf},
	Number = {NeurIPS},
	Pages = {1--12},
	Title = {{On GANs and GMMs}},
	Url = {http://arxiv.org/abs/1805.12462},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1805.12462}}

@article{Baldassi2015b,
	Abstract = {We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance.We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here,we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings.We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1509.05753v1},
	Author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	Doi = {10.1103/PhysRevLett.115.128101},
	Eprint = {arXiv:1509.05753v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.115.128101.pdf:pdf},
	Issn = {10797114},
	Journal = {Physical Review Letters},
	Number = {12},
	Pages = {1--5},
	Pmid = {26431018},
	Title = {{Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses}},
	Volume = {115},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.115.128101}}

@article{Krauth1989,
	Author = {Krauth, Werner and Mezard, Marc},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/89{\_}KM{\_}JDP.pdf:pdf},
	Journal = {J. Phys (France)},
	Pages = {3057--3066},
	Title = {{Storage capacity of memory networks with binary coupling}},
	Volume = {50},
	Year = {1989}}

@article{Fletcher2018,
	Abstract = {Deep generative networks provide a powerful tool for modeling complex data in a wide range of applications. In inverse problems that use these networks as generative priors on data, one must often perform inference of the inputs of the networks from the outputs. Inference is also required for sampling during stochastic training on these generative models. This paper considers inference in a deep stochastic neural network where the parameters (e.g., weights, biases and activation functions) are known and the problem is to estimate the values of the input and hidden units from the output. While several approximate algorithms have been proposed for this task, there are few analytic tools that can provide rigorous guarantees in the reconstruction error. This work presents a novel and computationally tractable output-to-input inference method called Multi-Layer Vector Approximate Message Passing (ML-VAMP). The proposed algorithm, derived from expectation propagation, extends earlier AMP methods that are known to achieve the replica predictions for optimality in simple linear inverse problems. Our main contribution shows that the mean-squared error (MSE) of ML-VAMP can be exactly predicted in a certain large system limit (LSL) where the numbers of layers is fixed and weight matrices are random and orthogonally-invariant with dimensions that grow to infinity. ML-VAMP is thus a principled method for output-to-input inference in deep networks with a rigorous and precise performance achievability result in high dimensions.},
	Author = {Fletcher, Alyson K. and Rangan, Sundeep and Schniter, Philip},
	Isbn = {9781538647806},
	Issn = {21578095},
	Journal = {IEEE International Symposium on Information Theory - Proceedings},
	Pages = {1884--1888},
	Title = {{Inference in Deep Networks in High Dimensions}},
	Volume = {2018-June},
	Year = {2018}
	}
	
@article{fletcher2018inference,
	Author = {Fletcher, Alyson K. and Rangan, Sundeep and Schniter, Philip},
	Isbn = {9781538647806},
	Issn = {21578095},
	Journal = {IEEE International Symposium on Information Theory - Proceedings},
	Pages = {1884--1888},
	Title = {{Inference in Deep Networks in High Dimensions}},
	Volume = {2018-June},
	Year = {2018}
	}

@article{Wyart2003,
	Abstract = {We study Sutton's 'microcanonical' model for the internal organization of firms, that leads to non-trivial scaling properties for the statistics of growth rates. We show that the growth rates are asymptotically Gaussian in this model, whereas empirical results suggest that the kurtosis of the distribution increases with size. We also obtain the conditional distribution of the number and size of sub-sectors in Sutton's model. We formulate and solve an alternative model, based on the assumption that the sector sizes follow a power-law distribution. We find in this new model both anomalous scaling of the variance of growth rates and non-Gaussian asymptotic distributions. We give some testable predictions of the two models that would differentiate them further. We also discuss why the growth rate statistics at the country level and at the company level should be identical. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0210479},
	Author = {Wyart, Matthieu and Bouchaud, Jean Philippe},
	Doi = {10.1016/S0378-4371(03)00267-X},
	Eprint = {0210479},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0210479v2.pdf:pdf},
	Issn = {03784371},
	Journal = {Physica A: Statistical Mechanics and its Applications},
	Keywords = {Corporate growth,Pareto distribution,Scaling,Size distribution},
	Number = {1-2},
	Pages = {241--255},
	Primaryclass = {cond-mat},
	Title = {{Statistical models for company growth}},
	Volume = {326},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/S0378-4371(03)00267-X}}

@article{Mei2018c,
	Abstract = {Most high-dimensional estimation and prediction methods propose to minimize a cost function (empirical risk) that is written as a sum of losses associated to each data point. In this paper we focus on the case of non-convex losses, which is practically important but still poorly understood. Classical empirical process theory implies uniform convergence of the empirical risk to the population risk. While uniform convergence implies consistency of the resulting M-estimator, it does not ensure that the latter can be computed efficiently. In order to capture the complexity of computing M-estimators, we propose to study the landscape of the empirical risk, namely its stationary points and their properties. We establish uniform convergence of the gradient and Hessian of the empirical risk to their population counterparts, as soon as the number of samples becomes larger than the number of unknown parameters (modulo logarithmic factors). Consequently, good properties of the population risk can be carried to the empirical risk, and we can establish one-to-one correspondence of their stationary points. We demonstrate that in several problems such as non-convex binary classification, robust regression, and Gaussian mixture model, this result implies a complete characterization of the landscape of the empirical risk, and of the convergence properties of descent algorithms. We extend our analysis to the very high-dimensional setting in which the number of parameters exceeds the number of samples, and provide a characterization of the empirical risk landscape under a nearly information-theoretically minimal condition. Namely, if the number of samples exceeds the sparsity of the unknown parameters vector (modulo logarithmic factors), then a suitable uniform convergence result takes place. We apply this result to non-convex binary classification and robust regression in very high-dimension.},
	Archiveprefix = {arXiv},
	Arxivid = {1607.06534},
	Author = {Mei, Song and Bai, Yu and Montanari, Andrea},
	Doi = {10.1214/17-AOS1637},
	Eprint = {1607.06534},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1607.06534.pdf:pdf},
	Issn = {00905364},
	Journal = {Annals of Statistics},
	Keywords = {Empirical risk minimization,Landscape,Nonconvex optimization,Uniform convergence},
	Number = {6A},
	Pages = {2747--2774},
	Title = {{The landscape of empirical risk for nonconvex losses}},
	Volume = {46},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1214/17-AOS1637}}

@article{Rattray1998,
	Abstract = {Natural gradient descent is an on-line variable-metric optimization algorithm which utilizes an underlying Riemannian parameter space. We analyze the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks. For a realizable learning scenario we find significant improvements over standard gradient descent for both the transient and asymptotic stages of learning, with a slower power law increase in learning time as task complexity grows. [S0031-9007(98)07950-2].},
	Author = {Rattray, M and Saad, D and Amari, S},
	Doi = {10.1103/PhysRevLett.81.5461},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.81.5461.pdf:pdf},
	Isbn = {0031-9007},
	Issn = {0031-9007},
	Journal = {Physical Review Letters},
	Keywords = {multilayer neural networks,online},
	Number = {24},
	Pages = {5461--5464},
	Title = {{Natural gradient descent for on-line learning}},
	Volume = {81},
	Year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.81.5461}}

@article{Majer1993,
	Author = {Majer, P. and Engel, A. and Zippelius, A.},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/P{\_}Majer{\_}1993{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}26{\_}015.pdf:pdf},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {24},
	Pages = {7405--7416},
	Title = {{Perceptrons above saturation}},
	Volume = {26},
	Year = {1993}}

@article{Pennington2017,
	Abstract = {Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix Y T Y , Y = f (W X), where W is a random weight matrix, X is a random data matrix, and f is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature networks on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.},
	Author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf:pdf},
	Journal = {Nips},
	Number = {31},
	Title = {{Nonlinear random matrix theory for deep learning}},
	Year = {2017}}

@article{Mezard2011,
	Abstract = {We discuss the analytical solution through the cavity method of a mean field model that displays at the same time an ideal glass transition and a set of jamming points. We establish the equations describing this system, and we discuss some approximate analytical solutions and a numerical strategy to solve them exactly. We compare these methods and we get insight into the reliability of the theory for the description of finite dimensional hard spheres.},
	Archiveprefix = {arXiv},
	Arxivid = {1011.5080},
	Author = {M{\'{e}}zard, Marc and Parisi, Giorgio and Tarzia, Marco and Zamponi, Francesco},
	Doi = {10.1088/1742-5468/2011/03/P03002},
	Eprint = {1011.5080},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Mezard-Parisi-Tarzia-Zamponi-On the solution of a 'solvable' model of an ideal glass of hard spheres displaying a jamming transition .pdf:pdf},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {cavity and replica method,classical phase transitions (theory),phase diagrams (theory),structural glasses (theory)},
	Number = {3},
	Title = {{On the solution of a 'solvable' model of an ideal glass of hard spheres displaying a jamming transition}},
	Volume = {2011},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2011/03/P03002}}

@article{Pennington2017a,
	Abstract = {In the nematode, Caenorhabditis elegans, VA and VB motor neurons arise from a common precursor cell but adopt different morphologies and synapse with separate sets of interneurons in the ventral nerve cord. A mutation that inactivates the unc-4 homeodomain gene causes VA motor neurons to assume the VB pattern of synaptic input while retaining normal axonal polarity and output; the disconnection of VA motor neurons from their usual presynaptic partners blocks backward locomotion. We show that expression of a functional unc-4-beta-galactosidase chimeric protein in VA motor neurons restores wild-type movement to an unc-4 mutant. We propose that unc-4 controls a differentiated characteristic of the VA motor neurons that distinguishes them from their VB sisters, thus dictating recognition by the appropriate interneurons. Our results show that synaptic choice can be controlled at the level of transcription in the post-synaptic neuron and identify a homeoprotein that defines a subset of cell-specific traits required for this choice.},
	Author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Pennington - nonlinear-random-matrix-theory-for-deep-learning.pdf:pdf},
	Isbn = {0950-1991 (Print)},
	Issn = {0950-1991},
	Journal = {Neural Information Processing Systems},
	Number = {31},
	Pmid = {7555714},
	Title = {{Nonlinear random matrix theory for deep learning}},
	Year = {2017}}

@article{Zdeborova2007,
	Abstract = {We consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. Using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions). We show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. First, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. Afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. Another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. Eventually, above the coloring threshold, no more solutions are available. We compute all the critical connectivities for Erdos-Renyi and regular random graphs and determine their asymptotic values for large number of colors. Finally, we discuss the algorithmic consequences of our findings. We argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. We also discuss the performance of a simple local Walk-COL algorithm and of the belief propagation algorithm in the light of our results.},
	Archiveprefix = {arXiv},
	Arxivid = {0704.1269},
	Author = {Zdeborov{\'{a}}, Lenka and Krza̧ka{\l}a, Florent},
	Doi = {10.1103/PhysRevE.76.031131},
	Eprint = {0704.1269},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/46. Phase transitions in the coloring graph.pdf:pdf},
	Issn = {15393755},
	Journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	Number = {3},
	Pages = {1--37},
	Pmid = {17930223},
	Title = {{Phase transitions in the coloring of random graphs}},
	Volume = {76},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.76.031131}}

@article{Asim2019,
	Abstract = {This paper proposes a novel approach to regularize the ill-posed blind image deconvolution (blind image deblurring) problem using deep generative networks. We employ two separate deep generative models - one trained to produce sharp images while the other trained to generate blur kernels from lower dimensional parameters. To deblur, we propose an alternating gradient descent scheme operating in the latent lower-dimensional space of each of the pretrained generative models. Our experiments show excellent deblurring results even under large blurs and heavy noise. To improve the performance on rich image datasets not well learned by the generative networks, we present a modification of the proposed scheme that governs the deblurring process under both generative and classical priors.},
	Archiveprefix = {arXiv},
	Arxivid = {1908.07404},
	Author = {Asim, Muhammad and Shamshad, Fahad and Ahmed, Ali},
	Eprint = {1908.07404},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1908.07404.pdf:pdf},
	Title = {{Blind Image Deconvolution using Pretrained Generative Priors}},
	Url = {http://arxiv.org/abs/1908.07404},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1908.07404}}

@article{Urbani2013,
	Author = {Urbani, Pierfrancesco},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Urbani.pdf:pdf},
	Journal = {Thesis},
	Number = {1088567},
	Title = {{Theory of fluctuations in disordered systems}},
	Year = {2013}}

@article{Luo2018,
	Abstract = {We present the optimal design of a spectral method widely used to initialize nonconvex optimization algorithms for solving phase retrieval and other signal recovery problems. Our work leverages recent results that provide an exact characterization of the performance of the spectral method in the high-dimensional limit. This characterization allows us to map the task of optimal design to a constrained optimization problem in a weighted {\$}L{\^{}}2{\$} function space. The latter has a closed-form solution. Interestingly, under a mild technical condition, our results show that there exists a fixed design that is uniformly optimal over all sampling ratios. Numerical simulations demonstrate the performance improvement brought by the proposed optimal design over existing constructions in the literature. In a recent work, Mondelli and Montanari have shown the existence of a weak reconstruction threshold below which the spectral method cannot provide useful estimates. Our results serve to complement that work by deriving the fundamental limit of the spectral method beyond the aforementioned threshold.},
	Archiveprefix = {arXiv},
	Arxivid = {1811.04420},
	Author = {Luo, Wangyu and Alghamdi, Wael and Lu, Yue M.},
	Eprint = {1811.04420},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1811.04420.pdf:pdf},
	Journal = {arXiv:1811.04420},
	Number = {2},
	Pages = {1--9},
	Title = {{Optimal Spectral Initialization for Signal Recovery with Applications to Phase Retrieval}},
	Url = {http://arxiv.org/abs/1811.04420},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1811.04420}}

@article{UlyanovD.VedaldiA.2018a,
	Abstract = {In this supplemental material we provide more details about the setup in each experiment. We also extend the figures from the main paper with more qualitative comparisons. The code for all experiments is published at https://github.com/DmitryUlyanov/ deep-image-prior. Interactive comparisons are available at the project page https://dmitryulyanov.github.io/deep{\_}image{\_}prior.},
	Author = {{Ulyanov D., Vedaldi A.}, Lempitsky V.},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/deep{\_}image{\_}prior{\_}journal.pdf:pdf},
	Journal = {Cvpr},
	Pages = {14},
	Title = {{Deep Image Prior Supplementary}},
	Url = {https://box.skoltech.ru/index.php/s/ib52BOoV58ztuPM{\#}pdfviewer},
	Year = {2018},
	Bdsk-Url-1 = {https://box.skoltech.ru/index.php/s/ib52BOoV58ztuPM%7B%5C#%7Dpdfviewer}}

@article{Huang2013,
	Abstract = {The statistical picture of the solution space for a binary perceptron is studied. The binary perceptron learns a random classification of input random patterns by a set of binary synaptic weights. The learning of this network is difficult especially when the pattern (constraint) density is close to the capacity, which is supposed to be intimately related to the structure of the solution space. The geometrical organization is elucidated by the entropy landscape from a reference configuration and of solution-pairs separated by a given Hamming distance in the solution space. We evaluate the entropy at the annealed level as well as replica symmetric level and the mean field result is confirmed by the numerical simulations on single instances using the proposed message passing algorithms. From the first landscape (a random configuration as a reference), we see clearly how the solution space shrinks as more constraints are added. From the second landscape of solution-pairs, we deduce the coexistence of clustering and freezing in the solution space.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1304.2850v2},
	Author = {Huang, Haiping and Wong, K. Y.Michael and Kabashima, Yoshiyuki},
	Doi = {10.1088/1751-8113/46/37/375002},
	Eprint = {arXiv:1304.2850v2},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1304.2850.pdf:pdf},
	Issn = {17518113},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {37},
	Title = {{Entropy landscape of solutions in the binary perceptron problem}},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8113/46/37/375002}}

@article{Mixon2018,
	Abstract = {It has been experimentally established that deep neural networks can be used to produce good generative models for real world data. It has also been established that such generative models can be exploited to solve classical inverse problems like compressed sensing and super resolution. In this work we focus on the classical signal processing problem of image denoising. We propose a theoretical setting that uses spherical harmonics to identify what mathematical properties of the activation functions will allow signal denoising with local methods.},
	Archiveprefix = {arXiv},
	Arxivid = {1803.09319},
	Author = {Mixon, Dustin G. and Villar, Soledad},
	Eprint = {1803.09319},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1803.09319.pdf:pdf},
	Pages = {1--17},
	Title = {{SUNLayer: Stable denoising with generative networks}},
	Url = {http://arxiv.org/abs/1803.09319},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1803.09319}}

@article{Erichsen1992,
	Author = {R Erichsen and W K Thuemann},
	Journal = {Journal of Physics A: Mathematical and General},
	Month = {jan},
	Number = {2},
	Pages = {L61--L68},
	Publisher = {{IOP} Publishing},
	Title = {Optimal storage of a neural network model: a replica symmetry-breaking solution},
	Volume = {26},
	Year = 1993}

@inproceedings{Chaudhari2016,
	Author = {Chaudhari, P and Choromanska, Anna and Soatto, S and LeCun, Yann and Baldassi, C and Borgs, C and Chayes, J and Sagun, Levent and Zecchina, R},
	Booktitle = {International Conference on Learning Representations (ICLR)},
	Title = {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
	Year = {2017}}

@article{Feizi2017,
	Abstract = {Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes their performance analysis challenging. In this paper, we take a novel approach to this problem by asking whether one can constrain neural network weights to make its optimization landscape have good theoretical properties while at the same time, be a good approximation for the unconstrained one. For two-layer neural networks, we provide affirmative answers to these questions by introducing Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.},
	Archiveprefix = {arXiv},
	Arxivid = {1710.02196},
	Author = {Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
	Eprint = {1710.02196},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1710.02196.pdf:pdf},
	Number = {Figure 1},
	Title = {{Porcupine Neural Networks: (Almost) All Local Optima are Global}},
	Url = {http://arxiv.org/abs/1710.02196},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1710.02196}}

@article{Interno2015,
	Author = {Interno, Relatore and {Caracciolo Correlatore}, Sergio and {Zecchina Correlatore}, Riccardo and {Carlo Lucibello}, Dott},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Amelio.pdf:pdf},
	Title = {{Corso Di Laurea Magistrale in Fisica Out-of-Equilibrium Analysis of Simple Neural Networks}},
	Year = {2015}}

@article{Allen-Zhu2018,
	Abstract = {The fundamental learning theory behind neural networks remains largely open. What classes of functions can neural networks actually learn? Why doesn't the trained neural networks overfit when the it is overparameterized (namely, having more parameters than statistically needed to overfit training data)? In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer parameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using polynomially many samples. The sample complexity can also be almost independent of the number of parameters in the overparameterized network.},
	Archiveprefix = {arXiv},
	Arxivid = {1811.04918},
	Author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
	Eprint = {1811.04918},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1811.04918.pdf:pdf},
	Pages = {1--84},
	Title = {{Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers}},
	Url = {http://arxiv.org/abs/1811.04918},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1811.04918}}

@article{Gabrie2017,
	Abstract = {We study in this paper the structure of solutions in the random hypergraph coloring problem and the phase transitions they undergo when the density of constraints is varied. Hypergraph coloring is a constraint satisfaction problem where each constraint includes {\$}K{\$} variables that must be assigned one out of {\$}q{\$} colors in such a way that there are no monochromatic constraints, i.e. there are at least two distinct colors in the set of variables belonging to every constraint. This problem generalizes naturally coloring of random graphs ({\$}K=2{\$}) and bicoloring of random hypergraphs ({\$}q=2{\$}), both of which were extensively studied in past works. The study of random hypergraph coloring gives us access to a case where both the size {\$}q{\$} of the domain of the variables and the arity {\$}K{\$} of the constraints can be varied at will. Our work provides explicit values and predictions for a number of phase transitions that were discovered in other constraint satisfaction problems but never evaluated before in hypergraph coloring. Among other cases we revisit the hypergraph bicoloring problem ({\$}q=2{\$}) where we find that for {\$}K=3{\$} and {\$}K=4{\$} the colorability threshold is not given by the one-step-replica-symmetry-breaking analysis as the latter is unstable towards more levels of replica symmetry breaking. We also unveil and discuss the coexistence of two different 1RSB solutions in the case of {\$}q=2{\$}, {\$}K \backslashge 4{\$}. Finally we present asymptotic expansions for the density of constraints at which various phase transitions occur, in the limit where {\$}q{\$} and/or {\$}K{\$} diverge.},
	Archiveprefix = {arXiv},
	Author = {Gabri{\'{e}}, Marylou and Dani, Varsha and Semerjian, Guilhem and Zdeborov{\'{a}}, Lenka},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Keywords = {cavity method,constraint satisfaction problem,disordered systems,message passing algorithms,phase transitions,random graphs},
	Number = {50},
	Title = {{Phase transitions in the q-coloring of random hypergraphs}},
	Volume = {50},
	Year = {2017}
	}

@article{WilliamWhyte1995,
	Author = {{William Whyte}},
	Title = {{Statistical Mechanics of Neural Networks}}}

@article{Martin2005a,
	Abstract = {The multi-index matching problem generalizes the well known matching problem by going from pairs to d -uplets. We use the cavity method from statistical physics to analyse its properties when the costs of the d -uplets are random. At low temperatures we find for d ≥ 3 a frozen glassy phase with vanishing entropy. We also investigate some properties of small samples by enumerating the lowest cost matchings to compare with our theoretical predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0507180v1},
	Author = {Martin, O. C. and M{\'{e}}zard, M. and Rivoire, O.},
	Doi = {10.1088/1742-5468/2005/09/P09006},
	Eprint = {0507180v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0507180.pdf:pdf},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {Cavity and replica method,Typical-case computational complexity},
	Number = {9},
	Pages = {159--193},
	Primaryclass = {cond-mat},
	Title = {{Random multi-index matching problems}},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2005/09/P09006}}

@article{Ros2018,
	Abstract = {We study rough high-dimensional landscapes in which an increasingly stronger preference for a given configuration emerges. Such energy landscapes arise in glass physics and inference. In particular we focus on random Gaussian functions, and on the spiked-tensor model and generalizations. We thoroughly analyze the statistical properties of the corresponding landscapes and characterize the associated geometrical phase transitions. In order to perform our study, we develop a framework based on the Kac-Rice method that allows to compute the complexity of the landscape, i.e. the logarithm of the typical number of stationary points and their Hessian. This approach generalizes the one used to compute rigorously the annealed complexity of mean-field glass models. We discuss its advantages with respect to previous frameworks, in particular the thermodynamical replica method which is shown to lead to partially incorrect predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {1804.02686},
	Author = {Ros, Valentina and {Ben Arous}, Gerard and Biroli, Giulio and Cammarota, Chiara},
	Eprint = {1804.02686},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1804.02686.pdf:pdf},
	Title = {{Complex energy landscapes in spiked-tensor and simple glassy models: ruggedness, arrangements of local minima and phase transitions}},
	Url = {http://arxiv.org/abs/1804.02686},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1804.02686}}

@article{Baldassi2016,
	Abstract = {In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare - but extremely dense and accessible - regions of configurations in the network weight space. We define a novel measure, which we call the "robust ensemble" (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.06444},
	Author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	Doi = {10.1073/pnas.1608103113},
	Eprint = {1605.06444},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/E7655.full.pdf:pdf},
	Issn = {0027-8424},
	Pmid = {27856745},
	Title = {{Unreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes}},
	Url = {http://arxiv.org/abs/1605.06444{\%}0Ahttp://dx.doi.org/10.1073/pnas.1608103113},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1073/pnas.1608103113}}

@article{Barkai1990,
	Abstract = {Statistical mechanics is applied to estimate the maximal information$\backslash$ncapacity per synapse (?c) of a multilayered feedforward neural network,$\backslash$nfunctioning as a parity machine. For a large number of hidden units,$\backslash$nK, the replica-symmetric solution overestimates dramatically the$\backslash$ncapacity, ?c?K2. However, a one-step replica-symmetry breaking gives$\backslash$n?c?lnK/ln2, which coincides with a theoretical upper bound. It is$\backslash$nsuggested that this asymptotic behavior is exact. Results for finite$\backslash$nK are also discussed.},
	Author = {Barkai, E. and Hansel, D. and Kanter, I.},
	Doi = {10.1103/PhysRevLett.65.2312},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/40. Statistical mechanics of feedforward neural networks.pdf:pdf},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {18},
	Pages = {2312--2315},
	Pmid = {10042513},
	Title = {{Statistical mechanics of a multilayered neural network}},
	Volume = {65},
	Year = {1990},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.65.2312}}

@article{Gualdi2017,
	Abstract = {We generalise the stylised macroeconomic Agent-Based model introduced in our previous paper ("Tipping points in macroeconomic agent-based models", JEDC 50 29--61 2015), with the aim of investigating the role and efficacy of monetary policy of a "Central Bank", that sets the interest rate such as to steer the economy towards a prescribed inflation and unemployment level. Our major finding is that provided its policy is not too aggressive (in a sense detailed in the paper) the Central Bank is successful in achieving its goals. However, the existence of different equilibrium states of the economy, separated by phase boundaries (or "dark corners"), can cause the monetary policy itself to trigger instabilities and be counter-productive. In other words, the Central Bank must navigate in a narrow window: too little is not enough, too much leads to instabilities and wildly oscillating economies. This conclusion strongly contrasts with the prediction of DSGE models. },
	Archiveprefix = {arXiv},
	Arxivid = {1501.00434},
	Author = {Gualdi, Stanislao and Tarzia, Marco and Zamponi, Francesco and Bouchaud, Jean Philippe},
	Doi = {10.1007/s11403-016-0174-z},
	Eprint = {1501.00434},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1501.00434v2.pdf:pdf},
	Issn = {18607128},
	Journal = {Journal of Economic Interaction and Coordination},
	Number = {3},
	Pages = {507--537},
	Title = {{Monetary policy and dark corners in a stylized agent-based model}},
	Volume = {12},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11403-016-0174-z}}

@article{Crisanti2005,
	Abstract = {The Complexity of the Thouless-Anderson-Palmer (TAP) solutions of the Ising p-spin is investigated in the temperature regime where the equilibrium phase is one step Replica Symmetry Breaking. Two solutions of the resulting saddle point equations are found. One is supersymmetric (SUSY) and includes the equilibrium value of the free energy while the other is non-SUSY. The two solutions cross exactly at a value of the free energy where the replicon eigenvalue is zero; at low free energy the complexity is described by the SUSY solution while at high free energy it is described by the non-SUSY solution. In particular the non-SUSY solution describes the total number of solutions, like in the Sherrington-Kirkpatrick (SK) model. The relevant TAP solutions corresponding to the non-SUSY solution share the same feature of the corresponding solutions in the SK model, in particular their Hessian has a vanishing isolated eigenvalue. The TAP solutions corresponding to the SUSY solution, instead, are well separated minima.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0406649},
	Author = {Crisanti, A. and Leuzzi, L. and Rizzo, T.},
	Doi = {10.1103/PhysRevB.71.094202},
	Eprint = {0406649},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevB.71.094202.pdf:pdf},
	Issn = {10980121},
	Journal = {Physical Review B - Condensed Matter and Materials Physics},
	Number = {9},
	Pages = {1--11},
	Primaryclass = {cond-mat},
	Title = {{Complexity in mean-field spin-glass models: Ising p-spin}},
	Volume = {71},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevB.71.094202}}

@article{Brunel1992,
	Abstract = {The authors study the information storage capacity of a simple perceptron in the error regime. For random unbiased patterns the geometrical analysis gives a logarithmic dependence for the information content in the asymptotic limit. In this case, the statistical physics approach, when used at the simplest level of replica theory, does not give satisfactory results. However for perceptrons with finite stability, the information content can be simply calculated with statistical physics methods in a region above the critical storage level, for biased as well as for unbiased patterns.},
	Author = {Brunel, N. and Nadal, J. P. and Toulouse, G.},
	Doi = {10.1088/0305-4470/25/19/015},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/brunel92.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {19},
	Pages = {5017--5038},
	Title = {{Information capacity of a perceptron}},
	Volume = {25},
	Year = {1992},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/25/19/015}}

@article{Bayati,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1001.3448v4},
	Author = {Bayati, Mohsen},
	Eprint = {arXiv:1001.3448v4},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1001.3448.pdf:pdf},
	Title = {{The dynamics of message passing on dense graphs , with applications to compressed sensing}}}

@article{Gauvin2010,
	Author = {Gauvin, Laetitia},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/These.pdf:pdf},
	Number = {Paris VI},
	Title = {{Mod{\'{e}}lisation de syst{\`{e}}mes socio-{\'{e}}conomiques {\`{a}} l ' aide des outils de physique statistique}},
	Year = {2010}}

@article{Watkin1993a,
	Abstract = {A summary is presented of the statistical mechanical theory of learning a rule with a neural network, a rapidly advancing area which is closely related to other inverse problems frequently encountered by physicists. By emphasizing the relationship between neural networks and strongly interacting physical systems, such as spin glasses, the authors show how learning theory has provided a workshop in which to develop new,exact analytical techniques.},
	Author = {Watkin, Timothy L H and Rau, Albrecht and Biehl, Michael},
	Doi = {10.1103/RevModPhys.65.499},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/RevModPhys.65.499.pdf:pdf},
	Issn = {0034-6861},
	Journal = {Rev. Mod. Phys.},
	Pages = {499--556},
	Title = {{The statistical mechanics of learning a rule}},
	Url = {http://link.aps.org/doi/10.1103/RevModPhys.65.499},
	Volume = {65},
	Year = {1993},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/RevModPhys.65.499},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/RevModPhys.65.499}}

@article{Barbier2016,
	Author = {Barbier, Jean and Dia, Mohamad and Macris, Nicolas},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/6380-mutual-information-for-symmetric-rank-one-matrix-estimation-a-proof-of-the-replica-formula.pdf:pdf},
	Number = {Mi},
	Pages = {1--9},
	Title = {{Mutual information for symmetric rank-one matrix estimation : A proof of the replica formula}},
	Year = {2016}}

@article{Mezard2017,
	Abstract = {Motivated by recent progress in using restricted Boltzmann machines as preprocessing algorithms for deep neural network, we revisit the mean-field equations (belief-propagation and TAP equations) in the best understood such machine, namely the Hopfield model of neural networks, and we explicit how they can be used as iterative message-passing algorithms, providing a fast method to compute the local polarizations of neurons. In the "retrieval phase" where neurons polarize in the direction of one memorized pattern, we point out a major difference between the belief propagation and TAP equations : the set of belief propagation equations depends on the pattern which is retrieved, while one can use a unique set of TAP equations. This makes the latter method much better suited for applications in the learning process of restricted Boltzmann machines. In the case where the patterns memorized in the Hopfield model are not independent, but are correlated through a combinatorial structure, we show that the TAP equations have to be modified. This modification can be seen either as an alteration of the reaction term in TAP equations, or, more interestingly, as the consequence of message passing on a graphical model with several hidden layers, where the number of hidden layers depends on the depth of the correlations in the memorized patterns. This layered structure is actually necessary when one deals with more general restricted Boltzmann machines.},
	Archiveprefix = {arXiv},
	Arxivid = {1608.01558},
	Author = {M{\'{e}}zard, Marc},
	Doi = {10.1103/PhysRevE.95.022117},
	Eprint = {1608.01558},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.95.022117.pdf:pdf},
	Issn = {24700053},
	Journal = {Physical Review E},
	Number = {2},
	Pages = {1--15},
	Title = {{Mean-field message-passing equations in the Hopfield model and its generalizations}},
	Volume = {95},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.95.022117}}

@article{Martin2005,
	Abstract = {The multi-index matching problem generalizes the well known matching problem by going from pairs to d -uplets. We use the cavity method from statistical physics to analyse its properties when the costs of the d -uplets are random. At low temperatures we find for d ≥ 3 a frozen glassy phase with vanishing entropy. We also investigate some properties of small samples by enumerating the lowest cost matchings to compare with our theoretical predictions.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/0507180v1},
	Author = {Martin, O. C. and M{\'{e}}zard, M. and Rivoire, O.},
	Doi = {10.1088/1742-5468/2005/09/P09006},
	Eprint = {0507180v1},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0507180-2.pdf:pdf},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {Cavity and replica method,Typical-case computational complexity},
	Number = {9},
	Pages = {159--193},
	Primaryclass = {cond-mat},
	Title = {{Random multi-index matching problems}},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2005/09/P09006}}

@article{Parisi1995,
	Abstract = {We study the metastable states in Ising spin models with orthogonal interaction matrices. We focus on three realizations of this model, the random case and two non-random cases, i.e.$\backslash$ the fully-frustrated model on an infinite dimensional hypercube and the so-called sine-model. We use the mean-field (or {\{}$\backslash$sc tap{\}}) equations which we derive by resuming the high-temperature expansion of the Gibbs free energy. In some special non-random cases, we can find the absolute minimum of the free energy. For the random case we compute the average number of solutions to the {\{}$\backslash$sc tap{\}} equations. We find that the configurational entropy (or complexity) is extensive in the range {\$}T{\_}{\{}\backslashmbox{\{}\backslashtiny RSB{\}}{\}}{\textless}T{\textless}T{\_}{\{}\backslashmbox{\{}\backslashtiny M{\}}{\}}{\$}. Finally we present an apparently unrelated replica calculation which reproduces the analytical expression for the total number of {\{}$\backslash$sc tap{\}} solutions.},
	Archiveprefix = {arXiv},
	Arxivid = {cond-mat/9503009},
	Author = {Parisi, G. and Potters, M.},
	Doi = {10.1088/0305-4470/28/18/016},
	Eprint = {9503009},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/9503009.pdf:pdf},
	Issn = {03054470},
	Journal = {Journal of Physics A: General Physics},
	Number = {18},
	Pages = {5267--5285},
	Primaryclass = {cond-mat},
	Title = {{Mean-field equations for spin models with orthogonal interaction matrices}},
	Url = {http://arxiv.org/abs/1910.},
	Volume = {28},
	Year = {1995},
	Bdsk-Url-1 = {http://arxiv.org/abs/1910.},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0305-4470/28/18/016}}

@article{Krzakala2012,
	Abstract = {Compressed sensing is a signal processing method that acquires data directly in a compressed form. This allows one to make less measurements than what was considered necessary to record a signal, enabling faster or more precise measurement protocols in a wide range of applications. Using an interdisciplinary approach, we have recently proposed in [arXiv:1109.4424] a strategy that allows compressed sensing to be performed at acquisition rates approaching to the theoretical optimal limits. In this paper, we give a more thorough presentation of our approach, and introduce many new results. We present the probabilistic approach to reconstruction and discuss its optimality and robustness. We detail the derivation of the message passing algorithm for reconstruction and expectation max- imization learning of signal-model parameters. We further develop the asymptotic analysis of the corresponding phase diagrams with and without measurement noise, for different distribution of signals, and discuss the best possible reconstruction performances regardless of the algorithm. We also present new efficient seeding matrices, test them on synthetic data and analyze their performance asymptotically.},
	Archiveprefix = {arXiv},
	Arxivid = {1206.3953},
	Author = {Krzakala, Florent and M{\'{e}}zard, Marc and Sausset, Francois and Sun, Yifan and Zdeborov{\'{a}}, Lenka},
	Doi = {10.1088/1742-5468/2012/08/P08009},
	Eprint = {1206.3953},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/31. Probabilistic Reconstruction in Compressed Sensing Algorithms Phase Diagrams and Threshold Achieving Matrices.pdf:pdf},
	Isbn = {1742-5468},
	Issn = {17425468},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Keywords = {cavity and replica method,error correcting codes,message-passing algorithms,statistical inference},
	Number = {8},
	Pages = {1--42},
	Title = {{Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices}},
	Volume = {2012},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2012/08/P08009}}

@book{vapnik2013nature,
	Author = {Vapnik, Vladimir},
	Publisher = {Springer science \& business media},
	Title = {The nature of statistical learning theory},
	Year = {2013}}

@book{shalev2014understanding,
	Author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	Publisher = {Cambridge university press},
	Title = {Understanding machine learning: From theory to algorithms},
	Year = {2014}}

@article{bartlett2002rademacher,
	Author = {Bartlett, Peter L and Mendelson, Shahar},
	Journal = {Journal of Machine Learning Research},
	Number = {Nov},
	Pages = {463--482},
	Title = {Rademacher and {G}aussian complexities: Risk bounds and structural results},
	Volume = {3},
	Year = {2002}}

@article{zhang2016understanding,
	Author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	Journal = {ICLR 2017, preprint arXiv:1611.03530},
	Title = {Understanding deep learning requires rethinking generalization},
	Year = {2016}}

@article{gardner1988optimal,
	Author = {Gardner, Elizabeth and Derrida, Bernard},
	Journal = {Journal of Physics A: Mathematical and general},
	Number = {1},
	Pages = {271},
	Publisher = {IOP Publishing},
	Title = {Optimal storage properties of neural network models},
	Volume = {21},
	Year = {1988}}

@book{ledoux2013probability,
	Author = {Ledoux, Michel and Talagrand, Michel},
	Publisher = {Springer Science \& Business Media},
	Title = {Probability in Banach Spaces: isoperimetry and processes},
	Year = {2013}}

@inproceedings{massart2000some,
	Author = {Massart, Pascal},
	Booktitle = {Annales de la Facult{\'e} des sciences de Toulouse: Math{\'e}matiques},
	Pages = {245--303},
	Title = {Some applications of concentration inequalities to statistics},
	Volume = {9},
	Year = {2000}}

@article{dudley1967sizes,
	Author = {Dudley, Richard M},
	Journal = {Journal of Functional Analysis},
	Number = {3},
	Pages = {290--330},
	Publisher = {Elsevier},
	Title = {The sizes of compact subsets of {H}ilbert space and continuity of {G}aussian processes},
	Volume = {1},
	Year = {1967}}

@book{hebb1962organization,
	Author = {Hebb, Donald Olding},
	Publisher = {Science Editions},
	Title = {The organization of behavior: a neuropsychological theory},
	Year = {1962}}

@book{bolthausen2007spin,
	Author = {Bolthausen, Erwin and Bovier, Anton},
	Publisher = {Springer},
	Title = {Spin glasses},
	Year = {2007}}

@article{schwarze1993learning,
	Author = {Schwarze, Henry},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {21},
	Pages = {5781},
	Publisher = {IOP Publishing},
	Title = {Learning a rule in a multilayer neural network},
	Volume = {26},
	Year = {1993}}

@article{panchenko2004bounds,
	Author = {Panchenko, Dmitry and Talagrand, Michel},
	Journal = {Probability Theory and Related Fields},
	Number = {3},
	Pages = {319--336},
	Publisher = {Springer},
	Title = {Bounds for diluted mean-fields spin glass models},
	Volume = {130},
	Year = {2004}}

@article{panchenko2018free,
	Author = {Panchenko, Dmitry and others},
	Journal = {The Annals of Probability},
	Number = {2},
	Pages = {829--864},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Free energy in the Potts spin glass},
	Volume = {46},
	Year = {2018}}

@article{urbanczik1997storage,
	Author = {Urbanczik, R},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {11},
	Pages = {L387},
	Publisher = {IOP Publishing},
	Title = {Storage capacity of the fully-connected committee machine},
	Volume = {30},
	Year = {1997}}

@inproceedings{xiong1998storage,
	Author = {Xiong, Yuansheng and Kwon, Chulan and Oh, Jong-Hoon},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {378--384},
	Title = {The storage capacity of a fully-connected committee machine},
	Year = {1998}}

@article{bartlett2003vapnik,
	Author = {Bartlett, Peter L and Maass, Wolfgang},
	Journal = {The handbook of brain theory and neural networks},
	Pages = {1188--1192},
	Publisher = {Citeseer},
	Title = {Vapnik-Chervonenkis dimension of neural nets},
	Year = {2003}}

@article{Bousquet2003,
	Author = {Bousquet, Olivier and Boucheron, Stephane and Lugosi, Gabor and Luxburg, U. and Ratsch, Gunnar},
	Doi = {10.1007/978-3-540-28650-9_8},
	Journal = {Advanced Lectures on Machine Learning, 169-207 (2004)},
	Month = {01},
	Title = {Introduction to Statistical Learning Theory},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-540-28650-9_8}}

@article{alaoui2016decoding,
	Author = {El Alaoui, Ahmed and Ramdas, Aaditya and Krzakala, Florent and Zdeborova, Lenka and Jordan, Michael},
	Date-Added = {2019-06-12 09:20:38 +0000},
	Date-Modified = {2019-06-12 09:20:55 +0000},
	Doi = {10.1137/18M1183339},
	Journal = {SIAM Journal on Mathematics of Data Science},
	Month = {11},
	Title = {Decoding from Pooled Data: Sharp Information-Theoretic Bounds},
	Volume = {1},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/18M1183339}}

@inproceedings{safran2017spurious,
	Abstract = {We consider the optimization problem associated with training simple ReLU neural networks of the form $\mathbf{x}\mapsto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once $6\le k\le 20$. By a concentration of measure argument, this implies that in high input dimensions, \emph{nearly all} target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.},
	Address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	Author = {Safran, Itay and Shamir, Ohad},
	Booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	Date-Added = {2019-06-12 08:55:35 +0000},
	Date-Modified = {2019-06-12 08:55:42 +0000},
	Month = {10--15 Jul},
	Pages = {4433--4441},
	Pdf = {http://proceedings.mlr.press/v80/safran18a/safran18a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Spurious Local Minima are Common in Two-Layer {R}e{LU} Neural Networks},
	Url = {http://proceedings.mlr.press/v80/safran18a.html},
	Volume = {80},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v80/safran18a.html}}

@article{bandeira2018notes,
	Abstract = {In these notes we describe heuristics to predict computational-to-statistical gaps in certain statistical problems. These are regimes in which the underlying statistical problem is information-theoretically possible although no efficient algorithm exists, rendering the problem essentially unsolvable for large instances. The methods we describe here are based on mature, albeit non-rigorous, tools from statistical physics.},
	Author = {Afonso Bandeira and Amelia Perry and Wein, {Alexander S.}},
	Date-Added = {2019-06-12 08:54:16 +0000},
	Date-Modified = {2019-06-12 08:54:36 +0000},
	Day = {1},
	Doi = {10.4171/PM/2014},
	Issn = {0032-5155},
	Journal = {Portugaliae Mathematica},
	Keywords = {Approximate message passing, Cavity method, Computational-to-statistical gaps, Phase transitions, Replica method},
	Language = {English (US)},
	Month = {1},
	Number = {2},
	Pages = {159--186},
	Publisher = {European Mathematical Society Publishing House},
	Title = {Notes on computational-to-statistical gaps: Predictions using statistical physics},
	Volume = {75},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.4171/PM/2014}}

@inproceedings{baity2018comparing,
	Abstract = {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are (1) the complexity of the loss landscape and of the dynamics within it, and (2) to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and datasets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large limes, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, showing that the statistical properties of the corresponding loss and energy landscapes arc different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.},
	Author = {Marco Baity-Jest and Levent Sagun and Geiger Mario and Stefano Spiglery and {Ben Arous}, Gerard and Chiara Cammarota and Yann LeCun and Matthieu Vvyart and Biroli, {Giu Jio}},
	Booktitle = {35th International Conference on Machine Learning, ICML 2018},
	Date-Added = {2019-06-12 08:51:12 +0000},
	Date-Modified = {2020-09-15 19:53:51 +0000},
	Day = {1},
	Language = {English (US)},
	Month = {1},
	Pages = {526--535},
	Publisher = {International Machine Learning Society (IMLS)},
	Title = {Comparing Dynamics: Deep Neural Networks versus Glassy Systems},
	Volume = {1},
	Year = {2018}}

@article{barbier2017phase,
	Abstract = {High-dimensional generalized linear models are basic building blocks of current data analysis tools including multilayers neural networks. They arise in signal processing, statistical inference, machine learning, communication theory, and other fields. We establish rigorously the intrinsic information-theoretic limitations of inference and learning for a class of randomly generated instances of generalized linear models, thus closing several decades-old conjectures. Moreover, we delimit regions of parameters for which the optimal error rates are efficiently achievable with currently known algorithms. Our proof technique is able to deal with the output nonlinearity and is hence of independent interest, opening ways to establish similar results for models of neural networks where nonlinearities are essential but in general difficult to account for.Generalized linear models (GLMs) are used in high-dimensional machine learning, statistics, communications, and signal processing. In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes, or benchmark models in neural networks. We evaluate the mutual information (or {\textquotedblleft}free entropy{\textquotedblright}) from which we deduce the Bayes-optimal estimation and generalization errors. Our analysis applies to the high-dimensional limit where both the number of samples and the dimension are large and their ratio is fixed. Nonrigorous predictions for the optimal errors existed for special cases of GLMs, e.g., for the perceptron, in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades-old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance and locate the associated sharp phase transitions separating learnable and nonlearnable regions. We believe that this random version of GLMs can serve as a challenging benchmark for multipurpose algorithms.},
	Author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
	Date-Added = {2019-06-12 08:48:10 +0000},
	Date-Modified = {2019-06-12 08:48:42 +0000},
	Doi = {10.1073/pnas.1802705116},
	Issn = {0027-8424},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {12},
	Pages = {5451--5460},
	Publisher = {National Academy of Sciences},
	Title = {Optimal errors and phase transitions in high-dimensional generalized linear models},
	Url = {https://www.pnas.org/content/116/12/5451},
	Volume = {116},
	Year = {2019}
	}

@article{barbier2019overlap,
	Author = {Barbier, Jean},
	Journal = {arXiv preprint arXiv:1904.02808},
	Title = {Overlap matrix concentration in optimal Bayesian inference},
	Year = {2019}}

@article{barbier2019mutual,
	Author = {Barbier, Jean and Luneau, Cl{\'e}ment and Macris, Nicolas},
	Journal = {arXiv preprint arXiv:1904.04565},
	Title = {Mutual Information for Low-Rank Even-Order Symmetric Tensor Factorization},
	Year = {2019}}

@article{barbier_adaptInterp_review,
	Abstract = {In this contribution we give a pedagogic introduction to the newly introduced adaptive interpolation method to prove in a simple and unified way replica formulas for Bayesian optimal inference problems. Many aspects of this method can already be explained at the level of the simple Curie-Weiss spin system. This provides a new method of solution for this model which does not appear to be known. We then generalize this analysis to a paradigmatic inference problem, namely rank-one matrix estimation, also referred to as the Wigner spike model in statistics. We give many pointers to the recent literature where the method has been successfully applied.},
	Author = {Barbier, Jean and Macris, Nicolas},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Title = {The adaptive interpolation method for proving replica formulas. Applications to the Curie-Weiss and Wigner spike models},
	Url = {http://iopscience.iop.org/10.1088/1751-8121/ab2735},
	Year = {2019},
	Bdsk-Url-1 = {http://iopscience.iop.org/10.1088/1751-8121/ab2735}}

@inproceedings{reeves2018mutual,
	Author = {Reeves, Galen and Pfister, Henry D and Dytso, Alex},
	Booktitle = {2018 IEEE International Symposium on Information Theory (ISIT)},
	Organization = {IEEE},
	Pages = {1754--1758},
	Title = {Mutual information as a function of matrix SNR for linear Gaussian channels},
	Year = {2018}}

@inproceedings{payaro2011yet,
	Author = {Payar{\'o}, Miquel and Gregori, Maria and Palomar, Daniel},
	Booktitle = {Wireless Communications and Signal Processing (WCSP), 2011 International Conference on},
	Organization = {IEEE},
	Pages = {1--5},
	Title = {Yet another entropy power inequality with an application},
	Year = {2011}}

@inproceedings{lamarca2009linear,
	Author = {Lamarca, Meritxell},
	Booktitle = {Wireless Communication Systems, 2009. ISWCS 2009. 6th International Symposium on},
	Organization = {IEEE},
	Pages = {26--30},
	Title = {Linear precoding for mutual information maximization in MIMO systems},
	Year = {2009}}

@misc{GitHub_AMP_Aubin,
	Author = {Benjamin Aubin and Antoine Maillard and Jean Barbier and Florent Krzakala and Nicolas Macris and Lenka Zdeborov\'a},
	Howpublished = {\url{https://github.com/benjaminaubin/TheCommitteeMachine}},
	Journal = {GitHub repository},
	Publisher = {GitHub},
	Title = {{AMP} implementation of the committee machine},
	Year = {2018}}

@article{SM_Arxiv_Aubin2018,
	Author = {Aubin, Benjamin and Maillard, Antoine and Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka},
	Date-Added = {2018-10-15 13:46:49 +0000},
	Date-Modified = {2018-10-15 13:50:30 +0000},
	Journal = {arXiv:1806.05451},
	Title = {The committee machine: Computational to statistical gaps in learning a two-layers neural network},
	Year = {2018}}

@incollection{opper1996statistical,
	Author = {Opper, Manfred and Kinzel, Wolfgang},
	Booktitle = {Models of neural networks III},
	Pages = {151--209},
	Publisher = {Springer},
	Title = {Statistical mechanics of generalization},
	Year = {1996}}

@article{seung1992statistical,
	Author = {Seung, Sebastian and Sompolinsky, Haim and Tishby, Naftali},
	Journal = {Physical Review A},
	Number = {8},
	Pages = {6056},
	Publisher = {APS},
	Title = {Statistical mechanics of learning from examples},
	Volume = {45},
	Year = {1992}}

@article{donoho2013accurate,
	Author = {Donoho, David L and Johnstone, Iain and Montanari, Andrea},
	Journal = {IEEE transactions on information theory},
	Number = {6},
	Pages = {3396--3433},
	Publisher = {IEEE},
	Title = {Accurate prediction of phase transitions in compressed sensing via a connection to minimax denoising},
	Volume = {59},
	Year = {2013}}

@article{watkin1993statistical,
	Author = {Watkin, Timothy LH and Rau, Albrecht and Biehl, Michael},
	Journal = {Reviews of Modern Physics},
	Number = {2},
	Pages = {499},
	Publisher = {APS},
	Title = {The statistical mechanics of learning a rule},
	Volume = {65},
	Year = {1993}}

@article{deshpande2015finding,
	Author = {Deshpande, Yash and Montanari, Andrea},
	Journal = {Foundations of Computational Mathematics},
	Number = {4},
	Pages = {1069--1128},
	Publisher = {Springer},
	Title = {Finding Hidden Cliques of Size $ $$\backslash$sqrt $\{$N/e$\}$ $ $ N/e in Nearly Linear Time},
	Volume = {15},
	Year = {2015}}

@article{schwarze1992generalization,
	Author = {Schwarze, Henry and Hertz, John},
	Journal = {EPL (Europhysics Letters)},
	Number = {4},
	Pages = {375},
	Publisher = {IOP Publishing},
	Title = {Generalization in a large committee machine},
	Volume = {20},
	Year = {1992}}

@article{schwarze1993generalization,
	Author = {Schwarze, Henry and Hertz, John},
	Journal = {EPL (Europhysics Letters)},
	Number = {7},
	Pages = {785},
	Publisher = {IOP Publishing},
	Title = {Generalization in fully connected committee machines},
	Volume = {21},
	Year = {1993}}

@article{monasson1995weight,
	Author = {Monasson, R{\'e}mi and Zecchina, Riccardo},
	Journal = {Physical review letters},
	Number = {12},
	Pages = {2432},
	Publisher = {APS},
	Title = {Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
	Volume = {75},
	Year = {1995}}

@article{opper1994learning,
	Author = {Opper, Manfred},
	Journal = {Physical review letters},
	Number = {13},
	Pages = {2113},
	Publisher = {APS},
	Title = {Learning and generalization in a two-layer neural network: The role of the Vapnik-Chervonvenkis dimension},
	Volume = {72},
	Year = {1994}}

@book{vapnik1998statistical,
	Author = {Vapnik, Vladimir},
	Publisher = {Wiley, New York},
	Title = {Statistical learning theory. 1998},
	Year = {1998}}

@article{martin2017rethinking,
	Author = {Martin, Charles H and Mahoney, Michael W},
	Journal = {arXiv preprint arXiv:1710.09553},
	Title = {Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior},
	Year = {2017}}

@article{thouless1977solution,
	Author = {Thouless, David J and Anderson, Philip W and Palmer, Robert G},
	Journal = {Philosophical Magazine},
	Number = {3},
	Pages = {593--601},
	Publisher = {Taylor \& Francis},
	Title = {Solution of'solvable model of a spin glass'},
	Volume = {35},
	Year = {1977}}

@article{Baldassi26062007,
	Author = {Baldassi, Carlo and Braunstein, Alfredo and Brunel, Nicolas and Zecchina, Riccardo},
	Doi = {10.1073/pnas.0700324104},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {26},
	Pages = {11079-11084},
	Title = {Efficient supervised learning in networks with binary synapses},
	Volume = {104},
	Year = {2007}
	}

@book{mezard1987spin,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio and Virasoro, Miguel},
	Publisher = {World Scientific Publishing Company},
	Title = {Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
	Volume = {9},
	Year = {1987}}

@book{mezard2009information,
	Author = {M{\'e}zard, Marc and Montanari, Andrea},
	Publisher = {Oxford University Press},
	Title = {Information, physics, and computation},
	Year = {2009}}

@article{donoho2009message,
	Author = {Donoho, David L and Maleki, Arian and Montanari, Andrea},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {45},
	Pages = {18914--18919},
	Publisher = {National Acad Sciences},
	Title = {Message-passing algorithms for compressed sensing},
	Volume = {106},
	Year = {2009}}

@inproceedings{rangan2011generalized,
	Author = {Rangan, Sundeep},
	Booktitle = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on},
	Organization = {IEEE},
	Pages = {2168--2172},
	Title = {Generalized approximate message passing for estimation with random linear mixing},
	Year = {2011}}

@inproceedings{arXiv:1207.3859,
	Author = {Kamilov, Ulugbek and Rangan, Sundeep and Unser, Michael and Fletcher, Alyson K},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {2438--2446},
	Title = {Approximate message passing with consistent parameter estimation and applications to sparse learning},
	Year = {2012}}

@article{bayati2015universality,
	Author = {Bayati, Mohsen and Lelarge, Marc and Montanari, Andrea and others},
	Journal = {The Annals of Applied Probability},
	Number = {2},
	Pages = {753--822},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Universality in polytope phase transitions and message passing algorithms},
	Volume = {25},
	Year = {2015}}

@article{javanmard2013state,
	Author = {Javanmard, Adel and Montanari, Andrea},
	Journal = {Information and Inference: A Journal of the IMA},
	Number = {2},
	Pages = {115--144},
	Publisher = {Oxford University Press},
	Title = {State evolution for general approximate message passing algorithms, with applications to spatial coupling},
	Volume = {2},
	Year = {2013}}

@book{hartman1982ordinary,
	Author = {Hartman, P.},
	Series = {Classics in Applied Mathematics},
	Title = {Ordinary Differential Equations: Second Edition},
	Year = {1982}
	}

@article{BarbierM17a,
	Author = {Barbier, Jean and Macris, Nicolas},
	Journal = {Probability Theory and Related Fields},
	Pages = {1--53},
	Publisher = {Springer},
	Title = {The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference},
	Year = {2018}}

@article{barbier_ieee_replicaCS,
	Author = {Jean Barbier and Nicolas Macris and Mohamad Dia and Florent Krzakala},
	Journal = {arXiv preprint arXiv:1701.05823},
	Title = {Mutual Information and Optimality of Approximate Message-Passing in Random Linear Estimation},
	Year = {2017}}

@inproceedings{el2017decoding,
	Author = {El Alaoui, Ahmed and Ramdas, Aaditya and Krzakala, Florent and Zdeborov{\'a}, Lenka and Jordan, Michael I},
	Booktitle = {Information Theory (ISIT), 2017 IEEE International Symposium on},
	Organization = {IEEE},
	Pages = {2780--2784},
	Title = {Decoding from pooled data: Phase transitions of message passing},
	Year = {2017}}

@article{zhu2017performance,
	Author = {Zhu, Junan and Baron, Dror and Krzakala, Florent},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {9},
	Pages = {2444--2454},
	Publisher = {IEEE},
	Title = {Performance limits for noisy multimeasurement vector problems},
	Volume = {65},
	Year = {2017}}

@article{saad1995line,
	Author = {Saad, David and Solla, Sara A},
	Journal = {Physical Review E},
	Number = {4},
	Pages = {4225},
	Publisher = {APS},
	Title = {On-line learning in soft committee machines},
	Volume = {52},
	Year = {1995}}

@article{MatoParga92,
	Abstract = {Generalization properties of multilayered neural networks with binary couplings are studied in the high-temperature limit. The transition to the perfect generalization phase is evaluated for systems with an arbitrary number of layers. It is found that the thermodynamic transition occurs for a number of examples lower than for the perceptron, but the opposite occurs for the transition in which the poor generalization solution disappears. The generalization error is also decomposed according to the contributions coming from different numbers of hidden neurons that have a wrong sign in the internal field. This allows the authors to describe the generalization behaviour of the hidden neurons.},
	Author = {German Mato and Nestor Parga},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {19},
	Pages = {5047},
	Title = {Generalization properties of multilayered neural networks},
	Url = {http://stacks.iop.org/0305-4470/25/i=19/a=017},
	Volume = {25},
	Year = {1992},
	Bdsk-Url-1 = {http://stacks.iop.org/0305-4470/25/i=19/a=017}}

@article{monasson1995learning,
	Author = {Monasson, R{\'e}mi and Zecchina, Riccardo},
	Journal = {Modern Physics Letters B},
	Number = {30},
	Pages = {1887--1897},
	Publisher = {World Scientific},
	Title = {Learning and generalization theories of large committee-machines},
	Volume = {9},
	Year = {1995}}

@article{opper1996mean,
	Author = {Opper, Manfred and Winther, Ole},
	Journal = {Physical review letters},
	Number = {11},
	Pages = {1964},
	Publisher = {APS},
	Title = {Mean field approach to Bayes learning in feed-forward neural networks},
	Volume = {76},
	Year = {1996}}

@article{wainwright2008graphical,
	Author = {Wainwright, Martin J and Jordan, Michael I and others},
	Journal = {Foundations and Trends{\textregistered} in Machine Learning},
	Number = {1--2},
	Pages = {1--305},
	Publisher = {Now Publishers, Inc.},
	Title = {Graphical models, exponential families, and variational inference},
	Volume = {1},
	Year = {2008}}

@inproceedings{choromanska2015open,
	Author = {Choromanska, Anna and LeCun, Yann and Arous, G{\'e}rard Ben},
	Booktitle = {Conference on Learning Theory},
	Pages = {1756--1760},
	Title = {Open problem: The landscape of the loss surfaces of multilayer networks},
	Year = {2015}}

@article{sagun2014explorations,
	Author = {Sagun, Levent and Guney, V Ugur and Arous, Gerard Ben and LeCun, Yann},
	Journal = {arXiv preprint arXiv:1412.6615},
	Title = {Explorations on high dimensional landscapes},
	Year = {2014}}

@book{talagrand2003spin,
	Author = {Talagrand, Michel},
	Publisher = {Springer Science \& Business Media},
	Title = {Spin glasses: a challenge for mathematicians: cavity and mean field models},
	Volume = {46},
	Year = {2003}}

@article{Barbier2017ApproximateMD,
	Author = {Jean Barbier and Florent Krzakala},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {4894-4927},
	Title = {Approximate Message-Passing Decoder and Capacity Achieving Sparse Superposition Codes},
	Volume = {63},
	Year = {2017}}

@article{gyorgyi1990first,
	Author = {Gy{\"o}rgyi, G{\'e}za},
	Journal = {Physical Review A},
	Number = {12},
	Pages = {7097},
	Publisher = {APS},
	Title = {First-order transition to perfect generalization in a neural network with binary synapses},
	Volume = {41},
	Year = {1990}}

@article{2017arXiv171101682M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171101682M},
	Archiveprefix = {arXiv},
	Author = {{Montanari}, A. and {Venkataramanan}, R.},
	Eprint = {1711.01682},
	Journal = {ArXiv e-prints},
	Keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	Month = nov,
	Primaryclass = {math.ST},
	Title = {{Estimation of Low-Rank Matrices via Approximate Message Passing}},
	Year = 2017}

@article{GuoShamaiVerdu_IMMSE,
	Author = {Dongning Guo and S. Shamai and S. Verd{\'u}},
	Doi = {10.1109/TIT.2005.844072},
	Issn = {0018-9448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Gaussian channels;Gaussian noise;information theory;least mean squares methods;nonlinear estimation;nonlinear filters;smoothing methods;Gaussian channels;MMSE;SNR;Wiener process;additive Gaussian noise channel;arbitrarily distributed finite-power input signal;continuous-time nonlinear estimation;minimum mean-square error;mutual information;noncausal smoothing;nonlinear filtering;signal-to-noise ratio;Additive noise;Filtering;Gaussian channels;Gaussian noise;Mutual information;Network address translation;Power filters;Signal to noise ratio;Smoothing methods;Statistics;Gaussian channel;Wiener process;minimum mean-square error (MMSE);mutual information;nonlinear filtering;optimal estimation;smoothing},
	Month = {April},
	Number = {4},
	Pages = {1261-1282},
	Title = {Mutual information and minimum mean-square error in Gaussian channels},
	Volume = {51},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIT.2005.844072}}

@article{Advani2016b,
	Author = {Advani, Madhu and Ganguli, Surya},
	Date-Added = {2020-04-02 14:09:46 +0000},
	Date-Modified = {2020-04-02 14:09:46 +0000},
	Doi = {10.1103/PhysRevX.6.031034},
	Issn = {21603308},
	Journal = {Physical Review X},
	Keywords = {complex systems,interdisciplinary physics},
	Number = {3},
	Pages = {1--16},
	Title = {{Statistical mechanics of optimal convex inference in high dimensions}},
	Volume = {6},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevX.6.031034}}

@article{Donoho2016,
	Abstract = {In a recent article, El Karoui et al. (Proc Natl Acad Sci 110(36):14557--14562, 2013) study the distribution of robust regression estimators in the regime in which the number of parameters p is of the same order as the number of samples n. Using numerical simulations and `highly plausible' heuristic arguments, they unveil a striking new phenomenon. Namely, the regression coefficients contain an extra Gaussian noise component that is not explained by classical concepts such as the Fisher information matrix. We show here that that this phenomenon can be characterized rigorously using techniques that were developed by the authors for analyzing the Lasso estimator under high-dimensional asymptotics. We introduce an approximate message passing (AMP) algorithm to compute M-estimators and deploy state evolution to evaluate the operating characteristics of AMP and so also M-estimates. Our analysis clarifies that the `extra Gaussian noise' encountered in this problem is fundamentally similar to phenomena already studied for regularized least squares in the setting n{\textless} p.},
	Author = {Donoho, David and Montanari, Andrea},
	Issn = {01788051},
	Journal = {Probability Theory and Related Fields},
	Number = {3-4},
	Pages = {935--969},
	Title = {{High dimensional robust M-estimation: asymptotic variance via approximate message passing}},
	Volume = {166},
	Year = {2016}
	}

@article{Bean2013,
	Abstract = {We consider, in the modern setting of high-dimensional statistics, the classic problem of optimizing the objective function in regression using M-estimates when the error distribution is assumed to be known. We propose an algorithm to compute this optimal objective function that takes into account the dimensionality of the problem. Although optimality is achieved under assumptions on the design matrix that will not always be satisfied, our analysis reveals generally interesting families of dimension-dependent objective functions.},
	Author = {Bean, Derek and Bickel, Peter J. and {El Karoui}, Noureddine and Yu, Bin},
	Date-Added = {2020-04-02 14:08:51 +0000},
	Date-Modified = {2020-04-02 14:08:51 +0000},
	Doi = {10.1073/pnas.1307845110},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/14563.full.pdf:pdf},
	Issn = {10916490},
	Journal = {Proceedings of the National Academy of Sciences of the United States of America},
	Keywords = {Prox function,Robust regression},
	Number = {36},
	Pages = {14563--14568},
	Title = {{Optimal M-estimation in high-dimensional regression}},
	Volume = {110},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1073/pnas.1307845110}}

@book{engel2001statistical,
	Author = {Engel, Andreas and Van den Broeck, Christian},
	Publisher = {Cambridge University Press},
	Title = {Statistical mechanics of learning},
	Year = {2001}}

@article{gerbelot2020asymptotic,
	Author = {Gerbelot, C{\'e}dric and Abbara, Alia and Krzakala, Florent},
	Journal = {arXiv preprint arXiv:2002.04372},
	Title = {Asymptotic errors for convex penalized linear regression beyond Gaussian matrices},
	Year = {2020}}

@article{Abbara2019,
	Author = {Abbara, Alia and Aubin, Benjamin and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Journal = {arXiv preprint arXiv:1912.02729},
	Title = {Rademacher complexity and spin glasses: A link between the replica and statistical theories of learning},
	Year = {2019}}

@article{Barbier2017b,
	Abstract = {We consider generalized linear models (GLMs) where an unknown {\$}n{\$}-dimensional signal vector is observed through the application of a random matrix and a non-linear (possibly probabilistic) componentwise output function. We consider the models in the high-dimensional limit, where the observation consists of m points, and {\$}m/n\backslashto\backslashalpha{\$} where {\$}\backslashalpha{\$} stays finite in the limit {\$}m,n\backslashto\backslashinfty{\$}. This situation is ubiquitous in applications ranging from supervised machine learning to signal processing. A substantial amount of theoretical work analyzed the model-case when the observation matrix has i.i.d. elements and the components of the ground-truth signal are taken independently from some known distribution. While statistical physics provided number of explicit conjectures for special cases of this model, results existing for non-linear output functions were so far non-rigorous. At the same time GLMs with non-linear output functions are used as a basic building block of powerful multilayer feedforward neural networks. Therefore rigorously establishing the formulas conjectured for the mutual information is a key open problem that we solve in this paper. We also provide an explicit asymptotic formula for the optimal generalization error, and confirm the prediction of phase transitions in GLMs. Analyzing the resulting formulas for several non-linear output functions, including the rectified linear unit or modulus functions, we obtain quantitative descriptions of information-theoretic limitations of high-dimensional inference. Our proof technique relies on a new version of the interpolation method with an adaptive interpolation path and is of independent interest. Furthermore we show that a polynomial-time algorithm referred to as generalized approximate message-passing reaches the optimal generalization error for a large set of parameters.},
	Archiveprefix = {arXiv},
	Arxivid = {1708.03395},
	Author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'{e}}o and Zdeborov{\'{a}}, Lenka},
	Date-Added = {2019-06-19 19:51:45 +0000},
	Date-Modified = {2019-06-19 19:51:45 +0000},
	Eprint = {1708.03395},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Barbier et al. - 2017 - Phase Transitions, Optimal Errors and Optimality of Message-Passing in Generalized Linear Models.pdf:pdf},
	Pages = {1--59},
	Title = {{Phase Transitions, Optimal Errors and Optimality of Message-Passing in Generalized Linear Models}},
	Url = {http://arxiv.org/abs/1708.03395},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1708.03395}}

@misc{schulke2016statistical,
 title={Statistical physics of linear and bilinear inference problems},
  author={Sch{\"u}lke, Christophe},
  journal={arXiv preprint arXiv:1607.00675},
  year={2016}
  }

@article{Opper1991,
	Abstract = {The generalization error of the Bayes optimal classification algorithm when learning a perceptron from noise-free random training examples is calculated exactly using methods of statistical mechanics. It is shown that if an assumption of replica symmetry is made, then, in the thermodynamic limit, the error of the Bayes optimal algorithm is less than the error of a canonical stochastic learning algorithm, by a factor approaching.},
	Author = {Opper, Manfred and Haussler, David},
	Doi = {10.1103/PhysRevLett.66.2677},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.66.2677.pdf:pdf},
	Issn = {00319007},
	Journal = {Physical Review Letters},
	Number = {20},
	Pages = {2677--2680},
	Title = {{Generalization performance of Bayes optimal classification algorithm for learning a perceptron}},
	Volume = {66},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.66.2677}}

@article{Kinouchi1996,
	Abstract = {A variational approach to the study of learning a linearly separable rule by a single-layer perceptron leads to a gradient descent learning algorithm with exactly the same generalization ability as the Bayes limit calculated by Opper and Haussler [Phys. Rev. Lett. 66, 2677 (1991)]. This is done by finding, through the Gardner-Derrida replica method, the student-teacher overlap [Formula Presented] as a functional of the algorithm cost function and maximizing this functional. The resulting cost function is closely related to the optimal cost function derived for on-line learning. {\textcopyright} 1996 The American Physical Society.},
	Author = {Kinouchi, Osame and Caticha, Nestor},
	Doi = {10.1103/PhysRevE.54.R54},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.54.R54.pdf:pdf},
	Issn = {1063651X},
	Journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
	Number = {1},
	Pages = {R54--R57},
	Title = {{Learning algorithm that gives the bayes generalization limit for perceptrons}},
	Volume = {54},
	Year = {1996},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.54.R54}}

@article{Advani2016,
	Abstract = {When recovering an unknown signal from noisy measurements, the computational difficulty of performing optimal Bayesian MMSE (minimum mean squared error) inference often necessitates the use of maximum a posteriori (MAP) inference, a special case of regularized M-estimation, as a surrogate. However, MAP is suboptimal in high dimensions, when the number of unknown signal components is similar to the number of measurements. In this work we demonstrate, when the signal distribution and the likelihood function associated with the noise are both log-concave, that optimal MMSE performance is asymptotically achievable via another M-estimation procedure. This procedure involves minimizing convex loss and regularizer functions that are nonlinearly smoothed versions of the widely applied MAP optimization problem. Our findings provide a new heuristic derivation and interpretation for recent optimal M-estimators found in the setting of linear measurements and additive noise, and further extend these results to nonlinear measurements with non-additive noise. We numerically demonstrate superior performance of our optimal M-estimators relative to MAP. Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration underlying MMSE inference, and high dimensional convex optimization underlying M-estimation. In essence we show that the former difficult integral may be computed by solving the latter, simpler optimization problem.},
	Archiveprefix = {arXiv},
	Arxivid = {1609.07060},
	Author = {Advani, Madhu and Ganguli, Surya},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/6599-an-equivalence-between-high-dimensional-bayes-optimal-inference-and-m-estimation.pdf:pdf},
	Issn = {10495258},
	Journal = {Advances in Neural Information Processing Systems},
	Number = {1},
	Pages = {3386--3394},
	Title = {{An equivalence between high dimensional Bayes optimal inference and M-estimation}},
	Year = {2016}}

@book{ModelofneuralnetworksIII,
	Author = {Axelrad, H and Eckmiller, R and Hertz, J A and Hopfield, J J and Sherrington, D and Virasoro, M A},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/(Physics of Neural Networks) Andreas V. M. Herz (auth.), Professor Eytan Domany, Professor Dr. J. Leo van Hemmen, Professor Klaus Schult.pdf:pdf},
	Isbn = {9781461268826},
	Title = {{Model of neural networks III}}}

@article{OpperLearningToGeneralize,
	Author = {Opper, Manfred},
	File = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Op01.pdf:pdf},
	Pages = {763--776},
	Title = {{Learning to Generalize ................................................}}}

@article{Mignacco2020,
	Abstract = {We consider a high-dimensional mixture of two Gaussians in the noisy regime where even an oracle knowing the centers of the clusters misclassifies a small but finite fraction of the points. We provide a rigorous analysis of the generalization error of regularized convex classifiers, including ridge, hinge and logistic regression, in the high-dimensional limit where the number {\$}n{\$} of samples and their dimension {\$}d{\$} go to infinity while their ratio is fixed to {\$}\backslashalpha= n/d{\$}. We discuss surprising effects of the regularization that in some cases allows to reach the Bayes-optimal performances. We also illustrate the interpolation peak at low regularization, and analyze the role of the respective sizes of the two clusters.},
  title={The role of regularization in classification of high-dimensional noisy Gaussian mixture},
  author={Mignacco, Francesca and Krzakala, Florent and Lu, Yue M and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:2002.11544},
  year={2020}
	}


@article{Nishimori_1980,
	Abstract = {The random Ising model with competing interactions is investigated on the basis of the gauge-invariant formulation of the problem. Exact results for the internal energy, specific heat and gauge-invariant correlation function are derived. The critical exponent alpha is shown to be negative at the phase boundary of the paramagnetic and ferromagnetic phases if the latter exists at fairly low concentration of antiferromagnetic bonds.},
	Author = {H Nishimori},
	Doi = {10.1088/0022-3719/13/21/012},
	Journal = {Journal of Physics C: Solid State Physics},
	Month = {jul},
	Number = {21},
	Pages = {4071--4076},
	Publisher = {{IOP} Publishing},
	Title = {Exact results and critical properties of the Ising model with competing interactions},
	Url = {https://doi.org/10.1088%2F0022-3719%2F13%2F21%2F012},
	Volume = {13},
	Year = 1980,
	Bdsk-Url-1 = {https://doi.org/10.1088%2F0022-3719%2F13%2F21%2F012},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/0022-3719/13/21/012}}

@misc{Montanari19,
	Archiveprefix = {arXiv},
	Arxivid = {1911.01544},
	Author = {Andrea Montanari and Feng Ruan and Youngtak Sohn and Jun Yan},
	Primaryclass = {math.ST},
	Title = {The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime},
	Year = {2019}}

@article{Thrampoulidis16,
	Author = {Thrampoulidis, Christos and Abbasi, Ehsan and Hassibi, Babak},
	Journal = {IEEE Transactions on Information Theory},
	Number = {8},
	Pages = {5592--5628},
	Publisher = {IEEE},
	Title = {Precise Error Analysis of Regularized $ M $-Estimators in High Dimensions},
	Volume = {64},
	Year = {2018}}

@misc{Hastie19,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1903.08560},
	Author = {Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
	Primaryclass = {math.ST},
	Title = {Surprises in High-Dimensional Ridgeless Least Squares Interpolation},
	Year = {2019}}

@misc{Candes18,
  title={The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression},
  author={Cand{\`e}s, Emmanuel J and Sur, Pragya and others},
  journal={The Annals of Statistics},
  volume={48},
  number={1},
  pages={27--42},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}


@article{engel1993statistical,
	Author = {Engel, A and Fink, W},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {23},
	Pages = {6893},
	Publisher = {IOP Publishing},
	Title = {Statistical mechanics calculation of Vapnik-Chervonenkis bounds for perceptrons},
	Volume = {26},
	Year = {1993}}

@article{gribonval2011should,
	Author = {Gribonval, R{\'e}mi},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {5},
	Pages = {2405--2410},
	Publisher = {IEEE},
	Title = {Should penalized least squares regression be interpreted as maximum a posteriori estimation?},
	Volume = {59},
	Year = {2011}}

@incollection{NIPS2013_4868,
	Author = {Gribonval, Remi and Machart, Pierre},
	Booktitle = {Advances in Neural Information Processing Systems 26},
	Pages = {2193--2201},
	Publisher = {Curran Associates, Inc.},
	Title = {Reconciling "priors" \&amp; "priors" without prejudice?},
	Year = {2013}}

@article{gribonval2019bayesian,
	Author = {Gribonval, R{\'e}mi and Nikolova, Mila},
	Journal = {Applied and Computational Harmonic Analysis},
	Publisher = {Elsevier},
	Title = {On bayesian estimation and proximity operators},
	Year = {2019}}

@article{gribonval2018characterization,
	Author = {Gribonval, R{\'e}mi and Nikolova, Mila},
	Journal = {arXiv preprint arXiv:1807.04014},
	Title = {A characterization of proximity operators},
	Year = {2018}}

@article{scikit-learn,
	Author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	Journal = {Journal of Machine Learning Research},
	Pages = {2825--2830},
	Title = {Scikit-learn: Machine Learning in {P}ython},
	Volume = {12},
	Year = {2011}}

@article{huber1964,
	Author = {Huber, Peter J.},
	Doi = {10.1214/aoms/1177703732},
	Fjournal = {Annals of Mathematical Statistics},
	Journal = {Ann. Math. Statist.},
	Month = {03},
	Number = {1},
	Pages = {73--101},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Robust Estimation of a Location Parameter},
	Url = {https://doi.org/10.1214/aoms/1177703732},
	Volume = {35},
	Year = {1964},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177703732},
	Bdsk-Url-2 = {http://dx.doi.org/10.1214/aoms/1177703732}}

@article{Cover1965,
	Author = {T. M. {Cover}},
	Journal = {IEEE Transactions on Electronic Computers},
	Number = {3},
	Pages = {326-334},
	Title = {Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition},
	Volume = {EC-14},
	Year = {1965}}

@book{Kinzel96,
	Author = {M. Opper and W. Kinzel},
	Chapter = {Statistical mechanics of generalization},
	Isbn = {9781461268826},
	Pages = {151--209},
	Publisher = {Springer},
	Title = {Models of neural networks III},
	Year = {1996}}

@book{vapnik2006estimation,
	Author = {Vapnik, Vladimir},
	Publisher = {Springer Science \& Business Media},
	Title = {Estimation of dependences based on empirical data},
	Year = {2006}}

@article{Geiger19,
	Author = {Geiger, Mario and Spigler, Stefano and d' Ascoli, St{\'e}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
	Doi = {10.1103/physreve.100.012115},
	Issn = {2470-0053},
	Journal = {Physical Review E},
	Month = {Jul},
	Number = {1},
	Publisher = {American Physical Society (APS)},
	Title = {Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
	Url = {http://dx.doi.org/10.1103/PhysRevE.100.012115},
	Volume = {100},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.100.012115},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/physreve.100.012115}}

@misc{Mitra2019,
	Archiveprefix = {arXiv},
	Arxivid = {1906.03667},
	Author = {Partha P Mitra},
	Title = {Understanding overfitting peaks in generalization error: Analytical risk curves for $l_2$ and $l_1$ penalized interpolation},
	Year = {2019}}

@misc{Bartlett98,
	Author = {Peter Bartlett and John Shawe-taylor},
	Title = {Generalization Performance of Support Vector Machines and Other Pattern Classifiers},
	Year = {1998}}

@incollection{Rosset04,
	Author = {Rosset, Saharon and Zhu, Ji and Trevor J. Hastie},
	Booktitle = {Advances in Neural Information Processing Systems 16},
	Pages = {1237--1244},
	Publisher = {MIT Press},
	Title = {Margin Maximizing Loss Functions},
	Url = {http://papers.nips.cc/paper/2433-margin-maximizing-loss-functions.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/2433-margin-maximizing-loss-functions.pdf}}

@article{bayati2011lasso,
	Author = {Bayati, Mohsen and Montanari, Andrea},
	Journal = {IEEE Transactions on Information Theory},
	Number = {4},
	Pages = {1997--2017},
	Publisher = {IEEE},
	Title = {The LASSO risk for Gaussian matrices},
	Volume = {58},
	Year = {2011}}

@article{deng2019model,
	Author = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
	Journal = {arXiv preprint arXiv:1911.05822},
	Title = {A Model of Double Descent for High-dimensional Binary Linear Classification},
	Year = {2019}}

@article{kini2020analytic,
	Author = {Kini, Ganesh and Thrampoulidis, Christos},
	Journal = {arXiv preprint arXiv:2001.11572},
	Title = {Analytic Study of Double Descent in Binary Classification: The Impact of Loss},
	Year = {2020}}


@article{belkin2019two,
	Author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
	Journal = {arXiv preprint arXiv:1903.07571},
	Title = {Two models of double descent for weak features},
	Year = {2019}}

@article{el2013robust,
	Author = {El Karoui, Noureddine and Bean, Derek and Bickel, Peter J and Lim, Chinghway and Yu, Bin},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {36},
	Pages = {14557--14562},
	Publisher = {National Acad Sciences},
	Title = {On robust regression with high-dimensional predictors},
	Volume = {110},
	Year = {2013}}

@article{mezard1989space,
	Author = {M{\'e}zard, Marc},
	Journal = {Journal of Physics A: Mathematical and General},
	Number = {12},
	Pages = {2181},
	Publisher = {IOP Publishing},
	Title = {The space of interactions in neural networks: Gardner's computation with the cavity method},
	Volume = {22},
	Year = {1989}}

@inproceedings{kabashima2004bp,
	Author = {Kabashima, Yoshiyuki and Uda, Shinsuke},
	Booktitle = {International Conference on Algorithmic Learning Theory},
	Organization = {Springer},
	Pages = {479--493},
	Title = {A BP-based algorithm for performing Bayesian inference in large perceptron-type networks},
	Year = {2004}}

@article{geiger2020scaling,
	Author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {2},
	Pages = {023401},
	Publisher = {IOP Publishing},
	Title = {Scaling description of generalization with number of parameters in deep learning},
	Volume = {2020},
	Year = {2020}}

@misc{StructuredPrior_demo_repo,
	Author = {Benjamin Aubin and Bruno Loureiro and Antoine Maillard and Florent Krzakala and Lenka Zdeborov{\'a}},
	Year = {2019},
	Howpublished = {\url{https://github.com/benjaminaubin/StructuredPrior_demo}},
	Title = {Demonstration codes - The spiked matrix model with generative priors},
	Url = {https://github.com/benjaminaubin/StructuredPrior_demo},
	Bdsk-Url-1 = {https://github.com/benjaminaubin/StructuredPrior_demo}}

@article{weyl1949inequalities,
	Author = {Weyl, Hermann},
	Journal = {Proceedings of the National Academy of Sciences of the United States of America},
	Number = {7},
	Pages = {408},
	Publisher = {National Academy of Sciences},
	Title = {Inequalities between the two kinds of eigenvalues of a linear transformation},
	Volume = {35},
	Year = {1949}}

@book{voiculescu1992free,
	Author = {Voiculescu, Dan V and Dykema, Ken J and Nica, Alexandru},
	Number = {1},
	Publisher = {American Mathematical Soc.},
	Title = {Free random variables},
	Year = {1992}}

@article{repo_GenerativePriors,
	Date-Added = {2019-05-16 14:37:17 +0000},
	Date-Modified = {2019-05-16 14:44:19 +0000},
	Title = {AMP, SE and lAMP implementations},
	Year = {2019}}

@article{berthier2017state,
	Author = {Berthier, Raphael and Montanari, Andrea and Nguyen, Phan-Minh},
	Journal = {Information and Inference: A Journal of the IMA},
	Note = {preprint arXiv:1708.03950},
	Title = {State evolution for approximate message passing with non-separable functions},
	Year = {2017}}

@article{miolane2017fundamental,
	Author = {Miolane, L{\'e}o},
	Journal = {arXiv preprint arXiv:1702.00473},
	Title = {Fundamental limits of low-rank matrix estimation: the non-symmetric case},
	Year = {2017}}

@article{baik2005phase,
	Author = {Baik, Jinho and Arous, G{\'e}rard Ben and P{\'e}ch{\'e}, Sandrine and others},
	Journal = {The Annals of Probability},
	Number = {5},
	Pages = {1643--1697},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
	Volume = {33},
	Year = {2005}}

@article{silverstein1995empirical,
	Author = {Silverstein, Jack W and Bai, ZD},
	Journal = {Journal of Multivariate analysis},
	Number = {2},
	Pages = {175--192},
	Publisher = {Elsevier},
	Title = {On the empirical distribution of eigenvalues of a class of large dimensional random matrices},
	Volume = {54},
	Year = {1995}}

@book{dunford1967linear,
	Author = {Dunford, Nelson and Schwartz, Jacob T},
	Publisher = {Interscience Publ.},
	Title = {Linear operators. 2. Spectral theory: self adjoint operators in Hilbert Space},
	Year = {1967}}

@article{silverstein1995analysis,
	Author = {Silverstein, Jack W and Choi, Sang-Il},
	Journal = {Journal of Multivariate Analysis},
	Number = {2},
	Pages = {295--309},
	Publisher = {Elsevier},
	Title = {Analysis of the limiting spectral distribution of large dimensional random matrices},
	Volume = {54},
	Year = {1995}}

@inproceedings{manoel2017multi,
	Author = {Manoel, Andre and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
	Booktitle = {2017 IEEE International Symposium on Information Theory (ISIT)},
	Organization = {IEEE},
	Pages = {2098--2102},
	Title = {Multi-layer generalized linear estimation},
	Year = {2017}}

@article{lee2016tracy,
	Author = {Lee, Ji Oon and Schnelli, Kevin and others},
	Journal = {The Annals of Applied Probability},
	Number = {6},
	Pages = {3786--3839},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Tracy--Widom distribution for the largest eigenvalue of real sample covariance matrices with general population},
	Volume = {26},
	Year = {2016}}

@article{coja-oghlan_information-theoretic_2018,
	Abstract = {Vindicating a sophisticated but non-rigorous physics approach called the cavity method, we establish a formula for the mutual information in statistical inference problems induced by random graphs and we show that the mutual information holds the key to understanding certain important phase transitions in random graph models. We work out several concrete applications of these general results. For instance, we pinpoint the exact condensation phase transition in the Potts antiferromagnet on the random graph, thereby improving prior approximate results (Contucci et al., 2013) [34]. Further, we prove the conjecture from Krzakala et al. (2007) [55] about the condensation phase transition in the random graph coloring problem for any number q≥3 of colors. Moreover, we prove the conjecture on the information-theoretic threshold in the disassortative stochastic block model (Decelle et al., 2011) [35]. Additionally, our general result implies the conjectured formula for the mutual information in Low-Density Generator Matrix codes (Montanari, 2005) [73].},
	Author = {Coja-Oghlan, Amin and Krzakala, Florent and Perkins, Will and Zdeborov{\'a}, Lenka},
	Doi = {10.1016/j.aim.2018.05.029},
	File = {ScienceDirect Snapshot:/Users/florentkrzakala/Zotero/storage/LAMFE39G/S0001870818302044.html:text/html;Submitted Version:/Users/florentkrzakala/Zotero/storage/S9REZCZC/Coja-Oghlan et al. - 2018 - Information-theoretic thresholds from the cavity m.pdf:application/pdf},
	Issn = {0001-8708},
	Journal = {Advances in Mathematics},
	Keywords = {Belief Propagation, Cavity method, Information theoretic thresholds, Random graph coloring, Stochastic block model},
	Month = jul,
	Pages = {694--795},
	Title = {Information-theoretic thresholds from the cavity method},
	Url = {http://www.sciencedirect.com/science/article/pii/S0001870818302044},
	Urldate = {2018-12-10},
	Volume = {333},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0001870818302044},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.aim.2018.05.029}}

@inproceedings{BarbierCS,
	Author = {J. Barbier and M. Dia and N. Macris and F. Krzakala},
	Booktitle = {2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	Doi = {10.1109/ALLERTON.2016.7852290},
	Keywords = {code division multiple access;compressed sensing;least mean squares methods;discrete bounded signals;random Gaussian linear estimation;minimal-mean-square error;state evolution analysis;spatial coupling;I-MMSE theorem;Guerra-type interpolation;statistical physics;heuristic replica method;code division multiple access;sparse superposition codes;compressed sensing;noisy linear random Gaussian projections;random linear estimation;Estimation;Multiaccess communication;Mutual information;Context;Integrated circuits;Physics;Upper bound},
	Month = {Sep.},
	Pages = {625-632},
	Title = {The mutual information in random linear estimation},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ALLERTON.2016.7852290}}

@inproceedings{barbier2016mutual,
	Author = {Barbier, Jean and Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov{\'a}, Lenka},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {424--432},
	Title = {Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula},
	Year = {2016}}

@article{lesieur_statistical_2017,
	Abstract = {We consider tensor factorizations using a generative model and a Bayesian approach. We compute rigorously the mutual information, the Minimal Mean Squared Error (MMSE), and unveil information-theoretic phase transitions. In addition, we study the performance of Approximate Message Passing (AMP) and show that it achieves the MMSE for a large set of parameters, and that factorization is algorithmically "easy" in a much wider region than previously believed. It exists, however, a "hard" region where AMP fails to reach the MMSE and we conjecture that no polynomial algorithm will improve on AMP.},
	Annote = {Comment: 17 pages, 3 figures, 1 table},
	Author = {Lesieur, Thibault and Miolane, L{\'e}o and Lelarge, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Doi = {10.1109/ISIT.2017.8006580},
	File = {arXiv\:1701.08010 PDF:/Users/florentkrzakala/Zotero/storage/AFEACG7W/Lesieur et al. - 2017 - Statistical and computational phase transitions in.pdf:application/pdf;arXiv.org Snapshot:/Users/florentkrzakala/Zotero/storage/B7E4HYEV/1701.html:text/html},
	Journal = {2017 IEEE International Symposium on Information Theory (ISIT)},
	Keywords = {Computer Science - Information Theory, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Statistics Theory},
	Month = jun,
	Note = {arXiv: 1701.08010},
	Pages = {511--515},
	Title = {Statistical and computational phase transitions in spiked tensor estimation},
	Url = {http://arxiv.org/abs/1701.08010},
	Urldate = {2018-12-09},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1701.08010},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/ISIT.2017.8006580}}

@article{krzakala_mutual_2016,
	Abstract = {We consider the estimation of a n-dimensional vector x from the knowledge of noisy and possibility non-linear element-wise measurements of xxT , a very generic problem that contains, e.g. stochastic 2-block model, submatrix localization or the spike perturbation of random matrices. We use an interpolation method proposed by Guerra and later refined by Korada and Macris. We prove that the Bethe mutual information (related to the Bethe free energy and conjectured to be exact by Lesieur et al. on the basis of the non-rigorous cavity method) always yields an upper bound to the exact mutual information. We also provide a lower bound using a similar technique. For concreteness, we illustrate our findings on the sparse PCA problem, and observe that (a) our bounds match for a large region of parameters and (b) that it exists a phase transition in a region where the spectum remains uninformative. While we present only the case of rank-one symmetric matrix estimation, our proof technique is readily extendable to low-rank symmetric matrix or low-rank symmetric tensor estimation},
	Annote = {Comment: 8 pages, 1 figures},
	Author = {Krzakala, Florent and Xu, Jiaming and Zdeborov{\'a}, Lenka},
	Doi = {10.1109/ITW.2016.7606798},
	File = {arXiv\:1603.08447 PDF:/Users/florentkrzakala/Zotero/storage/MTF65ANH/Krzakala et al. - 2016 - Mutual Information in Rank-One Matrix Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/florentkrzakala/Zotero/storage/826YYSCM/1603.html:text/html},
	Journal = {2016 IEEE Information Theory Workshop (ITW)},
	Keywords = {Computer Science - Information Theory, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Statistics Theory},
	Month = sep,
	Note = {arXiv: 1603.08447},
	Pages = {71--75},
	Title = {Mutual {Information} in {Rank}-{One} {Matrix} {Estimation}},
	Url = {http://arxiv.org/abs/1603.08447},
	Urldate = {2018-12-08},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1603.08447},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/ITW.2016.7606798}}

@article{zdeborova_statistical_2016,
	Abstract = {Many questions of fundamental interest in today's science can be formulated as inference problems: some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. The connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which, formulated as an inference problem, we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.},
	Author = {Zdeborov{\'a}, Lenka and Krzakala, Florent},
	Doi = {10.1080/00018732.2016.1211393},
	File = {Snapshot:/Users/florentkrzakala/Zotero/storage/J5C93QAR/00018732.2016.html:text/html;Submitted Version:/Users/florentkrzakala/Zotero/storage/J67ULEW9/Zdeborov{\'a} and Krzakala - 2016 - Statistical physics of inference thresholds and a.pdf:application/pdf},
	Issn = {0001-8732},
	Journal = {Advances in Physics},
	Keywords = {64.60.aq networks, 75.50.Lk magneticproperties of materials: spin glasses and other random magnets, 89.70.Eg interdisciplinary: computational complexity, 89.75.-k complex systems, Bayesian inference, belief propagation, compressed sensing, phase transitions in computer science, spin glass theory, stochastic block model},
	Month = sep,
	Number = {5},
	Pages = {453--552},
	Shorttitle = {Statistical physics of inference},
	Title = {Statistical physics of inference: thresholds and algorithms},
	Url = {https://doi.org/10.1080/00018732.2016.1211393},
	Urldate = {2018-11-30},
	Volume = {65},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1080/00018732.2016.1211393},
	Bdsk-Url-2 = {http://dx.doi.org/10.1080/00018732.2016.1211393}}

@incollection{schulke_blind_2013,
	Author = {Schulke, Christophe and Caltagirone, Francesco and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	File = {NIPS Full Text PDF:/Users/florentkrzakala/Zotero/storage/X8BKL7UK/Schulke et al. - 2013 - Blind Calibration in Compressed Sensing using Mess.pdf:application/pdf;NIPS Snapshot:/Users/florentkrzakala/Zotero/storage/U8BNQMRN/4947-blind-calibration-in-compressed-sensing-using-message-passing-algorithms.html:text/html},
	Pages = {566--574},
	Publisher = {Curran Associates, Inc.},
	Title = {Blind {Calibration} in {Compressed} {Sensing} using {Message} {Passing} {Algorithms}},
	Url = {http://papers.nips.cc/paper/4947-blind-calibration-in-compressed-sensing-using-message-passing-algorithms.pdf},
	Urldate = {2018-11-23},
	Year = {2013},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/4947-blind-calibration-in-compressed-sensing-using-message-passing-algorithms.pdf}}

@article{krzakala_path-integral_2008,
	Author = {Krzakala, Florent and Rosso, Alberto and Semerjian, Guilhem and Zamponi, Francesco},
	Doi = {10.1103/PhysRevB.78.134428},
	File = {Krzakala et al. - 2008 - Path-integral representation for quantum spin mode.pdf:/Users/florentkrzakala/Zotero/storage/Z53PUYJ2/Krzakala et al. - 2008 - Path-integral representation for quantum spin mode.pdf:application/pdf},
	Issn = {1098-0121, 1550-235X},
	Journal = {Physical Review B},
	Language = {en},
	Month = oct,
	Number = {13},
	Shorttitle = {Path-integral representation for quantum spin models},
	Title = {Path-integral representation for quantum spin models: {Application} to the quantum cavity method and {Monte} {Carlo} simulations},
	Url = {https://link.aps.org/doi/10.1103/PhysRevB.78.134428},
	Urldate = {2018-11-22},
	Volume = {78},
	Year = {2008},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevB.78.134428},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevB.78.134428}}

@article{berthet_topics_nodate,
	Author = {Berthet, Quentin},
	File = {Berthet - Topics in Statistical Theory.pdf:/Users/florentkrzakala/Zotero/storage/AGCK84JX/Berthet - Topics in Statistical Theory.pdf:application/pdf},
	Language = {en},
	Pages = {44},
	Title = {Topics in {Statistical} {Theory}}}

@article{mnih_playing_2013,
	Abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	Annote = {Comment: NIPS Deep Learning Workshop 2013},
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	File = {arXiv\:1312.5602 PDF:/Users/florentkrzakala/Zotero/storage/WUX9W9XD/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/florentkrzakala/Zotero/storage/VWTXIMY7/1312.html:text/html},
	Journal = {arXiv:1312.5602 [cs]},
	Keywords = {Computer Science - Machine Learning},
	Month = dec,
	Note = {arXiv: 1312.5602},
	Title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	Url = {http://arxiv.org/abs/1312.5602},
	Urldate = {2018-11-21},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1312.5602}}

@article{silver_mastering_2016,
	Author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	Doi = {10.1038/nature16961},
	File = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:/Users/florentkrzakala/Zotero/storage/HNQSYUNI/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
	Issn = {0028-0836, 1476-4687},
	Journal = {Nature},
	Language = {en},
	Month = jan,
	Number = {7587},
	Pages = {484--489},
	Title = {Mastering the game of {Go} with deep neural networks and tree search},
	Url = {http://www.nature.com/articles/nature16961},
	Urldate = {2018-11-21},
	Volume = {529},
	Year = {2016},
	Bdsk-Url-1 = {http://www.nature.com/articles/nature16961},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/nature16961}}

@article{chertkov_inference_2010,
	Author = {Chertkov, M. and Kroc, L. and Krzakala, F. and Vergassola, M. and Zdeborov{\'a}, L.},
	Doi = {10.1073/pnas.0910994107},
	File = {Chertkov et al. - 2010 - Inference in particle tracking experiments by pass.pdf:/Users/florentkrzakala/Zotero/storage/4FHJXS87/Chertkov et al. - 2010 - Inference in particle tracking experiments by pass.pdf:application/pdf},
	Issn = {0027-8424, 1091-6490},
	Journal = {Proceedings of the National Academy of Sciences},
	Language = {en},
	Month = apr,
	Number = {17},
	Pages = {7663--7668},
	Title = {Inference in particle tracking experiments by passing messages between images},
	Url = {http://www.pnas.org/lookup/doi/10.1073/pnas.0910994107},
	Urldate = {2018-11-21},
	Volume = {107},
	Year = {2010},
	Bdsk-Url-1 = {http://www.pnas.org/lookup/doi/10.1073/pnas.0910994107},
	Bdsk-Url-2 = {http://dx.doi.org/10.1073/pnas.0910994107}}

@article{krzakala_statistical-physics-based_2012,
	Author = {Krzakala, F. and M{\'e}zard, M. and Sausset, F. and Sun, Y. F. and Zdeborov{\'a}, L.},
	Doi = {10.1103/PhysRevX.2.021005},
	File = {Krzakala et al. - 2012 - Statistical-Physics-Based Reconstruction in Compre.pdf:/Users/florentkrzakala/Zotero/storage/G5DURRXC/Krzakala et al. - 2012 - Statistical-Physics-Based Reconstruction in Compre.pdf:application/pdf},
	Issn = {2160-3308},
	Journal = {Physical Review X},
	Language = {en},
	Month = may,
	Number = {2},
	Title = {Statistical-{Physics}-{Based} {Reconstruction} in {Compressed} {Sensing}},
	Url = {https://link.aps.org/doi/10.1103/PhysRevX.2.021005},
	Urldate = {2018-11-21},
	Volume = {2},
	Year = {2012},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevX.2.021005},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevX.2.021005}}

@incollection{deshpande_sparse_2014,
	Author = {Deshpande, Yash and Montanari, Andrea},
	Booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	File = {NIPS Full Text PDF:/Users/florentkrzakala/Zotero/storage/MRSN8G98/Deshpande and Montanari - 2014 - Sparse PCA via Covariance Thresholding.pdf:application/pdf;NIPS Snapshot:/Users/florentkrzakala/Zotero/storage/GRTE9HL7/5406-sparse-pca-via-covariance-thresholding.html:text/html},
	Pages = {334--342},
	Publisher = {Curran Associates, Inc.},
	Title = {Sparse {PCA} via {Covariance} {Thresholding}},
	Url = {http://papers.nips.cc/paper/5406-sparse-pca-via-covariance-thresholding.pdf},
	Urldate = {2018-11-21},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5406-sparse-pca-via-covariance-thresholding.pdf}}

@article{krzakala_gibbs_2007,
	Author = {Krzakala, F. and Montanari, A. and Ricci-Tersenghi, F. and Semerjian, G. and Zdeborova, L.},
	Copyright = {All rights reserved},
	Doi = {10.1073/pnas.0703685104},
	File = {Krzakala et al. - 2007 - Gibbs states and the set of solutions of random co.pdf:/Users/florentkrzakala/Zotero/storage/BTY7Q2B7/Krzakala et al. - 2007 - Gibbs states and the set of solutions of random co.pdf:application/pdf},
	Issn = {0027-8424, 1091-6490},
	Journal = {Proceedings of the National Academy of Sciences},
	Language = {en},
	Month = jun,
	Number = {25},
	Pages = {10318--10323},
	Title = {Gibbs states and the set of solutions of random constraint satisfaction problems},
	Url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0703685104},
	Urldate = {2018-11-20},
	Volume = {104},
	Year = {2007},
	Bdsk-Url-1 = {http://www.pnas.org/cgi/doi/10.1073/pnas.0703685104},
	Bdsk-Url-2 = {http://dx.doi.org/10.1073/pnas.0703685104}}

@article{decelle_asymptotic_2011,
	Author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov{\'a}, Lenka},
	Copyright = {All rights reserved},
	Doi = {10.1103/PhysRevE.84.066106},
	File = {Decelle et al. - 2011 - Asymptotic analysis of the stochastic block model .pdf:/Users/florentkrzakala/Zotero/storage/87FLLU79/Decelle et al. - 2011 - Asymptotic analysis of the stochastic block model .pdf:application/pdf},
	Issn = {1539-3755, 1550-2376},
	Journal = {Physical Review E},
	Language = {en},
	Month = dec,
	Number = {6},
	Title = {Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications},
	Url = {https://link.aps.org/doi/10.1103/PhysRevE.84.066106},
	Urldate = {2018-11-20},
	Volume = {84},
	Year = {2011},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevE.84.066106},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevE.84.066106}}

@article{johnstone2001distribution,
	Author = {Johnstone, Iain M},
	Journal = {Annals of statistics},
	Pages = {295--327},
	Publisher = {JSTOR},
	Title = {On the distribution of the largest eigenvalue in principal components analysis},
	Year = {2001}}

@article{peche2006largest,
	Author = {P{\'e}ch{\'e}, Sandrine},
	Journal = {Probability Theory and Related Fields},
	Number = {1},
	Pages = {127--173},
	Publisher = {Springer},
	Title = {The largest eigenvalue of small rank perturbations of Hermitian random matrices},
	Volume = {134},
	Year = {2006}}

@inproceedings{deshpande2014information,
	Author = {Deshpande, Yash and Montanari, Andrea},
	Booktitle = {2014 IEEE International Symposium on Information Theory},
	Organization = {IEEE},
	Pages = {2197--2201},
	Title = {Information-theoretically optimal sparse {PCA}},
	Year = {2014}}

@inproceedings{rangan2012iterative,
	Author = {Rangan, Sundeep and Fletcher, Alyson K},
	Booktitle = {Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on},
	Organization = {IEEE},
	Pages = {1246--1250},
	Title = {Iterative estimation of constrained rank-one matrices in noise},
	Year = {2012}}

@article{alaoui2017finite,
	Author = {Alaoui, Ahmed El and Krzakala, Florent and Jordan, Michael I},
	Journal = {arXiv preprint arXiv:1710.02903},
	Title = {Finite size corrections and likelihood ratio fluctuations in the spiked {W}igner model},
	Year = {2017}}

@inproceedings{AlaouiKrzakala,
	Author = {Ahmed El Alaoui and Florent Krzakala},
	Booktitle = {2018 IEEE International Symposium on Information Theory (ISIT)},
	Doi = {10.1109/ISIT.2018.8437810},
	Issn = {2157-8117},
	Month = {June},
	Pages = {1874-1878},
	Title = {Estimation in the Spiked {W}igner Model: A Short Proof of the Replica Formula},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISIT.2018.8437810}}

@article{deshpande2016asymptotic,
	Author = {Deshpande, Yash and Abbe, Emmanuel and Montanari, Andrea},
	Journal = {Information and Inference: A Journal of the IMA},
	Number = {2},
	Pages = {125--170},
	Publisher = {Oxford University Press},
	Title = {Asymptotic mutual information for the balanced binary stochastic block model},
	Volume = {6},
	Year = {2016}}

@inproceedings{DBLP:conf/allerton/LesieurKZ15,
	Author = {Thibault Lesieur and Florent Krzakala and Lenka Zdeborov{\'{a}}},
	Booktitle = {53rd Annual Allerton Conference on Communication, Control, and Computing, Allerton 2015, Allerton Park {\&} Retreat Center, Monticello, IL, USA, September 29 - October 2, 2015},
	Pages = {680--687},
	Title = {{MMSE} of probabilistic low-rank matrix estimation: Universality with respect to the output channel},
	Year = {2015}}

@article{franz1995recipes,
	Author = {Franz, Silvio and Parisi, Giorgio},
	Journal = {Journal de Physique I},
	Number = {11},
	Pages = {1401--1415},
	Publisher = {EDP Sciences},
	Title = {Recipes for metastable states in spin glasses},
	Volume = {5},
	Year = {1995}}

@inproceedings{lesieur2015phase,
	Author = {Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Booktitle = {2015 IEEE International Symposium on Information Theory (ISIT)},
	Organization = {IEEE},
	Pages = {1635--1639},
	Title = {Phase transitions in sparse {PCA}},
	Year = {2015}}

@article{lelarge2019fundamental,
	Author = {Lelarge, Marc and Miolane, L{\'e}o},
	Journal = {Probability Theory and Related Fields},
	Number = {3-4},
	Pages = {859--929},
	Publisher = {Springer},
	Title = {Fundamental limits of symmetric low-rank matrix estimation},
	Volume = {173},
	Year = {2019}}

@article{zou2006sparse,
	Author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	Journal = {Journal of computational and graphical statistics},
	Number = {2},
	Pages = {265--286},
	Publisher = {Taylor \& Francis},
	Title = {Sparse principal component analysis},
	Volume = {15},
	Year = {2006}}

@inproceedings{hand2018phase,
	Author = {Hand, Paul and Leong, Oscar and Voroninski, Vlad},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {9136--9146},
	Title = {Phase retrieval under a generative prior},
	Year = {2018}}

@inproceedings{gabrie2018entropy,
	Author = {Gabri\'{e}, Marylou and Manoel, Andre and Luneau, Cl\'{e}ment and Barbier, Jean and Macris, Nicolas and Krzakala, Florent and Zdeborov\'{a}, Lenka},
	Booktitle = {Advances in Neural Information Processing Systems 31},
	Pages = {1821--1831},
	Publisher = {Curran Associates, Inc.},
	Title = {Entropy and mutual information in models of deep neural networks},
	Year = {2018}}

@inproceedings{bora2017compressed,
	Author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	Organization = {JMLR. org},
	Pages = {537--546},
	Title = {Compressed sensing using generative models},
	Year = {2017}}

@inproceedings{deshpande2014sparse,
	Author = {Deshpande, Yash and Montanari, Andrea},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {334--342},
	Title = {Sparse {PCA} via covariance thresholding},
	Year = {2014}}

@article{amini2009high,
	Author = {Amini, Arash A and Wainwright, Martin J},
	Journal = {The Annals of Statistics},
	Pages = {2877--2921},
	Publisher = {JSTOR},
	Title = {High-Dimensional Analysis of Semidefinite Relaxations for Sparse Principal Components},
	Year = {2009}}

@article{berthet2013computational,
	Author = {Berthet, Quentin and Rigollet, Philippe},
	Journal = {arXiv preprint arXiv:1304.0828},
	Title = {Computational lower bounds for sparse {PCA}},
	Year = {2013}}

@article{mixon2018sunlayer,
	Author = {Mixon, Dustin G and Villar, Soledad},
	Journal = {arXiv preprint arXiv:1803.09319},
	Title = {SUNLayer: Stable denoising with generative networks},
	Year = {2018}}

@unpublished{BlogSoledad,
	Author = {Villar, Soledad},
	Date-Modified = {2020-09-15 21:30:53 +0000},
	Title = {Generative models are the new sparsity?},
	Year = {2018}}

@article{benaych2011eigenvalues,
	Author = {Benaych-Georges, Florent and Nadakuditi, Raj Rao},
	Journal = {Advances in Mathematics},
	Number = {1},
	Pages = {494--521},
	Publisher = {Elsevier},
	Title = {The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
	Volume = {227},
	Year = {2011}}

@article{mourrat2019hamilton,
	Author = {Mourrat, Jean-Christophe},
	Journal = {arXiv preprint arXiv:1904.05294},
	Title = {Hamilton-{J}acobi equations for finite-rank matrix inference},
	Year = {2019}}

@inproceedings{matsushita2013low,
	Author = {Matsushita, Ryosuke and Tanaka, Toshiyuki},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {917--925},
	Title = {Low-rank matrix reconstruction and clustering via approximate message passing},
	Year = {2013}}

@article{metzler2016denoising,
	Author = {Metzler, Christopher A and Maleki, Arian and Baraniuk, Richard G},
	Journal = {IEEE Transactions on Information Theory},
	Number = {9},
	Pages = {5117--5144},
	Publisher = {IEEE},
	Title = {From denoising to compressed sensing},
	Volume = {62},
	Year = {2016}}

@article{korada2009exact,
	Author = {Korada, Satish Babu and Macris, Nicolas},
	Journal = {Journal of Statistical Physics},
	Number = {2},
	Pages = {205--230},
	Publisher = {Springer},
	Title = {Exact solution of the gauge symmetric p-spin glass model on a complete graph},
	Volume = {136},
	Year = {2009}}

@article{barbier2018adaptive,
	Author = {Barbier, Jean and Macris, Nicolas},
	Journal = {Probability Theory and Related Fields},
	Pages = {1--53},
	Publisher = {Springer},
	Title = {The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference},
	Year = {2018}}

@misc{tao2009,
	Author = {Terence Tao},
	Howpublished = {Clay-Mahler Lecture Series},
	Title = {Compressed sensing, or: the equation ${A}x=b$, revisited},
	Year = {2009}}

@misc{h2018phase,
	Archiveprefix = {arXiv},
	Author = {Paul Hand and Oscar Leong and Vladislav Voroninski},
	Primaryclass = {cs.IT},
	Title = {Phase Retrieval Under a Generative Prior},
	Year = {2018}}

@article{candes2015phase,
	Author = {Candes, Emmanuel J and Eldar, Yonina C and Strohmer, Thomas and Voroninski, Vladislav},
	Journal = {SIAM review},
	Number = {2},
	Pages = {225--251},
	Title = {Phase retrieval via matrix completion},
	Volume = {57},
	Year = {2015}}

@inproceedings{netrapalli2013phase,
	Author = {Netrapalli, Praneeth and Jain, Prateek and Sanghavi, Sujay},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {2796--2804},
	Title = {Phase retrieval using alternating minimization},
	Year = {2013}}

@article{candes2006near,
	Author = {Candes, Emmanuel J and Tao, Terence},
	Journal = {IEEE transactions on information theory},
	Number = {12},
	Pages = {5406--5425},
	Publisher = {IEEE},
	Title = {Near-optimal signal recovery from random projections: Universal encoding strategies?},
	Volume = {52},
	Year = {2006}}

@book{boucheron2013concentration,
	Author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
	Publisher = {Oxford university press},
	Title = {Concentration inequalities: A nonasymptotic theory of independence},
	Year = {2013}}

@inproceedings{jenatton2010structured,
	Author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
	Booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	Pages = {366--373},
	Title = {Structured sparse principal component analysis},
	Year = {2010}}

@article{tramel2016approximate,
	Author = {Tramel, Eric W and Dr{\'e}meau, Ang{\'e}lique and Krzakala, Florent},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {7},
	Pages = {073401},
	Publisher = {IOP Publishing},
	Title = {Approximate message passing with restricted {B}oltzmann machine priors},
	Volume = {2016},
	Year = {2016}}

@article{Barbier2017c,
	Author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {12},
	Pages = {5451--5460},
	Publisher = {National Acad Sciences},
	Title = {Optimal errors and phase transitions in high-dimensional generalized linear models},
	Volume = {116},
	Year = {2019}}

@incollection{wigner1993characteristic,
	Author = {Wigner, Eugene P},
	Booktitle = {The Collected Works of {E}ugene {P}aul {W}igner},
	Pages = {524--540},
	Publisher = {Springer},
	Title = {Characteristic vectors of bordered matrices with infinite dimensions i},
	Year = {1993}}

@article{marvcenko1967distribution,
	Author = {Mar{\v{c}}enko, Vladimir A and Pastur, Leonid Andreevich},
	Journal = {Mathematics of the USSR-Sbornik},
	Number = {4},
	Pages = {457},
	Publisher = {IOP Publishing},
	Title = {Distribution of eigenvalues for some sets of random matrices},
	Volume = {1},
	Year = {1967}}

@inproceedings{Reeves,
	Author = {G. Reeves and H. D. Pfister},
	Booktitle = {2016 IEEE International Symposium on Information Theory (ISIT)},
	Doi = {10.1109/ISIT.2016.7541382},
	Issn = {2157-8117},
	Keywords = {compressed sensing;Gaussian processes;least mean squares methods;prediction theory;statistical analysis;statistical physics;replica method;MMSE;minimum mean-square error;asymptotic mutual information;i.i.d. Gaussian measurement matrices;i.i.d. signal distributions;compressed sensing;replica-symmetric prediction;Mutual information;Manganese;Measurement uncertainty;Compressed sensing;Noise measurement;Multiaccess communication},
	Month = {July},
	Pages = {665-669},
	Title = {The replica-symmetric prediction for compressed sensing with Gaussian matrices is exact},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISIT.2016.7541382}}

@inproceedings{barbier2016mutual_b,
	Author = {Barbier, Jean and Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov{\'a}, Lenka},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {424--432},
	Title = {Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula},
	Year = {2016}}

@book{richard_s_sutton_reinforcement_1998,
	Author = {{Richard S Sutton} and {Andrew G Barto}},
	File = {Snapshot:/Users/florentkrzakala/Zotero/storage/XEWN2RSM/view.html:text/html},
	Isbn = {978-0-262-19398-6},
	Publisher = {MIT Press},
	Title = {Reinforcement {Learning}: an introduction},
	Url = {https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view?usp=embed_facebook},
	Urldate = {2018-11-21},
	Volume = {135},
	Year = {1998},
	Bdsk-Url-1 = {https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view?usp=embed_facebook}}

@article{silver_mastering_2017,
	Author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	Doi = {10.1038/nature24270},
	File = {Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:/Users/florentkrzakala/Zotero/storage/PHTFHFEE/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
	Issn = {0028-0836, 1476-4687},
	Journal = {Nature},
	Language = {en},
	Month = oct,
	Number = {7676},
	Pages = {354--359},
	Title = {Mastering the game of {Go} without human knowledge},
	Url = {http://www.nature.com/doifinder/10.1038/nature24270},
	Urldate = {2018-11-21},
	Volume = {550},
	Year = {2017},
	Bdsk-Url-1 = {http://www.nature.com/doifinder/10.1038/nature24270},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/nature24270}}

@article{krzakala_spectral_2013,
	Author = {Krzakala, F. and Moore, C. and Mossel, E. and Neeman, J. and Sly, A. and Zdeborov\'a, L. and Zhang, P.},
	Copyright = {All rights reserved},
	Doi = {10.1073/pnas.1312486110},
	File = {Krzakala et al. - 2013 - Spectral redemption in clustering sparse networks.pdf:/Users/florentkrzakala/Zotero/storage/88UUID8Z/Krzakala et al. - 2013 - Spectral redemption in clustering sparse networks.pdf:application/pdf},
	Issn = {0027-8424, 1091-6490},
	Journal = {Proceedings of the National Academy of Sciences},
	Language = {en},
	Month = dec,
	Number = {52},
	Pages = {20935--20940},
	Title = {Spectral redemption in clustering sparse networks},
	Url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1312486110},
	Urldate = {2018-11-21},
	Volume = {110},
	Year = {2013},
	Bdsk-Url-1 = {http://www.pnas.org/cgi/doi/10.1073/pnas.1312486110},
	Bdsk-Url-2 = {http://dx.doi.org/10.1073/pnas.1312486110}}

@article{lesieur2017constrained,
	Author = {Thibault Lesieur and Florent Krzakala and Lenka Zdeborov{\'a}},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {7},
	Pages = {073403},
	Title = {Constrained low-rank matrix estimation: phase transitions, approximate message passing and applications},
	Url = {http://stacks.iop.org/1742-5468/2017/i=7/a=073403},
	Volume = {2017},
	Year = {2017},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-5468/2017/i=7/a=073403}}

@article{olshausen1997sparse,
	Author = {Olshausen, Bruno A and Field, David J},
	Journal = {Vision research},
	Number = {23},
	Pages = {3311--3325},
	Publisher = {Elsevier},
	Title = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
	Volume = {37},
	Year = {1997}}

@article{perry2016optimality,
	Author = {Perry, Amelia and Wein, Alexander S and Bandeira, Afonso S and Moitra, Ankur},
	Journal = {arXiv preprint arXiv:1609.05573},
	Title = {Optimality and sub-optimality of {PCA} for spiked random matrices and synchronization},
	Year = {2016}}

@inproceedings{hand2017global,
	Author = {Hand, Paul and Voroninski, Vladislav},
	Booktitle = {Conference On Learning Theory},
	Pages = {970--978},
	Title = {Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk},
	Year = {2018}}

@article{dudeja2019information,
	Author = {Dudeja, Rishabh and Ma, Junjie and Maleki, Arian},
	Journal = {arXiv preprint arXiv:1910.11849},
	Title = {Information Theoretic Limits for Phase Retrieval with Subsampled {H}aar Sensing Matrices},
	Year = {2019}}

@article{abbara2019universality,
	Author = {Abbara, Alia and Baker, Antoine and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Journal = {arXiv preprint arXiv:1906.04735},
	Title = {On the Universality of Noiseless Linear Estimation with Respect to the Measurement Matrix},
	Year = {2019}}

@inproceedings{kabashima2008inference,
	Author = {Kabashima, Yoshiyuki},
	Booktitle = {Journal of Physics: Conference Series},
	Number = {1},
	Organization = {IOP Publishing},
	Pages = {012001},
	Title = {Inference from correlated patterns: a unified theory for perceptron learning and linear vector channels},
	Volume = {95},
	Year = {2008}}

@incollection{aubin2019spiked,
  title     = {The spiked matrix model with generative priors},
  author    = {Aubin, Benjamin and Loureiro, Bruno and Maillard, Antoine and Krzakala, Florent and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8366--8377},
  year      = {2019},
  url       = {http://papers.nips.cc/paper/9045-the-spiked-matrix-model-with-generative-priors.pdf},
  abstract  = {Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical and algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets.},
    Arxivid={1905.12385},
    Eprint={https://arxiv.org/abs/1905.12385}
}


@article{wu2012optimal,
	Author = {Wu, Yihong and Verd{\'u}, Sergio},
	Journal = {IEEE Transactions on Information Theory},
	Number = {10},
	Pages = {6241--6263},
	Publisher = {IEEE},
	Title = {Optimal phase transitions in compressed sensing},
	Volume = {58},
	Year = {2012}}

@inproceedings{reeves2012compressed,
	Author = {Reeves, Galen and Gastpar, Michael},
	Booktitle = {2012 46th Annual Conference on Information Sciences and Systems (CISS)},
	Organization = {IEEE},
	Pages = {1--6},
	Title = {Compressed sensing phase transitions: Rigorous bounds versus replica predictions},
	Year = {2012}}

@article{schniter2014compressive,
	Author = {Schniter, Philip and Rangan, Sundeep},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {4},
	Pages = {1043--1055},
	Publisher = {IEEE},
	Title = {Compressive phase retrieval via generalized approximate message passing},
	Volume = {63},
	Year = {2014}}

@inproceedings{metzler2017coherent,
	Author = {Metzler, Christopher A and Sharma, Manoj K and Nagesh, Sudarshan and Baraniuk, Richard G and Cossairt, Oliver and Veeraraghavan, Ashok},
	Booktitle = {2017 IEEE International Conference on Computational Photography (ICCP)},
	Organization = {IEEE},
	Pages = {1--16},
	Title = {Coherent inverse scattering via transmission matrices: Efficient phase retrieval algorithms and a public dataset},
	Year = {2017}}

@article{zdeborova2016statistical,
	Author = {Zdeborov{\'a}, Lenka and Krzakala, Florent},
	Journal = {Advances in Physics},
	Number = {5},
	Pages = {453--552},
	Publisher = {Taylor \& Francis},
	Title = {Statistical physics of inference: Thresholds and algorithms},
	Volume = {65},
	Year = {2016}}

@article{dremeau2015reference,
	Author = {Dr{\'e}meau, Ang{\'e}lique and Liutkus, Antoine and Martina, David and Katz, Ori and Sch{\"u}lke, Christophe and Krzakala, Florent and Gigan, Sylvain and Daudet, Laurent},
	Journal = {Optics express},
	Number = {9},
	Pages = {11898--11911},
	Publisher = {Optical Society of America},
	Title = {Reference-less measurement of the transmission matrix of a highly scattering material using a DMD and phase retrieval techniques},
	Volume = {23},
	Year = {2015}}

@inproceedings{reeves2017additivity,
	Author = {Reeves, Galen},
	Booktitle = {2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	Organization = {IEEE},
	Pages = {1064--1070},
	Title = {Additivity of information in multilayer networks via additive Gaussian noise transforms},
	Year = {2017}}

@article{Parikh2014,
	Author = {Parikh, Neal and Boyd, Stephen},
	Doi = {10.1561/2400000003},
	Journal = {Foundations and Trends in Optimization},
	Number = {3},
	Pages = {127--239},
	Title = {{Proximal Algorithms}},
	Volume = {1},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1561/2400000003}}

@article{tan2015compressive,
	Author = {Tan, Jin and Ma, Yanting and Baron, Dror},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {8},
	Pages = {2085--2092},
	Publisher = {IEEE},
	Title = {Compressive imaging via approximate message passing with image denoising},
	Volume = {63},
	Year = {2015}}

@inproceedings{metzler2015optimal,
	Author = {Metzler, Christopher A and Maleki, Arian and Baraniuk, Richard G},
	Booktitle = {2015 International Conference on Sampling Theory and Applications (SampTA)},
	Organization = {IEEE},
	Pages = {508--512},
	Title = {Optimal recovery from compressive measurements via denoising-based approximate message passing},
	Year = {2015}}

@inproceedings{dia2016mutual,
	Author = {Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov{\'a}, Lenka and others},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {424--432},
	Title = {Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula},
	Year = {2016}}

@article{Bishop2013,
	Author = {Bishop, Christopher M.},
	Journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	Keywords = {Bayesian inference,Graphical probabilistic programming,Infer. NET},
	Number = {1984},
	Title = {{Model-based machine learning}},
	Volume = {371},
	Year = {2013}}

@article{Bishop2006JMLR,
	Author = {Bishop, C.M. and Winn, J.},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bayesian networks,message passing,variational inference},
	Number = {1},
	Pages = {661},
	Title = {{Variational message passing}},
	Volume = {6},
	Year = {2006}}

@misc{InferNET18,
	Author = {Minka, T. and Winn, J.M. and Guiver, J.P. and Zaykov, Y. and Fabian, D. and Bronskill, J.},
	Note = {Microsoft Research Cambridge. http://dotnet.github.io/infer},
	Title = {/{Infer.NET 0.3}},
	Year = 2018}

@article{Schreiber2018,
	Author = {Schreiber, Jacob},
	Journal = {Journal of Machine Learning Research},
	Keywords = {Big data,Cython,Machine learning,Probabilistic modeling,Python},
	Pages = {1--6},
	Title = {{pomegranate: Fast and flexible probabilistic modeling in python}},
	Volume = {18},
	Year = {2018}}

@article{Tran2016,
	Archiveprefix = {arXiv},
	Arxivid = {1610.09787},
	Author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
	Journal = {arXiv 1610.09787},
	Keywords = {bayesian inference,computation,model criticism,neural networks,probabilistic models,probabilistic programming,scalable},
	Title = {{Edward: A library for probabilistic modeling, inference, and criticism}},
	Year = {2016}}

@article{Bingham2018pyro,
	Author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
	Journal = {Journal of Machine Learning Research},
	Title = {{Pyro: Deep Universal Probabilistic Programming}},
	Year = {2018}}

@inproceedings{Goodman2008,
	Author = {Goodman, Noah D. and Mansinghka, Vikash K. and Roy, Daniel and Bonawitz, Keith and Tenenbaum, Joshua B.},
	Booktitle = {Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence, UAI 2008},
	Isbn = {0974903949},
	Pages = {220--229},
	Title = {{Church: A language for generative models}},
	Year = {2008}}

@article{Wood2014,
	Author = {Wood, Frank and {Van De Meent}, Jan Willem and Mansinghka, Vikash},
	Journal = {Journal of Machine Learning Research},
	Pages = {1024--1032},
	Title = {{A new approach to probabilistic programming inference}},
	Volume = {33},
	Year = {2014}}

@article{Carpenter2017,
	Author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	Journal = {Journal of Statistical Software},
	Number = {1},
	Title = {{Stan: A probabilistic programming language}},
	Volume = {76},
	Year = {2017}}

@article{Kschischang2001,
	Author = {Kschischang, Frank R. and Frey, Brendan J. and Loeliger, Hans Andrea},
	Journal = {IEEE Transactions on Information Theory},
	Number = {2},
	Pages = {498--519},
	Title = {{Factor graphs and the sum-product algorithm}},
	Volume = {47},
	Year = {2001}}

@article{shinzato2008learning,
	Author = {Shinzato, Takashi and Kabashima, Yoshiyuki},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {1},
	Pages = {015005},
	Publisher = {IOP Publishing},
	Title = {Learning from correlated patterns by simple perceptrons},
	Volume = {42},
	Year = {2008}}

@article{shinzato2008perceptron,
	Author = {Shinzato, Takashi and Kabashima, Yoshiyuki},
	Journal = {Journal of Physics A: Mathematical and Theoretical},
	Number = {32},
	Pages = {324013},
	Publisher = {IOP Publishing},
	Title = {Perceptron capacity revisited: classification ability for correlated patterns},
	Volume = {41},
	Year = {2008}}

@inproceedings{Ge2018,
	Author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
	Booktitle = {International Conference on Artificial Intelligence and Statistics, AISTATS 2018},
	Title = {{Turing: A language for flexible probabilistic inference}},
	Volume = {84},
	Year = {2018}}

@inproceedings{Gabrie2018,
	Author = {Gabri{\'{e}}, Marylou and Manoel, Andre and Luneau, Cl{\'{e}}ment and Barbier, Jean and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
	Booktitle = {Advances in Neural Information Processing Systems 31},
	Title = {{Entropy and mutual information in models of deep neural networks}},
	Year = {2018}}

@article{Vehtari2014,
	Author = {Vehtari, Aki and Gelman, Andrew and Sivula, Tuomas and Jyl{\"{a}}nki, Pasi and Tran, Dustin and Sahai, Swupnil and Blomstedt, Paul and Cunningham, John P. and Schiminovich, David and Robert, Christian},
	Journal = {arXiv},
	Title = {{Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data}},
	Volume = {1412.4869},
	Year = {2014}}

@article{Manoel2018,
	Author = {Manoel, Andre and Krzakala, Florent and Varoquaux, Ga{\"{e}}l and Thirion, Bertrand and Zdeborov{\'{a}}, Lenka},
	Journal = {arXiv},
	Title = {{Approximate message-passing for convex optimization with non-separable penalties}},
	Volume = {1809.06304},
	Year = {2018}}

@inproceedings{krzakala2014variational,
	Author = {Krzakala, Florent and Manoel, Andre and Tramel, Eric W and Zdeborov{\'a}, Lenka},
	Booktitle = {2014 IEEE International Symposium on Information Theory},
	Organization = {IEEE},
	Pages = {1499--1503},
	Title = {Variational free energies for compressed sensing},
	Year = {2014}}

@article{Lesieur2017,
	Author = {Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {7},
	Title = {{Constrained low-rank matrix estimation: Phase transitions, approximate message passing and applications}},
	Volume = {2017},
	Year = {2017}}

@inproceedings{Manoel2017,
	Author = {Manoel, Andre and Krzakala, Florent and Mezard, Marc and Zdeborova, Lenka},
	Booktitle = {IEEE International Symposium on Information Theory},
	Title = {{Multi-layer generalized linear estimation}},
	Year = {2017}}

@article{Reeves2017,
	Author = {Reeves, Galen},
	Journal = {arXiv},
	Title = {{Additivity of Information in Multilayer Networks via Additive Gaussian Noise Transforms}},
	Volume = {1710.04580v1},
	Year = {2017}}

@article{Opper2005JMLR,
	Author = {Opper, Manfred and Winther, Ole},
	Journal = {Journal of Machine Learning Research},
	Pages = {2177--2206},
	Title = {{Expectation consistent approximate inference}},
	Volume = {6},
	Year = {2005}}

@article{Opper2005NIPS,
	Author = {Opper, Manfred and Winther, Ole},
	Journal = {Advances in Neural Information Processing Systems},
	Pages = {1001--1008},
	Title = {{Expectation consistent free energies for approximate inference}},
	Volume = {17},
	Year = {2005}}

@inproceedings{csato2002tap,
	Author = {Csat{\'o}, Lehel and Opper, Manfred and Winther, Ole},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {657--663},
	Title = {TAP Gibbs free energy, belief propagation and sparsity},
	Year = {2002}}

@phdthesis{MinkaThesis,
	Author = {Minka, Thomas P},
	School = {MIT Media Lab},
	Title = {A family of algorithms for approximate Bayesian inference},
	Year = {2001}}

@inproceedings{Minka_EP_2001,
	Author = {Minka, Thomas P},
	Booktitle = {Proceedings of the UAI},
	Title = {{Expectation Propagation for approximate Bayesian inference}},
	Year = {2001}}

@misc{Minka_free_2001,
	Author = {Minka, Thomas P},
	Title = {{The EP energy function and minimization schemes}},
	Url = {https://tminka.github.io/papers/ep/minka-ep-energy.pdf},
	Year = {2001},
	Bdsk-Url-1 = {https://tminka.github.io/papers/ep/minka-ep-energy.pdf}}

@article{Csiszar1975,
	Author = {Csiszar, I},
	Journal = {The Annals of Probability},
	Number = {1},
	Pages = {146--158},
	Title = {{I-divergence geometry of probabilty distributions and minimization problems}},
	Volume = {3},
	Year = {1975}}

@article{Csiszar1984,
	Author = {Csiszar, I and Tusnady, G},
	Journal = {Statistics and Decisions},
	Pages = {205--237},
	Title = {{Information geometry and alternating minimization procedures}},
	Volume = {1},
	Year = {1984}}

@phdthesis{Wainwright2002,
	Author = {Wainwright, Martin J},
	Booktitle = {Thesis},
	School = {MIT},
	Title = {{Stochastic Processes on graphs with cycles: Geometric and variational approaches}},
	Year = {2002}}

@article{Dehaene2018,
	Author = {Dehaene, Guillaume and Barthelm{\'{e}}, Simon},
	Journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	Month = {jan},
	Number = {1},
	Pages = {199--217},
	Publisher = {Blackwell Publishing Ltd},
	Title = {{Expectation propagation in the large data limit}},
	Volume = {80},
	Year = {2018}}

@book{Wainwright2008,
	Author = {Wainwright, Martin J. and Jordan, Michael I.},
	Booktitle = {Foundations and Trends in Machine Learning},
	Title = {{Graphical models, exponential families, and variational inference}},
	Year = {2008}}

@article{Yedidia2001,
	Author = {Yedidia, JS and Freeman, WT},
	Journal = {Exploring artificial intelligence in the new millennium},
	Pages = {239 -- 269},
	Title = {{Understanding belief propagation and its generalizations}},
	Year = {2001}}

@article{Yedidia2005,
	Author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
	Journal = {IEEE Transactions on Information Theory},
	Number = {7},
	Pages = {2282--2312},
	Title = {{Constructing free-energy approximations and generalized belief propagation algorithms}},
	Volume = {51},
	Year = {2005}}

@article{Guo2005,
	Author = {Guo, Dongning and Shamai, Shlomo and Verd{\'{u}}, Sergio},
	Journal = {IEEE Transactions on Information Theory},
	Number = {4},
	Pages = {1261--1282},
	Title = {{Mutual information and minimum mean-square error in Gaussian channels}},
	Volume = {51},
	Year = {2005}}

@article{Cohen2015,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1505.04413v2},
	Author = {Cohen, Taco S. and Welling, Max},
	Journal = {32nd International Conference on Machine Learning, ICML 2015},
	Pages = {1757--1765},
	Title = {{Harmonic exponential families on manifolds}},
	Volume = {3},
	Year = {2015}}

@article{Perry2018,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1610.04583v1},
	Author = {Perry, Amelia and Wein, Alexander S. and Bandeira, Afonso S. and Moitra, Ankur},
	Journal = {Communications on Pure and Applied Mathematics},
	Number = {11},
	Pages = {2275--2322},
	Title = {{Message-Passing Algorithms for Synchronization Problems over Compact Groups}},
	Volume = {71},
	Year = {2018}}

@book{Bishop2006,
	Author = {Bishop, Christopher M.},
	Pages = {738},
	Publisher = {Springer},
	Title = {{Pattern recognition and machine learning}},
	Year = {2006}}

@book{Rasmussen2006,
	Author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	Publisher = {MIT Press},
	Title = {{Gaussian Processes for Machine Learning}},
	Year = {2006}}

@book{Tulino2004,
	Author = {Tulino, Antonia M. and Verd{\'{u}}, Sergio},
	Booktitle = {Foundations and Trends in Communications and Information Theory},
	Pages = {1--182},
	Title = {{Random matrix theory and wireless communications}},
	Year = {2004}}

@inproceedings{Rangan2018,
	Author = {Rangan, Sundeep and Schniter, Philip and Fletcher, Alyson K},
	Booktitle = {IEEE International Symposium on Information Theory - Proceedings},
	Pages = {1588--1592},
	Title = {{Vector approximate message passing}},
	Year = {2017}}

@article{maillard2019high,
	Author = {Maillard, Antoine and Foini, Laura and Castellanos, Alejandro Lage and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {11},
	Pages = {113301},
	Publisher = {IOP Publishing},
	Title = {High-temperature expansions and message passing algorithms},
	Volume = {2019},
	Year = {2019}}

@article{Cakmak2016,
	Author = {{\c{C}}akmak, Burak and Opper, Manfred and Fleury, Bernard H. and Winther, Ole},
	Journal = {arXiv},
	Title = {{Self-Averaging Expectation Propagation}},
	Volume = {1608.06602},
	Year = {2016}}

@article{Cakmak2014,
	Author = {{\c{C}}akmak, Burak and Winther, Ole and Fleury, Bernard H.},
	Journal = {2014 IEEE Information Theory Workshop, ITW 2014},
	Keywords = {S-transform in free probability,Variational inference,approximate message passing,free energy optimization},
	Number = {4},
	Pages = {192--196},
	Title = {{S-AMP: Approximate message passing for general matrix ensembles}},
	Volume = {1},
	Year = {2014}}

@article{Zoeter2005,
	Author = {Zoeter, Onno and Heskes, Tom},
	Journal = {Journal of Machine Learning Research},
	Pages = {1999--2026},
	Title = {{Change point problems in linear dynamical systems}},
	Volume = {6},
	Year = {2005}}

@article{som2012compressive,
	Author = {Som, Subhojit and Schniter, Philip},
	Journal = {IEEE transactions on signal processing},
	Number = {7},
	Pages = {3439--3448},
	Publisher = {IEEE},
	Title = {Compressive imaging using approximate message passing and a Markov-tree prior},
	Volume = {60},
	Year = {2012}}

@article{ma2017orthogonal,
	Author = {Ma, Junjie and Ping, Li},
	Journal = {IEEE Access},
	Pages = {2020--2033},
	Publisher = {IEEE},
	Title = {Orthogonal amp},
	Volume = {5},
	Year = {2017}}


@misc{keras_vae,
	Author = {Keras-VAE},
	Howpublished = {\url{https://keras.io/examples/generative/vae/}},
	Year = {2020},
	Title = {Example of VAE on MNIST dataset using MLP}}

@inproceedings{Heskes2002,
	Archiveprefix = {arXiv},
	Arxivid = {1301.0572},
	Author = {Heskes, Tom and Zoeter, Onno},
	Booktitle = {Proceedings UAI-2002},
	Pages = {216--233},
	Title = {{Expectation Propogation for approximate inference in dynamic Bayesian networks}},
	Year = {2002}}

@article{Vila2015,
	Author = {Vila, Jeremy and Schniter, Philip and Rangan, Sundeep and Krzakala, Florent and Zdeborova, Lenka},
	Journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	Pages = {2021--2025},
	Title = {{Adaptive damping and mean removal for the generalized approximate message passing algorithm}},
	Year = {2015}}

@article{Opper2000,
	Author = {Manfred Opper and Ole Winther},
	Journal = {Neural Computation},
	Pages = {2655-2684},
	Title = {Gaussian Processes for Classification: Mean-Field Algorithms},
	Volume = {12},
	Year = {2000}}

@article{Dempster1977,
	Author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	Journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	Title = {{ Maximum Likelihood from Incomplete Data Via the EM Algorithm }},
	Year = {1977}}

@inproceedings{Reeves2016,
	Author = {Reeves, Galen and Pfister, Henry D.},
	Booktitle = {IEEE International Symposium on Information Theory - Proceedings},
	Title = {{The replica-symmetric prediction for compressed sensing with Gaussian matrices is exact}},
	Year = {2016}}

@article{pymc3,
	Author = {Salvatier, John and Wiecki, Thomas and Fonnesbeck, Christopher},
	Doi = {10.7287/PEERJ.PREPRINTS.1686V1},
	Month = {01},
	Title = {Probabilistic programming in Python using PyMC3},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.7287/PEERJ.PREPRINTS.1686V1}}

@inproceedings{aubin2019exact,
  title        = {Exact asymptotics for phase retrieval and compressed sensing with random generative priors},
  author       = {Aubin, Benjamin and Loureiro, Bruno and Baker, Antoine and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle    = {Mathematical and Scientific Machine Learning},
  pages        = {55--73},
  year         = {2020},
  organization = {PMLR},
  Arxivid={1912.02008},
  Eprint={https://arxiv.org/abs/1912.02008}
}

@book{Pearl1988,
	Address = {San Francisco, CA, USA},
	Author = {Pearl, Judea},
	Isbn = {0934613737},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference},
	Year = {1988}}

@book{hiriartUrruty1993,
	title = {Convex analysis and minimization algorithms II},
	author = {J.-B. Hiriart-Urruty and C. Lemarechal},
	publisher = {Springer-Verlag},
	year = 1993}

@InProceedings{pmlr-v40-Thrampoulidis15,
  title = 	 {Regularized Linear Regression: A Precise Analysis of the Estimation Error},
  author = 	 {Christos Thrampoulidis and Samet Oymak and Babak Hassibi},
  booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
  pages = 	 {1683--1709},
  year = 	 {2015},
  volume = 	 {40},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Paris, France},
  month = 	 {03--06 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v40/Thrampoulidis15.pdf},
  url = 	 {http://proceedings.mlr.press/v40/Thrampoulidis15.html},
  abstract = 	 {Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, Group-LASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The  machinery builds upon  Gordon’s Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization  (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.}
}



@article{baldi1991temporal,
  title={Temporal evolution of generalization during learning in linear networks},
  author={Baldi, Pierre and Chauvin, Yves},
  journal={Neural Computation},
  volume={3},
  number={4},
  pages={589--603},
  year={1991},
  publisher={MIT Press}
}

@article{baldi1995learning,
  title={Learning in linear neural networks: A survey},
  author={Baldi, Pierre F and Hornik, Kurt},
  journal={IEEE Transactions on neural networks},
  volume={6},
  number={4},
  pages={837--858},
  year={1995},
  publisher={IEEE}
}

@article{dunmur1993learning,
  title={Learning and generalization in a linear perceptron stochastically trained with noisy data},
  author={Dunmur, AP and Wallace, DJ},
  journal={Journal of Physics A: Mathematical and General},
  volume={26},
  number={21},
  pages={5767},
  year={1993},
  publisher={IOP Publishing}
}

@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}


@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8141--8150},
  year={2019}
}
@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8572--8583},
  year={2019}
}


@inproceedings{yuille2001double,
  title={A double-loop algorithm to minimize the bethe free energy},
  author={Yuille, Alan},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={3--18},
  year={2001},
  organization={Springer}
}
@inproceedings{arora2018gans,
  title={Do GANs learn the distribution? some theory and empirics},
  author={Arora, Sanjeev and Risteski, Andrej and Zhang, Yi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@incollection{hoeffding1994probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  booktitle={The Collected Works of Wassily Hoeffding},
  pages={409--426},
  year={1994},
  publisher={Springer}
}

@article{griewank1992achieving,
  title={Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation},
  author={Griewank, Andreas},
  journal={Optimization Methods and software},
  volume={1},
  number={1},
  pages={35--54},
  year={1992},
  publisher={Taylor \& Francis}
}


@article{goldt2019modelling,
  title={Modelling the influence of data structure on learning in neural networks},
  author={Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1909.11500},
  year={2019}
}

@inproceedings{goldt2019dynamics,
	Author = {Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {6981--6991},
	Title = {Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
	Year = {2019}}


@article{nishimori1980exact,
  title={Exact results and critical properties of the Ising model with competing interactions},
  author={Nishimori, Hidetoshi},
  journal={Journal of Physics C: Solid State Physics},
  volume={13},
  number={21},
  pages={4071},
  year={1980},
  publisher={IOP Publishing}
}

@article{nishimori1981internal,
  title={Internal energy, specific heat and correlation function of the bond-random ising model},
  author={Nishimori, Hidetoshi},
  journal={Progress of Theoretical Physics},
  volume={66},
  number={4},
  pages={1169--1181},
  year={1981},
  publisher={Oxford University Press}
}

@article{georges1985exact,
  title={Exact properties of spin glasses. II. Nishimori's line: new results and physical implications},
  author={Georges, Antoine and Hansel, David and Le Doussal, Pierre and Bouchaud, J-P},
  journal={Journal de Physique},
  volume={46},
  number={11},
  pages={1827--1836},
  year={1985},
  publisher={Soci{\'e}t{\'e} fran{\c{c}}aise de physique}
}

@article{hartmann2018high,
  title={High-precision simulation of the height distribution for the KPZ equation},
  author={Hartmann, Alexander K and Le Doussal, Pierre and Majumdar, Satya N and Rosso, Alberto and Schehr, Gregory},
  journal={EPL (Europhysics Letters)},
  volume={121},
  number={6},
  pages={67004},
  year={2018},
  publisher={IOP Publishing}
}


@article{louart2018random,
  title={A random matrix approach to neural networks},
  author={Louart, Cosme and Liao, Zhenyu and Couillet, Romain and others},
  journal={The Annals of Applied Probability},
  volume={28},
  number={2},
  pages={1190--1248},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@book{couillet2011random,
  title={Random matrix methods for wireless communications},
  author={Couillet, Romain and Debbah, Merouane},
  year={2011},
  publisher={Cambridge University Press}
}


